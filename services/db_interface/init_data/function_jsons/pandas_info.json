[
    {
        "name": "DataFrame.at",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.DataFrame.at.html",
        "description": "Access a single value for a row/column label pair.  Similar to ``loc``, in that both provide label-based lookups. Use ``at`` if you only need to get or set a single value in a DataFrame or Series.  Raises ------ KeyError * If getting a value and 'label' does not exist in a DataFrame or Series. ValueError * If row/column label pair is not a tuple or if any label from the pair is not a scalar for DataFrame. * If label is list-like (*excluding* NamedTuple) for Series.  See Also -------- DataFrame.at : Access a single value for a row/column pair by label. DataFrame.iat : Access a single value for a row/column pair by integer position. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s). Series.at : Access a single value by label. Series.iat : Access a single value by integer position. Series.loc : Access a group of rows by label(s). Series.iloc : Access a group of rows by integer position(s).  Notes ----- See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>` for more details.  Examples -------- >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ...                   index=[4, 5, 6], columns=['A', 'B', 'C']) >>> df A   B   C 4   0   2   3 5   0   4   1 6  10  20  30  Get value at specified row/column pair  >>> df.at[4, 'B'] 2  Set value at specified row/column pair  >>> df.at[4, 'B'] = 10 >>> df.at[4, 'B'] 10  Get value within a Series  >>> df.loc[5].at['B'] 4",
        "source": "@property\n    def at(self) -> _AtIndexer:\n        \n        return _AtIndexer(\"at\", self)"
    },
    {
        "name": "DataFrame.dtypes",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html",
        "description": "Return the dtypes in the DataFrame.  This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the ``object`` dtype. See :ref:`the User Guide <basics.dtypes>` for more.  Returns ------- pandas.Series The data type of each column.  Examples -------- >>> df = pd.DataFrame({'float': [1.0], ...                    'int': [1], ...                    'datetime': [pd.Timestamp('20180310')], ...                    'string': ['foo']}) >>> df.dtypes float              float64 int                  int64 datetime    datetime64[ns] string              object dtype: object",
        "source": "@property\n    def dtypes(self):\n        \n        data = self._mgr.get_dtypes()\n        return self._constructor_sliced(data, index=self._info_axis, dtype=np.object_)"
    },
    {
        "name": "DataFrame.values",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html",
        "description": "Return a Numpy representation of the DataFrame.  .. warning::  We recommend using :meth:`DataFrame.to_numpy` instead.  Only the values in the DataFrame will be returned, the axes labels will be removed.  Returns ------- numpy.ndarray The values of the DataFrame.  See Also -------- DataFrame.to_numpy : Recommended alternative to this method. DataFrame.index : Retrieve the index labels. DataFrame.columns : Retrieving the column names.  Notes ----- The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks.  e.g. If the dtypes are float16 and float32, dtype will be upcast to float32.  If dtypes are int32 and uint8, dtype will be upcast to int32. By :func:`numpy.find_common_type` convention, mixing int64 and uint64 will result in a float64 dtype.  Examples -------- A DataFrame where all columns are the same type (e.g., int64) results in an array of the same type.  >>> df = pd.DataFrame({'age':    [ 3,  29], ...                    'height': [94, 170], ...                    'weight': [31, 115]}) >>> df age  height  weight 0    3      94      31 1   29     170     115 >>> df.dtypes age       int64 height    int64 weight    int64 dtype: object >>> df.values array([[  3,  94,  31], [ 29, 170, 115]])  A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray of the broadest type that accommodates these mixed types (e.g., object).  >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'), ...                     ('lion',     80.5, 1), ...                     ('monkey', np.nan, None)], ...                   columns=('name', 'max_speed', 'rank')) >>> df2.dtypes name          object max_speed    float64 rank          object dtype: object >>> df2.values array([['parrot', 24.0, 'second'], ['lion', 80.5, 1], ['monkey', nan, None]], dtype=object)",
        "source": "@property\n    def values(self) -> np.ndarray:\n        \n        return self._mgr.as_array()"
    },
    {
        "name": "DataFrame.axes",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.axes.html",
        "description": "Return a list representing the axes of the DataFrame.  It has the row axis labels and column axis labels as the only members. They are returned in that order.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.axes [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='object')]",
        "source": "@property\n    def axes(self) -> list[Index]:\n        \n        return [self.index, self.columns]"
    },
    {
        "name": "DataFrame.ndim",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ndim.html",
        "description": "Return an int representing the number of axes / array dimensions.  Return 1 if Series. Otherwise return 2 if DataFrame.  See Also -------- ndarray.ndim : Number of array dimensions.  Examples -------- >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3}) >>> s.ndim 1  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.ndim 2",
        "source": "@final\n    @property\n    def ndim(self) -> int:\n        \n        return self._mgr.ndim"
    },
    {
        "name": "DataFrame.size",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html",
        "description": "Return an int representing the number of elements in this object.  Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.  See Also -------- ndarray.size : Number of elements in the array.  Examples -------- >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3}) >>> s.size 3  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.size 4",
        "source": "@final\n    @property\n    def size(self) -> int:\n        \n\n        return int(np.prod(self.shape))"
    },
    {
        "name": "DataFrame.shape",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html",
        "description": "Return a tuple representing the dimensionality of the DataFrame.  See Also -------- ndarray.shape : Tuple of array dimensions.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.shape (2, 2)  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4], ...                    'col3': [5, 6]}) >>> df.shape (2, 3)",
        "source": "@property\n    def shape(self) -> tuple[int, int]:\n        \n        return len(self.index), len(self.columns)"
    },
    {
        "name": "DataFrame.memory_usage",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.memory_usage.html",
        "description": "Return the memory usage of each column in bytes.  The memory usage can optionally include the contribution of the index and elements of `object` dtype.  This value is displayed in `DataFrame.info` by default. This can be suppressed by setting ``pandas.options.display.memory_usage`` to False.  Parameters ---------- index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If ``index=True``, the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating `object` dtypes for system-level memory consumption, and include it in the returned values.  Returns ------- Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes.  See Also -------- numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame.  Notes ----- See the :ref:`Frequently Asked Questions <df-memory-usage>` for more details.  Examples -------- >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t)) ...              for t in dtypes]) >>> df = pd.DataFrame(data) >>> df.head() int64  float64            complex128  object  bool 0      1      1.0              1.0+0.0j       1  True 1      1      1.0              1.0+0.0j       1  True 2      1      1.0              1.0+0.0j       1  True 3      1      1.0              1.0+0.0j       1  True 4      1      1.0              1.0+0.0j       1  True  >>> df.memory_usage() Index           128 int64         40000 float64       40000 complex128    80000 object        40000 bool           5000 dtype: int64  >>> df.memory_usage(index=False) int64         40000 float64       40000 complex128    80000 object        40000 bool           5000 dtype: int64  The memory footprint of `object` dtype columns is ignored by default:  >>> df.memory_usage(deep=True) Index            128 int64          40000 float64        40000 complex128     80000 object        180000 bool            5000 dtype: int64  Use a Categorical for efficient storage of an object-dtype column with many repeated values.  >>> df['object'].astype('category').memory_usage(deep=True) 5244",
        "source": "def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        \n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n            dtype=np.intp,\n        )\n        if index:\n            index_memory_usage = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            )\n            result = index_memory_usage._append(result)\n        return result"
    },
    {
        "name": "DataFrame.empty",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.empty.html",
        "description": "Indicator whether Series/DataFrame is empty.  True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0.  Returns ------- bool If Series/DataFrame is empty, return True, if not return False.  See Also -------- Series.dropna : Return series without null values. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing.  Notes ----- If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.  Examples -------- An example of an actual empty DataFrame. Notice the index is empty:  >>> df_empty = pd.DataFrame({'A' : []}) >>> df_empty Empty DataFrame Columns: [A] Index: [] >>> df_empty.empty True  If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty:  >>> df = pd.DataFrame({'A' : [np.nan]}) >>> df A 0 NaN >>> df.empty False >>> df.dropna().empty True  >>> ser_empty = pd.Series({'A' : []}) >>> ser_empty A    [] dtype: object >>> ser_empty.empty False >>> ser_empty = pd.Series() >>> ser_empty.empty True",
        "source": "@property\n    def empty(self) -> bool_t:\n        \n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)"
    },
    {
        "name": "DataFrame.set_flags",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_flags.html",
        "description": "Return a new object with updated flags.  Parameters ---------- copy : bool, default False Specify if a copy of the object should be made. allows_duplicate_labels : bool, optional Whether the returned object allows duplicate labels.  Returns ------- Series or DataFrame The same type as the caller.  See Also -------- DataFrame.attrs : Global metadata applying to this dataset. DataFrame.flags : Global flags applying to this object.  Notes ----- This method returns a new object that's a view on the same data as the input. Mutating the input or the output values will be reflected in the other.  This method is intended to be used in method chains.  \"Flags\" differ from \"metadata\". Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in :attr:`DataFrame.attrs`.  Examples -------- >>> df = pd.DataFrame({\"A\": [1, 2]}) >>> df.flags.allows_duplicate_labels True >>> df2 = df.set_flags(allows_duplicate_labels=False) >>> df2.flags.allows_duplicate_labels False",
        "source": "@final\n    def set_flags(\n        self,\n        *,\n        copy: bool_t = False,\n        allows_duplicate_labels: bool_t | None = None,\n    ) -> Self:\n        \n        df = self.copy(deep=copy and not using_copy_on_write())\n        if allows_duplicate_labels is not None:\n            df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels\n        return df"
    },
    {
        "name": "DataFrame.convert_dtypes",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html",
        "description": "Convert columns to the best possible dtypes using dtypes supporting ``pd.NA``.  Parameters ---------- infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to ``StringDtype()``. convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to ``BooleanDtypes()``. convert_floating : bool, defaults True Whether, if possible, conversion can be done to floating extension types. If `convert_integer` is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers.  .. versionadded:: 1.2.0 dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable' Back-end data type applied to the resultant :class:`DataFrame` (still experimental). Behaviour is as follows:  * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame` (default). * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype` DataFrame.  .. versionadded:: 2.0  Returns ------- Series or DataFrame Copy of input object with new dtype.  See Also -------- infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type.  Notes ----- By default, ``convert_dtypes`` will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options ``convert_string``, ``convert_integer``, ``convert_boolean`` and ``convert_floating``, it is possible to turn off individual conversions to ``StringDtype``, the integer extension types, ``BooleanDtype`` or floating extension types, respectively.  For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference rules as during normal Series/DataFrame construction.  Then, if possible, convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer or floating extension type, otherwise leave as ``object``.  If the dtype is integer, convert to an appropriate integer extension type.  If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type.  .. versionchanged:: 1.2 Starting with pandas 1.2, this method also converts float columns to the nullable floating extension type.  In the future, as new dtypes are added that support ``pd.NA``, the results of this method will change to support those new dtypes.  Examples -------- >>> df = pd.DataFrame( ...     { ...         \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")), ...         \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")), ...         \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")), ...         \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")), ...         \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")), ...         \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")), ...     } ... )  Start with a DataFrame with default dtypes.  >>> df a  b      c    d     e      f 0  1  x   True    h  10.0    NaN 1  2  y  False    i   NaN  100.5 2  3  z    NaN  NaN  20.0  200.0  >>> df.dtypes a      int32 b     object c     object d     object e    float64 f    float64 dtype: object  Convert the DataFrame to use best possible dtypes.  >>> dfn = df.convert_dtypes() >>> dfn a  b      c     d     e      f 0  1  x   True     h    10   <NA> 1  2  y  False     i  <NA>  100.5 2  3  z   <NA>  <NA>    20  200.0  >>> dfn.dtypes a             Int32 b    string[python] c           boolean d    string[python] e             Int64 f           Float64 dtype: object  Start with a Series of strings and missing data represented by ``np.nan``.  >>> s = pd.Series([\"a\", \"b\", np.nan]) >>> s 0      a 1      b 2    NaN dtype: object  Obtain a Series with dtype ``StringDtype``.  >>> s.convert_dtypes() 0       a 1       b 2    <NA> dtype: string",
        "source": "@final\n    def convert_dtypes(\n        self,\n        infer_objects: bool_t = True,\n        convert_string: bool_t = True,\n        convert_integer: bool_t = True,\n        convert_boolean: bool_t = True,\n        convert_floating: bool_t = True,\n        dtype_backend: DtypeBackend = \"numpy_nullable\",\n    ) -> Self:\n        \n        check_dtype_backend(dtype_backend)\n        if self.ndim == 1:\n            return self._convert_dtypes(\n                infer_objects,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n                dtype_backend=dtype_backend,\n            )\n        else:\n            results = [\n                col._convert_dtypes(\n                    infer_objects,\n                    convert_string,\n                    convert_integer,\n                    convert_boolean,\n                    convert_floating,\n                    dtype_backend=dtype_backend,\n                )\n                for col_name, col in self.items()\n            ]\n            if len(results) > 0:\n                result = concat(results, axis=1, copy=False, keys=self.columns)\n                cons = cast(type[\"DataFrame\"], self._constructor)\n                result = cons(result)\n                result = result.__finalize__(self, method=\"convert_dtypes\")\n                # https://github.com/python/mypy/issues/8354\n                return cast(Self, result)\n            else:\n                return self.copy(deep=None)"
    },
    {
        "name": "DataFrame.infer_objects",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.infer_objects.html",
        "description": "Attempt to infer better dtypes for object columns.  Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.  Parameters ---------- copy : bool, default True Whether to make a copy for non-object or non-inferable columns or Series.  Returns ------- same type as input object  See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype.  Examples -------- >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]}) >>> df = df.iloc[1:] >>> df A 1  1 2  2 3  3  >>> df.dtypes A    object dtype: object  >>> df.infer_objects().dtypes A    int64 dtype: object",
        "source": "@final\n    def infer_objects(self, copy: bool_t | None = None) -> Self:\n        \n        new_mgr = self._mgr.convert(copy=copy)\n        res = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n        return res.__finalize__(self, method=\"infer_objects\")"
    },
    {
        "name": "DataFrame.copy",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html",
        "description": "Make a copy of this object's indices and data.  When ``deep=True`` (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below).  When ``deep=False``, a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa).  Parameters ---------- deep : bool, default True Make a deep copy, including a copy of the data and the indices. With ``deep=False`` neither the indices nor the data are copied.  Returns ------- Series or DataFrame Object type matches caller.  Notes ----- When ``deep=True``, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to `copy.deepcopy` in the Standard Library, which recursively copies object data (see examples below).  While ``Index`` objects are copied when ``deep=True``, the underlying numpy array is not copied for performance reasons. Since ``Index`` is immutable, the underlying data can be safely shared and a copy is not needed.  Since pandas is not thread safe, see the :ref:`gotchas <gotchas.thread-safety>` when copying in a threading environment.  When ``copy_on_write`` in pandas config is set to ``True``, the ``copy_on_write`` config takes effect even when ``deep=False``. This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See :ref:`Copy_on_Write <copy_on_write>` for more information.  Examples -------- >>> s = pd.Series([1, 2], index=[\"a\", \"b\"]) >>> s a    1 b    2 dtype: int64  >>> s_copy = s.copy() >>> s_copy a    1 b    2 dtype: int64  **Shallow copy versus default (deep) copy:**  >>> s = pd.Series([1, 2], index=[\"a\", \"b\"]) >>> deep = s.copy() >>> shallow = s.copy(deep=False)  Shallow copy shares data and index with original.  >>> s is shallow False >>> s.values is shallow.values and s.index is shallow.index True  Deep copy has own copy of data and index.  >>> s is deep False >>> s.values is deep.values or s.index is deep.index False  Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged.  >>> s.iloc[0] = 3 >>> shallow.iloc[1] = 4 >>> s a    3 b    4 dtype: int64 >>> shallow a    3 b    4 dtype: int64 >>> deep a    1 b    2 dtype: int64  Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy.  >>> s = pd.Series([[1, 2], [3, 4]]) >>> deep = s.copy() >>> s[0][0] = 10 >>> s 0    [10, 2] 1     [3, 4] dtype: object >>> deep 0    [10, 2] 1     [3, 4] dtype: object  ** Copy-on-Write is set to true: **  >>> with pd.option_context(\"mode.copy_on_write\", True): ...     s = pd.Series([1, 2], index=[\"a\", \"b\"]) ...     copy = s.copy(deep=False) ...     s.iloc[0] = 100 ...     s a    100 b      2 dtype: int64 >>> copy a    1 b    2 dtype: int64",
        "source": "@final\n    def copy(self, deep: bool_t | None = True) -> Self:\n        \n        data = self._mgr.copy(deep=deep)\n        self._clear_item_cache()\n        return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n            self, method=\"copy\"\n        )"
    },
    {
        "name": "DataFrame.bool",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.bool.html",
        "description": "Return the bool of a single element Series or DataFrame.  .. deprecated:: 2.1.0  bool is deprecated and will be removed in future version of pandas  This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns ------- bool The value in the Series or DataFrame.  See Also -------- Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.  Examples -------- The method will only work for single element objects with a boolean value:  >>> pd.Series([True]).bool()  # doctest: +SKIP True >>> pd.Series([False]).bool()  # doctest: +SKIP False  >>> pd.DataFrame({'col': [True]}).bool()  # doctest: +SKIP True >>> pd.DataFrame({'col': [False]}).bool()  # doctest: +SKIP False",
        "source": "@final\n    def bool(self) -> bool_t:\n        warnings.warn(\n            f\"{type(self).__name__}.bool is now deprecated and will be removed \"\n            \"in future version of pandas\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        v = self.squeeze()\n        if isinstance(v, (bool, np.bool_)):\n            return bool(v)\n        elif is_scalar(v):\n            raise ValueError(\n                \"bool cannot act on a non-boolean single element \"\n                f\"{type(self).__name__}\"\n            )\n\n        self.__nonzero__()\n        # for mypy (__nonzero__ raises)\n        return True"
    },
    {
        "name": "DataFrame.head",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html",
        "description": "Return the first `n` rows.  This function returns the first `n` rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.  For negative values of `n`, this function returns all rows except the last `|n|` rows, equivalent to ``df[:n]``.  If n is larger than the number of rows, this function returns all rows.  Parameters ---------- n : int, default 5 Number of rows to select.  Returns ------- same type as caller The first `n` rows of the caller object.  See Also -------- DataFrame.tail: Returns the last `n` rows.  Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey 5     parrot 6      shark 7      whale 8      zebra  Viewing the first 5 lines  >>> df.head() animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey  Viewing the first `n` lines (three in this case)  >>> df.head(3) animal 0  alligator 1        bee 2     falcon  For negative values of `n`  >>> df.head(-3) animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey 5     parrot",
        "source": "@final\n    def head(self, n: int = 5) -> Self:\n        \n        if using_copy_on_write():\n            return self.iloc[:n].copy()\n        return self.iloc[:n]"
    },
    {
        "name": "DataFrame.iat",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iat.html",
        "description": "Access a single value for a row/column pair by integer position.  Similar to ``iloc``, in that both provide integer-based lookups. Use ``iat`` if you only need to get or set a single value in a DataFrame or Series.  Raises ------ IndexError When integer position is out of bounds.  See Also -------- DataFrame.at : Access a single value for a row/column label pair. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s).  Examples -------- >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ...                   columns=['A', 'B', 'C']) >>> df A   B   C 0   0   2   3 1   0   4   1 2  10  20  30  Get value at specified row/column pair  >>> df.iat[1, 2] 1  Set value at specified row/column pair  >>> df.iat[1, 2] = 10 >>> df.iat[1, 2] 10  Get value within a series  >>> df.loc[0].iat[1] 2",
        "source": "@property\n    def iat(self) -> _iAtIndexer:\n        \n        return _iAtIndexer(\"iat\", self)"
    },
    {
        "name": "DataFrame.loc",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html",
        "description": "Access a group of rows and columns by label(s) or a boolean array.  ``.loc[]`` is primarily label based, but may also be used with a boolean array.  Allowed inputs are:  - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is interpreted as a *label* of the index, and **never** as an integer position along the index). - A list or array of labels, e.g. ``['a', 'b', 'c']``. - A slice object with labels, e.g. ``'a':'f'``.  .. warning:: Note that contrary to usual python slices, **both** the start and the stop are included  - A boolean array of the same length as the axis being sliced, e.g. ``[True, False, True]``. - An alignable boolean Series. The index of the key will be aligned before masking. - An alignable Index. The Index of the returned selection will be the input. - A ``callable`` function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)  See more at :ref:`Selection by Label <indexing.label>`.  Raises ------ KeyError If any items are not found. IndexingError If an indexed key is passed and its index is unalignable to the frame index.  See Also -------- DataFrame.at : Access a single value for a row/column label pair. DataFrame.iloc : Access group of rows and columns by integer position(s). DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. Series.loc : Access group of values using labels.  Examples -------- **Getting values**  >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ...      index=['cobra', 'viper', 'sidewinder'], ...      columns=['max_speed', 'shield']) >>> df max_speed  shield cobra               1       2 viper               4       5 sidewinder          7       8  Single label. Note this returns the row as a Series.  >>> df.loc['viper'] max_speed    4 shield       5 Name: viper, dtype: int64  List of labels. Note using ``[[]]`` returns a DataFrame.  >>> df.loc[['viper', 'sidewinder']] max_speed  shield viper               4       5 sidewinder          7       8  Single label for row and column  >>> df.loc['cobra', 'shield'] 2  Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included.  >>> df.loc['cobra':'viper', 'max_speed'] cobra    1 viper    4 Name: max_speed, dtype: int64  Boolean list with the same length as the row axis  >>> df.loc[[False, False, True]] max_speed  shield sidewinder          7       8  Alignable boolean Series:  >>> df.loc[pd.Series([False, True, False], ...        index=['viper', 'sidewinder', 'cobra'])] max_speed  shield sidewinder          7       8  Index (same behavior as ``df.reindex``)  >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")] max_speed  shield foo cobra          1       2 viper          4       5  Conditional that returns a boolean Series  >>> df.loc[df['shield'] > 6] max_speed  shield sidewinder          7       8  Conditional that returns a boolean Series with column labels specified  >>> df.loc[df['shield'] > 6, ['max_speed']] max_speed sidewinder          7  Multiple conditional using ``&`` that returns a boolean Series  >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)] max_speed  shield viper          4       5  Multiple conditional using ``|`` that returns a boolean Series  >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)] max_speed  shield cobra               1       2 sidewinder          7       8  Please ensure that each condition is wrapped in parentheses ``()``. See the :ref:`user guide<indexing.boolean>` for more details and explanations of Boolean indexing.  .. note:: If you find yourself using 3 or more conditionals in ``.loc[]``, consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.  See below for using ``.loc[]`` on MultiIndex DataFrames.  Callable that returns a boolean Series  >>> df.loc[lambda df: df['shield'] == 8] max_speed  shield sidewinder          7       8  **Setting values**  Set value for all items matching the list of labels  >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50 >>> df max_speed  shield cobra               1       2 viper               4      50 sidewinder          7      50  Set value for an entire row  >>> df.loc['cobra'] = 10 >>> df max_speed  shield cobra              10      10 viper               4      50 sidewinder          7      50  Set value for an entire column  >>> df.loc[:, 'max_speed'] = 30 >>> df max_speed  shield cobra              30      10 viper              30      50 sidewinder         30      50  Set value for rows matching callable condition  >>> df.loc[df['shield'] > 35] = 0 >>> df max_speed  shield cobra              30      10 viper               0       0 sidewinder          0       0  Add value matching location  >>> df.loc[\"viper\", \"shield\"] += 5 >>> df max_speed  shield cobra              30      10 viper               0       5 sidewinder          0       0  Setting using a ``Series`` or a ``DataFrame`` sets the values matching the index labels, not the index positions.  >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]] >>> df.loc[:] += shuffled_df >>> df max_speed  shield cobra              60      20 viper               0      10 sidewinder          0       0  **Getting values on a DataFrame with an index that has integer labels**  Another example using integers for the index  >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ...      index=[7, 8, 9], columns=['max_speed', 'shield']) >>> df max_speed  shield 7          1       2 8          4       5 9          7       8  Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included.  >>> df.loc[7:9] max_speed  shield 7          1       2 8          4       5 9          7       8  **Getting values with a MultiIndex**  A number of examples using a DataFrame with a MultiIndex  >>> tuples = [ ...    ('cobra', 'mark i'), ('cobra', 'mark ii'), ...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'), ...    ('viper', 'mark ii'), ('viper', 'mark iii') ... ] >>> index = pd.MultiIndex.from_tuples(tuples) >>> values = [[12, 2], [0, 4], [10, 20], ...         [1, 4], [7, 1], [16, 36]] >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index) >>> df max_speed  shield cobra      mark i           12       2 mark ii           0       4 sidewinder mark i           10      20 mark ii           1       4 viper      mark ii           7       1 mark iii         16      36  Single label. Note this returns a DataFrame with a single index.  >>> df.loc['cobra'] max_speed  shield mark i          12       2 mark ii          0       4  Single index tuple. Note this returns a Series.  >>> df.loc[('cobra', 'mark ii')] max_speed    0 shield       4 Name: (cobra, mark ii), dtype: int64  Single label for row and column. Similar to passing in a tuple, this returns a Series.  >>> df.loc['cobra', 'mark i'] max_speed    12 shield        2 Name: (cobra, mark i), dtype: int64  Single tuple. Note using ``[[]]`` returns a DataFrame.  >>> df.loc[[('cobra', 'mark ii')]] max_speed  shield cobra mark ii          0       4  Single tuple for the index with a single label for the column  >>> df.loc[('cobra', 'mark i'), 'shield'] 2  Slice from index tuple to single label  >>> df.loc[('cobra', 'mark i'):'viper'] max_speed  shield cobra      mark i           12       2 mark ii           0       4 sidewinder mark i           10      20 mark ii           1       4 viper      mark ii           7       1 mark iii         16      36  Slice from index tuple to index tuple  >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')] max_speed  shield cobra      mark i          12       2 mark ii          0       4 sidewinder mark i          10      20 mark ii          1       4 viper      mark ii          7       1  Please see the :ref:`user guide<advanced.advanced_hierarchical>` for more details and explanations of advanced indexing.",
        "source": "@property\n    def loc(self) -> _LocIndexer:\n        \n        return _LocIndexer(\"loc\", self)"
    },
    {
        "name": "DataFrame.insert",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html",
        "description": "Insert column into DataFrame at specified location.  Raises a ValueError if `column` is already contained in the DataFrame, unless `allow_duplicates` is set to True.  Parameters ---------- loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : Scalar, Series, or array-like allow_duplicates : bool, optional, default lib.no_default  See Also -------- Index.insert : Insert new item by index.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df col1  col2 0     1     3 1     2     4 >>> df.insert(1, \"newcol\", [99, 99]) >>> df col1  newcol  col2 0     1      99     3 1     2      99     4 >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True) >>> df col1  col1  newcol  col2 0   100     1      99     3 1   100     2      99     4  Notice that pandas uses index alignment in case of `value` from type `Series`:  >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2])) >>> df col0  col1  col1  newcol  col2 0   NaN   100     1      99     3 1   5.0   100     2      99     4",
        "source": "def insert(\n        self,\n        loc: int,\n        column: Hashable,\n        value: Scalar | AnyArrayLike,\n        allow_duplicates: bool | lib.NoDefault = lib.no_default,\n    ) -> None:\n        \n        if allow_duplicates is lib.no_default:\n            allow_duplicates = False\n        if allow_duplicates and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'allow_duplicates=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n        if not allow_duplicates and column in self.columns:\n            # Should this be a different kind of error??\n            raise ValueError(f\"cannot insert {column}, already exists\")\n        if not is_integer(loc):\n            raise TypeError(\"loc must be int\")\n        # convert non stdlib ints to satisfy typing checks\n        loc = int(loc)\n        if isinstance(value, DataFrame) and len(value.columns) > 1:\n            raise ValueError(\n                f\"Expected a one-dimensional object, got a DataFrame with \"\n                f\"{len(value.columns)} columns instead.\"\n            )\n        elif isinstance(value, DataFrame):\n            value = value.iloc[:, 0]\n\n        value, refs = self._sanitize_column(value)\n        self._mgr.insert(loc, column, value, refs=refs)"
    },
    {
        "name": "DataFrame.keys",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.keys.html",
        "description": "Get the 'info axis' (see Indexing for more).  This is index for Series, columns for DataFrame.  Returns ------- Index Info axis.  Examples -------- >>> d = pd.DataFrame(data={'A': [1, 2, 3], 'B': [0, 4, 8]}, ...                  index=['a', 'b', 'c']) >>> d A  B a  1  0 b  2  4 c  3  8 >>> d.keys() Index(['A', 'B'], dtype='object')",
        "source": "def keys(self) -> Index:\n        \n        return self._info_axis"
    },
    {
        "name": "DataFrame.pop",
        "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pop.html",
        "description": "Return item and drop from frame. Raise KeyError if not found.  Parameters ---------- item : label Label of column to be popped.  Returns ------- Series  Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ...                    ('parrot', 'bird', 24.0), ...                    ('lion', 'mammal', 80.5), ...                    ('monkey', 'mammal', np.nan)], ...                   columns=('name', 'class', 'max_speed')) >>> df name   class  max_speed 0  falcon    bird      389.0 1  parrot    bird       24.0 2    lion  mammal       80.5 3  monkey  mammal        NaN  >>> df.pop('class') 0      bird 1      bird 2    mammal 3    mammal Name: class, dtype: object  >>> df name  max_speed 0  falcon      389.0 1  parrot       24.0 2    lion       80.5 3  monkey        NaN",
        "source": "def pop(self, item: Hashable) -> Series:\n        \n        return super().pop(item=item)"
    }
]