[
    {
        "name": "numpy.ptp",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.ptp.html",
        "description": "Range of values (maximum - minimum) along an axis.  The name of the function comes from the acronym for 'peak to peak'.  .. warning:: `ptp` preserves the data type of the array. This means the return value for an input of signed integers with n bits (e.g. `np.int8`, `np.int16`, etc) is also a signed integer with n bits.  In that case, peak-to-peak values greater than ``2**(n-1)-1`` will be returned as negative values. An example with a work-around is shown below.  Parameters ---------- a : array_like Input values. axis : None or int or tuple of ints, optional Axis along which to find the peaks.  By default, flatten the array.  `axis` may be negative, in which case it counts from the last to the first axis.  .. versionadded:: 1.15.0  If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before. out : array_like Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type of the output values will be cast if necessary.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `ptp` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  Returns ------- ptp : ndarray or scalar The range of a given array - `scalar` if array is one-dimensional or a new array holding the result along the given axis  Examples -------- >>> x = np.array([[4, 9, 2, 10], ...               [6, 9, 7, 12]])  >>> np.ptp(x, axis=1) array([8, 6])  >>> np.ptp(x, axis=0) array([2, 0, 5, 2])  >>> np.ptp(x) 10  This example shows that a negative value can be returned when the input is an array of signed integers.  >>> y = np.array([[1, 127], ...               [0, 127], ...               [-1, 127], ...               [-2, 127]], dtype=np.int8) >>> np.ptp(y, axis=1) array([ 126,  127, -128, -127], dtype=int8)  A work-around is to use the `view()` method to view the result as unsigned integers with the same bit width:  >>> np.ptp(y, axis=1).view(np.uint8) array([126, 127, 128, 129], dtype=uint8)",
        "source": "@array_function_dispatch(_ptp_dispatcher)\ndef ptp(a, axis=None, out=None, keepdims=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if type(a) is not mu.ndarray:\n        try:\n            ptp = a.ptp\n        except AttributeError:\n            pass\n        else:\n            return ptp(axis=axis, out=out, **kwargs)\n    return _methods._ptp(a, axis=axis, out=out, **kwargs)"
    },
    {
        "name": "numpy.percentile",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.percentile.html",
        "description": "Compute the q-th percentile of the data along the specified axis.  Returns the q-th percentile(s) of the array elements.  Parameters ---------- a : array_like of real numbers Input array or object that can be converted to an array. q : array_like of float Percentage or sequence of percentages for the percentiles to compute. Values must be between 0 and 100 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array.  .. versionchanged:: 1.9.0 A tuple of axes is supported out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the percentile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  .. versionadded:: 1.9.0  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- percentile : scalar or ndarray If `q` is a single percentile and `axis=None`, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean median : equivalent to ``percentile(..., 50)`` nanpercentile quantile : equivalent to percentile, except q in the range [0, 1].  Notes ----- Given a vector ``V`` of length ``n``, the q-th percentile of ``V`` is the value ``q/100`` of the way from the minimum to the maximum in a sorted copy of ``V``. The values and distances of the two nearest neighbors as well as the `method` parameter will determine the percentile if the normalized ranking does not match the location of ``q`` exactly. This function is the same as the median if ``q=50``, the same as the minimum if ``q=0`` and the same as the maximum if ``q=100``.  The optional `method` parameter specifies the method to use when the desired percentile lies between two indexes ``i`` and ``j = i + 1``. In that case, we first determine ``i + g``, a virtual index that lies between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the fractional part of the index. The final result is, then, an interpolation of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``, ``i`` and ``j`` are modified using correction constants ``alpha`` and ``beta`` whose choices depend on the ``method`` used. Finally, note that since Python uses 0-based indexing, the code subtracts another 1 from the index internally.  The following formula determines the virtual index ``i + g``, the location of the percentile in the sorted sample:  .. math:: i + g = (q / 100) * ( n - alpha - beta + 1 ) + alpha  The different methods then work as follows  inverted_cdf: method 1 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then take i  averaged_inverted_cdf: method 2 of H&F [1]_. This method give discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then average between bounds  closest_observation: method 3 of H&F [1]_. This method give discontinuous results:  * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i  interpolated_inverted_cdf: method 4 of H&F [1]_. This method give continuous results using:  * alpha = 0 * beta = 1  hazen: method 5 of H&F [1]_. This method give continuous results using:  * alpha = 1/2 * beta = 1/2  weibull: method 6 of H&F [1]_. This method give continuous results using:  * alpha = 0 * beta = 0  linear: method 7 of H&F [1]_. This method give continuous results using:  * alpha = 1 * beta = 1  median_unbiased: method 8 of H&F [1]_. This method is probably the best method if the sample distribution function is unknown (see reference). This method give continuous results using:  * alpha = 1/3 * beta = 1/3  normal_unbiased: method 9 of H&F [1]_. This method is probably the best method if the sample distribution function is known to be normal. This method give continuous results using:  * alpha = 3/8 * beta = 3/8  lower: NumPy method kept for backwards compatibility. Takes ``i`` as the interpolation point.  higher: NumPy method kept for backwards compatibility. Takes ``j`` as the interpolation point.  nearest: NumPy method kept for backwards compatibility. Takes ``i`` or ``j``, whichever is nearest.  midpoint: NumPy method kept for backwards compatibility. Uses ``(i + j) / 2``.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.percentile(a, 50) 3.5 >>> np.percentile(a, 50, axis=0) array([6.5, 4.5, 2.5]) >>> np.percentile(a, 50, axis=1) array([7.,  2.]) >>> np.percentile(a, 50, axis=1, keepdims=True) array([[7.], [2.]])  >>> m = np.percentile(a, 50, axis=0) >>> out = np.zeros_like(m) >>> np.percentile(a, 50, axis=0, out=out) array([6.5, 4.5, 2.5]) >>> m array([6.5, 4.5, 2.5])  >>> b = a.copy() >>> np.percentile(b, 50, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a == b)  The different methods can be visualized graphically:  .. plot::  import matplotlib.pyplot as plt  a = np.arange(4) p = np.linspace(0, 100, 6001) ax = plt.gca() lines = [ ('linear', '-', 'C0'), ('inverted_cdf', ':', 'C1'), # Almost the same as `inverted_cdf`: ('averaged_inverted_cdf', '-.', 'C1'), ('closest_observation', ':', 'C2'), ('interpolated_inverted_cdf', '--', 'C1'), ('hazen', '--', 'C3'), ('weibull', '-.', 'C4'), ('median_unbiased', '--', 'C5'), ('normal_unbiased', '-.', 'C6'), ] for method, style, color in lines: ax.plot( p, np.percentile(a, p, method=method), label=method, linestyle=style, color=color) ax.set( title='Percentiles for different methods and data: ' + str(a), xlabel='Percentile', ylabel='Estimated percentile value', yticks=a) ax.legend(bbox_to_anchor=(1.03, 1)) plt.tight_layout() plt.show()  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
        "source": "@array_function_dispatch(_percentile_dispatcher)\ndef percentile(a,\n               q,\n               axis=None,\n               out=None,\n               overwrite_input=False,\n               method=\"linear\",\n               keepdims=False,\n               *,\n               interpolation=None):\n    \n    if interpolation is not None:\n        method = _check_interpolation_as_method(\n            method, interpolation, \"percentile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.true_divide(q, 100)\n    q = asanyarray(q)  # undo any decay that the ufunc performed (see gh-13105)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Percentiles must be in the range [0, 100]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
    },
    {
        "name": "numpy.nanpercentile",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.nanpercentile.html",
        "description": "Compute the qth percentile of the data along the specified axis, while ignoring nan values.  Returns the qth percentile(s) of the array elements.  .. versionadded:: 1.9.0  Parameters ---------- a : array_like Input array or object that can be converted to an array, containing nan values to be ignored. q : array_like of float Percentile or sequence of percentiles to compute, which must be between 0 and 100 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the percentile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  If this is anything but the default value it will be passed through (in the special case of an empty array) to the `mean` function of the underlying array.  If the array is a sub-class and `mean` does not have the kwarg `keepdims` this will raise a RuntimeError.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- percentile : scalar or ndarray If `q` is a single percentile and `axis=None`, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- nanmean nanmedian : equivalent to ``nanpercentile(..., 50)`` percentile, median, mean nanquantile : equivalent to nanpercentile, except q in range [0, 1].  Notes ----- For more information please see `numpy.percentile`  Examples -------- >>> a = np.array([[10., 7., 4.], [3., 2., 1.]]) >>> a[0][1] = np.nan >>> a array([[10.,  nan,   4.], [ 3.,   2.,   1.]]) >>> np.percentile(a, 50) nan >>> np.nanpercentile(a, 50) 3.0 >>> np.nanpercentile(a, 50, axis=0) array([6.5, 2. , 2.5]) >>> np.nanpercentile(a, 50, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.nanpercentile(a, 50, axis=0) >>> out = np.zeros_like(m) >>> np.nanpercentile(a, 50, axis=0, out=out) array([6.5, 2. , 2.5]) >>> m array([6.5,  2. ,  2.5])  >>> b = a.copy() >>> np.nanpercentile(b, 50, axis=1, overwrite_input=True) array([7., 2.]) >>> assert not np.all(a==b)  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
        "source": "@array_function_dispatch(_nanpercentile_dispatcher)\ndef nanpercentile(\n        a,\n        q,\n        axis=None,\n        out=None,\n        overwrite_input=False,\n        method=\"linear\",\n        keepdims=np._NoValue,\n        *,\n        interpolation=None,\n):\n    \n    if interpolation is not None:\n        method = function_base._check_interpolation_as_method(\n            method, interpolation, \"nanpercentile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.true_divide(q, 100.0)\n    # undo any decay that the ufunc performed (see gh-13105)\n    q = np.asanyarray(q)\n    if not function_base._quantile_is_valid(q):\n        raise ValueError(\"Percentiles must be in the range [0, 100]\")\n    return _nanquantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
    },
    {
        "name": "numpy.quantile",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.quantile.html",
        "description": "Compute the q-th quantile of the data along the specified axis.  .. versionadded:: 1.15.0  Parameters ---------- a : array_like of real numbers Input array or object that can be converted to an array. q : array_like of float Probability or sequence of probabilities for the quantiles to compute. Values must be between 0 and 1 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the quantile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- quantile : scalar or ndarray If `q` is a single probability and `axis=None`, then the result is a scalar. If multiple probabilies levels are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean percentile : equivalent to quantile, but with q in the range [0, 100]. median : equivalent to ``quantile(..., 0.5)`` nanquantile  Notes ----- Given a vector ``V`` of length ``n``, the q-th quantile of ``V`` is the value ``q`` of the way from the minimum to the maximum in a sorted copy of ``V``. The values and distances of the two nearest neighbors as well as the `method` parameter will determine the quantile if the normalized ranking does not match the location of ``q`` exactly. This function is the same as the median if ``q=0.5``, the same as the minimum if ``q=0.0`` and the same as the maximum if ``q=1.0``.  The optional `method` parameter specifies the method to use when the desired quantile lies between two indexes ``i`` and ``j = i + 1``. In that case, we first determine ``i + g``, a virtual index that lies between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the fractional part of the index. The final result is, then, an interpolation of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``, ``i`` and ``j`` are modified using correction constants ``alpha`` and ``beta`` whose choices depend on the ``method`` used. Finally, note that since Python uses 0-based indexing, the code subtracts another 1 from the index internally.  The following formula determines the virtual index ``i + g``, the location of the quantile in the sorted sample:  .. math:: i + g = q * ( n - alpha - beta + 1 ) + alpha  The different methods then work as follows  inverted_cdf: method 1 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then take i  averaged_inverted_cdf: method 2 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then average between bounds  closest_observation: method 3 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i  interpolated_inverted_cdf: method 4 of H&F [1]_. This method gives continuous results using:  * alpha = 0 * beta = 1  hazen: method 5 of H&F [1]_. This method gives continuous results using:  * alpha = 1/2 * beta = 1/2  weibull: method 6 of H&F [1]_. This method gives continuous results using:  * alpha = 0 * beta = 0  linear: method 7 of H&F [1]_. This method gives continuous results using:  * alpha = 1 * beta = 1  median_unbiased: method 8 of H&F [1]_. This method is probably the best method if the sample distribution function is unknown (see reference). This method gives continuous results using:  * alpha = 1/3 * beta = 1/3  normal_unbiased: method 9 of H&F [1]_. This method is probably the best method if the sample distribution function is known to be normal. This method gives continuous results using:  * alpha = 3/8 * beta = 3/8  lower: NumPy method kept for backwards compatibility. Takes ``i`` as the interpolation point.  higher: NumPy method kept for backwards compatibility. Takes ``j`` as the interpolation point.  nearest: NumPy method kept for backwards compatibility. Takes ``i`` or ``j``, whichever is nearest.  midpoint: NumPy method kept for backwards compatibility. Uses ``(i + j) / 2``.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.quantile(a, 0.5) 3.5 >>> np.quantile(a, 0.5, axis=0) array([6.5, 4.5, 2.5]) >>> np.quantile(a, 0.5, axis=1) array([7.,  2.]) >>> np.quantile(a, 0.5, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.quantile(a, 0.5, axis=0) >>> out = np.zeros_like(m) >>> np.quantile(a, 0.5, axis=0, out=out) array([6.5, 4.5, 2.5]) >>> m array([6.5, 4.5, 2.5]) >>> b = a.copy() >>> np.quantile(b, 0.5, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a == b)  See also `numpy.percentile` for a visualization of most methods.  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
        "source": "@array_function_dispatch(_quantile_dispatcher)\ndef quantile(a,\n             q,\n             axis=None,\n             out=None,\n             overwrite_input=False,\n             method=\"linear\",\n             keepdims=False,\n             *,\n             interpolation=None):\n    \n    if interpolation is not None:\n        method = _check_interpolation_as_method(\n            method, interpolation, \"quantile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.asanyarray(q)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
    },
    {
        "name": "numpy.nanquantile",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.nanquantile.html",
        "description": "Compute the qth quantile of the data along the specified axis, while ignoring nan values. Returns the qth quantile(s) of the array elements.  .. versionadded:: 1.15.0  Parameters ---------- a : array_like Input array or object that can be converted to an array, containing nan values to be ignored q : array_like of float Probability or sequence of probabilities for the quantiles to compute. Values must be between 0 and 1 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the quantile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  If this is anything but the default value it will be passed through (in the special case of an empty array) to the `mean` function of the underlying array.  If the array is a sub-class and `mean` does not have the kwarg `keepdims` this will raise a RuntimeError.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- quantile : scalar or ndarray If `q` is a single probability and `axis=None`, then the result is a scalar. If multiple probability levels are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- quantile nanmean, nanmedian nanmedian : equivalent to ``nanquantile(..., 0.5)`` nanpercentile : same as nanquantile, but with q in the range [0, 100].  Notes ----- For more information please see `numpy.quantile`  Examples -------- >>> a = np.array([[10., 7., 4.], [3., 2., 1.]]) >>> a[0][1] = np.nan >>> a array([[10.,  nan,   4.], [ 3.,   2.,   1.]]) >>> np.quantile(a, 0.5) nan >>> np.nanquantile(a, 0.5) 3.0 >>> np.nanquantile(a, 0.5, axis=0) array([6.5, 2. , 2.5]) >>> np.nanquantile(a, 0.5, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.nanquantile(a, 0.5, axis=0) >>> out = np.zeros_like(m) >>> np.nanquantile(a, 0.5, axis=0, out=out) array([6.5, 2. , 2.5]) >>> m array([6.5,  2. ,  2.5]) >>> b = a.copy() >>> np.nanquantile(b, 0.5, axis=1, overwrite_input=True) array([7., 2.]) >>> assert not np.all(a==b)  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
        "source": "@array_function_dispatch(_nanquantile_dispatcher)\ndef nanquantile(\n        a,\n        q,\n        axis=None,\n        out=None,\n        overwrite_input=False,\n        method=\"linear\",\n        keepdims=np._NoValue,\n        *,\n        interpolation=None,\n):\n    \n\n    if interpolation is not None:\n        method = function_base._check_interpolation_as_method(\n            method, interpolation, \"nanquantile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.asanyarray(q)\n    if not function_base._quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n    return _nanquantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
    },
    {
        "name": "numpy.median",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.median.html",
        "description": "Compute the median along the specified axis.  Returns the median of the array elements.  Parameters ---------- a : array_like Input array or object that can be converted to an array. axis : {int, sequence of int, None}, optional Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow use of memory of input array `a` for calculations. The input array will be modified by the call to `median`. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If `overwrite_input` is ``True`` and `a` is not already an `ndarray`, an error will be raised. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`.  .. versionadded:: 1.9.0  Returns ------- median : ndarray A new array holding the result. If the input contains integers or floats smaller than ``float64``, then the output data-type is ``np.float64``.  Otherwise, the data-type of the output is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean, percentile  Notes ----- Given a vector ``V`` of length ``N``, the median of ``V`` is the middle value of a sorted copy of ``V``, ``V_sorted`` - i e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the two middle values of ``V_sorted`` when ``N`` is even.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.median(a) 3.5 >>> np.median(a, axis=0) array([6.5, 4.5, 2.5]) >>> np.median(a, axis=1) array([7.,  2.]) >>> m = np.median(a, axis=0) >>> out = np.zeros_like(m) >>> np.median(a, axis=0, out=m) array([6.5,  4.5,  2.5]) >>> m array([6.5,  4.5,  2.5]) >>> b = a.copy() >>> np.median(b, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a==b) >>> b = a.copy() >>> np.median(b, axis=None, overwrite_input=True) 3.5 >>> assert not np.all(a==b)",
        "source": "@array_function_dispatch(_median_dispatcher)\ndef median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n    \n    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n                    overwrite_input=overwrite_input)"
    },
    {
        "name": "numpy.average",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.average.html",
        "description": "Compute the weighted average along the specified axis.  Parameters ---------- a : array_like Array containing data to be averaged. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which to average `a`.  The default, axis=None, will average over all of the elements of the input array. If axis is negative it counts from the last to the first axis.  .. versionadded:: 1.7.0  If axis is a tuple of ints, averaging is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before. weights : array_like, optional An array of weights associated with the values in `a`. Each value in `a` contributes to the average according to its associated weight. The weights array can either be 1-D (in which case its length must be the size of `a` along the given axis) or of the same shape as `a`. If `weights=None`, then all data in `a` are assumed to have a weight equal to one.  The 1-D calculation is::  avg = sum(a * weights) / sum(weights)  The only constraint on `weights` is that `sum(weights)` must not be 0. returned : bool, optional Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`) is returned, otherwise only the average is returned. If `weights=None`, `sum_of_weights` is equivalent to the number of elements over which the average is taken. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `a`. *Note:* `keepdims` will not work with instances of `numpy.matrix` or other classes whose methods do not support `keepdims`.  .. versionadded:: 1.23.0  Returns ------- retval, [sum_of_weights] : array_type or double Return the average along the specified axis. When `returned` is `True`, return a tuple with the average as the first element and the sum of the weights as the second element. `sum_of_weights` is of the same type as `retval`. The result dtype follows a genereal pattern. If `weights` is None, the result dtype will be that of `a` , or ``float64`` if `a` is integral. Otherwise, if `weights` is not None and `a` is non- integral, the result type will be the type of lowest precision capable of representing values of both `a` and `weights`. If `a` happens to be integral, the previous rules still applies but the result dtype will at least be ``float64``.  Raises ------ ZeroDivisionError When all weights along axis are zero. See `numpy.ma.average` for a version robust to this type of error. TypeError When the length of 1D `weights` is not the same as the shape of `a` along axis.  See Also -------- mean  ma.average : average for masked arrays -- useful if your data contains \"missing\" values numpy.result_type : Returns the type that results from applying the numpy type promotion rules to the arguments.  Examples -------- >>> data = np.arange(1, 5) >>> data array([1, 2, 3, 4]) >>> np.average(data) 2.5 >>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1)) 4.0  >>> data = np.arange(6).reshape((3, 2)) >>> data array([[0, 1], [2, 3], [4, 5]]) >>> np.average(data, axis=1, weights=[1./4, 3./4]) array([0.75, 2.75, 4.75]) >>> np.average(data, weights=[1./4, 3./4]) Traceback (most recent call last): ... TypeError: Axis must be specified when shapes of a and weights differ.  >>> a = np.ones(5, dtype=np.float128) >>> w = np.ones(5, dtype=np.complex64) >>> avg = np.average(a, weights=w) >>> print(avg.dtype) complex256  With ``keepdims=True``, the following result has shape (3, 1).  >>> np.average(data, axis=1, keepdims=True) array([[0.5], [2.5], [4.5]])",
        "source": "@array_function_dispatch(_average_dispatcher)\ndef average(a, axis=None, weights=None, returned=False, *,\n            keepdims=np._NoValue):\n    \n    a = np.asanyarray(a)\n\n    if keepdims is np._NoValue:\n        # Don't pass on the keepdims argument if one wasn't given.\n        keepdims_kw = {}\n    else:\n        keepdims_kw = {'keepdims': keepdims}\n\n    if weights is None:\n        avg = a.mean(axis, **keepdims_kw)\n        avg_as_array = np.asanyarray(avg)\n        scl = avg_as_array.dtype.type(a.size/avg_as_array.size)\n    else:\n        wgt = np.asanyarray(weights)\n\n        if issubclass(a.dtype.type, (np.integer, np.bool_)):\n            result_dtype = np.result_type(a.dtype, wgt.dtype, 'f8')\n        else:\n            result_dtype = np.result_type(a.dtype, wgt.dtype)\n\n        # Sanity checks\n        if a.shape != wgt.shape:\n            if axis is None:\n                raise TypeError(\n                    \"Axis must be specified when shapes of a and weights \"\n                    \"differ.\")\n            if wgt.ndim != 1:\n                raise TypeError(\n                    \"1D weights expected when shapes of a and weights differ.\")\n            if wgt.shape[0] != a.shape[axis]:\n                raise ValueError(\n                    \"Length of weights not compatible with specified axis.\")\n\n            # setup wgt to broadcast along axis\n            wgt = np.broadcast_to(wgt, (a.ndim-1)*(1,) + wgt.shape)\n            wgt = wgt.swapaxes(-1, axis)\n\n        scl = wgt.sum(axis=axis, dtype=result_dtype, **keepdims_kw)\n        if np.any(scl == 0.0):\n            raise ZeroDivisionError(\n                \"Weights sum to zero, can't be normalized\")\n\n        avg = avg_as_array = np.multiply(a, wgt,\n                          dtype=result_dtype).sum(axis, **keepdims_kw) / scl\n\n    if returned:\n        if scl.shape != avg_as_array.shape:\n            scl = np.broadcast_to(scl, avg_as_array.shape).copy()\n        return avg, scl\n    else:\n        return avg"
    },
    {
        "name": "numpy.mean",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.mean.html",
        "description": "Compute the arithmetic mean along the specified axis.  Returns the average of the array elements.  The average is taken over the flattened array by default, otherwise over the specified axis. `float64` intermediate and return values are used for integer inputs.  Parameters ---------- a : array_like Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the mean.  For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype. out : ndarray, optional Alternate output array in which to place the result.  The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See :ref:`ufuncs-output-type` for more details.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `mean` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the mean. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- m : ndarray, see dtype parameter above If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned.  See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar  Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements.  Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue.  By default, `float16` results are computed using `float32` intermediates for extra precision.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.mean(a) 2.5 >>> np.mean(a, axis=0) array([2., 3.]) >>> np.mean(a, axis=1) array([1.5, 3.5])  In single precision, `mean` can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.mean(a) 0.54999924  Computing the mean in float64 is more accurate:  >>> np.mean(a, dtype=np.float64) 0.55000000074505806 # may vary  Specifying a where argument:  >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]]) >>> np.mean(a) 12.0 >>> np.mean(a, where=[[True], [False], [False]]) 9.0",
        "source": "@array_function_dispatch(_mean_dispatcher)\ndef mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n         where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            mean = a.mean\n        except AttributeError:\n            pass\n        else:\n            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\n    return _methods._mean(a, axis=axis, dtype=dtype,\n                          out=out, **kwargs)"
    },
    {
        "name": "numpy.std",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.std.html",
        "description": "Compute the standard deviation along the specified axis.  Returns the standard deviation, a measure of the spread of a distribution, of the array elements. The standard deviation is computed for the flattened array by default, otherwise over the specified axis.  Parameters ---------- a : array_like Calculate the standard deviation of these values. axis : None or int or tuple of ints, optional Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single axis or all the axes as before. dtype : dtype, optional Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary. ddof : int, optional Means Delta Degrees of Freedom.  The divisor used in calculations is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `std` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the standard deviation. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- standard_deviation : ndarray, see dtype parameter above. If `out` is None, return a new array containing the standard deviation, otherwise return a reference to the output array.  See Also -------- var, mean, nanmean, nanstd, nanvar :ref:`ufuncs-output-type`  Notes ----- The standard deviation is the square root of the average of the squared deviations from the mean, i.e., ``std = sqrt(mean(x))``, where ``x = abs(a - a.mean())**2``.  The average squared deviation is typically calculated as ``x.sum() / N``, where ``N = len(x)``. If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of the infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ``ddof=1``, it will not be an unbiased estimate of the standard deviation per se.  Note that, for complex numbers, `std` takes the absolute value before squaring, so that the result is always real and nonnegative.  For floating-point input, the *std* is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the `dtype` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.std(a) 1.1180339887498949 # may vary >>> np.std(a, axis=0) array([1.,  1.]) >>> np.std(a, axis=1) array([0.5,  0.5])  In single precision, std() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.std(a) 0.45000005  Computing the standard deviation in float64 is more accurate:  >>> np.std(a, dtype=np.float64) 0.44999999925494177 # may vary  Specifying a where argument:  >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]]) >>> np.std(a) 2.614064523559687 # may vary >>> np.std(a, where=[[True], [True], [False]]) 2.0",
        "source": "@array_function_dispatch(_std_dispatcher)\ndef std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            std = a.std\n        except AttributeError:\n            pass\n        else:\n            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)"
    },
    {
        "name": "numpy.var",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.var.html",
        "description": "Compute the variance along the specified axis.  Returns the variance of the array elements, a measure of the spread of a distribution.  The variance is computed for the flattened array by default, otherwise over the specified axis.  Parameters ---------- a : array_like Array containing numbers whose variance is desired.  If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the variance is computed.  The default is to compute the variance of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the variance.  For arrays of integer type the default is `float64`; for arrays of float types it is the same as the array type. out : ndarray, optional Alternate output array in which to place the result.  It must have the same shape as the expected output, but the type is cast if necessary. ddof : int, optional \"Delta Degrees of Freedom\": the divisor used in the calculation is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `var` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the variance. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- variance : ndarray, see dtype parameter above If ``out=None``, returns a new array containing the variance; otherwise, a reference to the output array is returned.  See Also -------- std, mean, nanmean, nanstd, nanvar :ref:`ufuncs-output-type`  Notes ----- The variance is the average of the squared deviations from the mean, i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.  The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``. If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead.  In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of a hypothetical infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables.  Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.  For floating-point input, the variance is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-accuracy accumulator using the ``dtype`` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.var(a) 1.25 >>> np.var(a, axis=0) array([1.,  1.]) >>> np.var(a, axis=1) array([0.25,  0.25])  In single precision, var() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.var(a) 0.20250003  Computing the variance in float64 is more accurate:  >>> np.var(a, dtype=np.float64) 0.20249999932944759 # may vary >>> ((1-0.55)**2 + (0.1-0.55)**2)/2 0.2025  Specifying a where argument:  >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]]) >>> np.var(a) 6.833333333333333 # may vary >>> np.var(a, where=[[True], [True], [False]]) 4.0",
        "source": "@array_function_dispatch(_var_dispatcher)\ndef var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n\n    if type(a) is not mu.ndarray:\n        try:\n            var = a.var\n\n        except AttributeError:\n            pass\n        else:\n            return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)"
    },
    {
        "name": "numpy.corrcoef",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html",
        "description": "Return Pearson product-moment correlation coefficients.  Please refer to the documentation for `cov` for more detail.  The relationship between the correlation coefficient matrix, `R`, and the covariance matrix, `C`, is  .. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }  The values of `R` are between -1 and 1, inclusive.  Parameters ---------- x : array_like A 1-D or 2-D array containing multiple variables and observations. Each row of `x` represents a variable, and each column a single observation of all those variables. Also see `rowvar` below. y : array_like, optional An additional set of variables and observations. `y` has the same shape as `x`. rowvar : bool, optional If `rowvar` is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations. bias : _NoValue, optional Has no effect, do not use.  .. deprecated:: 1.10.0 ddof : _NoValue, optional Has no effect, do not use.  .. deprecated:: 1.10.0 dtype : data-type, optional Data-type of the result. By default, the return data-type will have at least `numpy.float64` precision.  .. versionadded:: 1.20  Returns ------- R : ndarray The correlation coefficient matrix of the variables.  See Also -------- cov : Covariance matrix  Notes ----- Due to floating point rounding the resulting array may not be Hermitian, the diagonal elements may not be 1, and the elements may not satisfy the inequality abs(a) <= 1. The real and imaginary parts are clipped to the interval [-1,  1] in an attempt to improve on that situation but is not much help in the complex case.  This function accepts but discards arguments `bias` and `ddof`.  This is for backwards compatibility with previous versions of this function.  These arguments had no effect on the return values of the function and can be safely ignored in this and previous versions of numpy.  Examples -------- In this example we generate two random arrays, ``xarr`` and ``yarr``, and compute the row-wise and column-wise Pearson correlation coefficients, ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise Pearson correlation coefficients between the variables of ``xarr``.  >>> import numpy as np >>> rng = np.random.default_rng(seed=42) >>> xarr = rng.random((3, 3)) >>> xarr array([[0.77395605, 0.43887844, 0.85859792], [0.69736803, 0.09417735, 0.97562235], [0.7611397 , 0.78606431, 0.12811363]]) >>> R1 = np.corrcoef(xarr) >>> R1 array([[ 1.        ,  0.99256089, -0.68080986], [ 0.99256089,  1.        , -0.76492172], [-0.68080986, -0.76492172,  1.        ]])  If we add another set of variables and observations ``yarr``, we can compute the row-wise Pearson correlation coefficients between the variables in ``xarr`` and ``yarr``.  >>> yarr = rng.random((3, 3)) >>> yarr array([[0.45038594, 0.37079802, 0.92676499], [0.64386512, 0.82276161, 0.4434142 ], [0.22723872, 0.55458479, 0.06381726]]) >>> R2 = np.corrcoef(xarr, yarr) >>> R2 array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  , -0.99004057], [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098, -0.99981569], [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355, 0.77714685], [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855, -0.83571711], [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        , 0.97517215], [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215, 1.        ]])  Finally if we use the option ``rowvar=False``, the columns are now being treated as the variables and we will find the column-wise Pearson correlation coefficients between variables in ``xarr`` and ``yarr``.  >>> R3 = np.corrcoef(xarr, yarr, rowvar=False) >>> R3 array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 , 0.22423734], [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587, -0.44069024], [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648, 0.75137473], [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469, 0.47536961], [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        , -0.46666491], [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491, 1.        ]])",
        "source": "@array_function_dispatch(_corrcoef_dispatcher)\ndef corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,\n             dtype=None):\n    \n    if bias is not np._NoValue or ddof is not np._NoValue:\n        # 2015-03-15, 1.10\n        warnings.warn('bias and ddof have no effect and are deprecated',\n                      DeprecationWarning, stacklevel=2)\n    c = cov(x, y, rowvar, dtype=dtype)\n    try:\n        d = diag(c)\n    except ValueError:\n        # scalar covariance\n        # nan if incorrect value (nan, inf, 0), 1 otherwise\n        return c / c\n    stddev = sqrt(d.real)\n    c /= stddev[:, None]\n    c /= stddev[None, :]\n\n    # Clip real and imaginary parts to [-1, 1].  This does not guarantee\n    # abs(a[i,j]) <= 1 for complex arrays, but is the best we can do without\n    # excessive work.\n    np.clip(c.real, -1, 1, out=c.real)\n    if np.iscomplexobj(c):\n        np.clip(c.imag, -1, 1, out=c.imag)\n\n    return c"
    },
    {
        "name": "numpy.correlate",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.correlate.html",
        "description": "Cross-correlation of two 1-dimensional sequences.  This function computes the correlation as generally defined in signal processing texts:  .. math:: c_k = \\sum_n a_{n+k} \\cdot \\overline{v}_n  with a and v sequences being zero-padded where necessary and :math:`\\overline x` denoting complex conjugation.  Parameters ---------- a, v : array_like Input sequences. mode : {'valid', 'same', 'full'}, optional Refer to the `convolve` docstring.  Note that the default is 'valid', unlike `convolve`, which uses 'full'. old_behavior : bool `old_behavior` was removed in NumPy 1.10. If you need the old behavior, use `multiarray.correlate`.  Returns ------- out : ndarray Discrete cross-correlation of `a` and `v`.  See Also -------- convolve : Discrete, linear convolution of two one-dimensional sequences. multiarray.correlate : Old, no conjugate, version of correlate. scipy.signal.correlate : uses FFT which has superior performance on large arrays.  Notes ----- The definition of correlation above is not unique and sometimes correlation may be defined differently. Another common definition is:  .. math:: c'_k = \\sum_n a_{n} \\cdot \\overline{v_{n+k}}  which is related to :math:`c_k` by :math:`c'_k = c_{-k}`.  `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might be preferable.   Examples -------- >>> np.correlate([1, 2, 3], [0, 1, 0.5]) array([3.5]) >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\") array([2. ,  3.5,  3. ]) >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\") array([0.5,  2. ,  3.5,  3. ,  0. ])  Using complex sequences:  >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full') array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])  Note that you get the time reversed, complex conjugated result (:math:`\\overline{c_{-k}}`) when the two input sequences a and v change places:  >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full') array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])",
        "source": "@array_function_dispatch(_correlate_dispatcher)\ndef correlate(a, v, mode='valid'):\n    \n    return multiarray.correlate2(a, v, mode)"
    },
    {
        "name": "numpy.cov",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.cov.html",
        "description": "Estimate a covariance matrix, given data and weights.  Covariance indicates the level to which two variables vary together. If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`, then the covariance matrix element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance of :math:`x_i`.  See the notes for an outline of the algorithm.  Parameters ---------- m : array_like A 1-D or 2-D array containing multiple variables and observations. Each row of `m` represents a variable, and each column a single observation of all those variables. Also see `rowvar` below. y : array_like, optional An additional set of variables and observations. `y` has the same form as that of `m`. rowvar : bool, optional If `rowvar` is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations. bias : bool, optional Default normalization (False) is by ``(N - 1)``, where ``N`` is the number of observations given (unbiased estimate). If `bias` is True, then normalization is by ``N``. These values can be overridden by using the keyword ``ddof`` in numpy versions >= 1.5. ddof : int, optional If not ``None`` the default value implied by `bias` is overridden. Note that ``ddof=1`` will return the unbiased estimate, even if both `fweights` and `aweights` are specified, and ``ddof=0`` will return the simple average. See the notes for the details. The default value is ``None``.  .. versionadded:: 1.5 fweights : array_like, int, optional 1-D array of integer frequency weights; the number of times each observation vector should be repeated.  .. versionadded:: 1.10 aweights : array_like, optional 1-D array of observation vector weights. These relative weights are typically large for observations considered \"important\" and smaller for observations considered less \"important\". If ``ddof=0`` the array of weights can be used to assign probabilities to observation vectors.  .. versionadded:: 1.10 dtype : data-type, optional Data-type of the result. By default, the return data-type will have at least `numpy.float64` precision.  .. versionadded:: 1.20  Returns ------- out : ndarray The covariance matrix of the variables.  See Also -------- corrcoef : Normalized covariance matrix  Notes ----- Assume that the observations are in the columns of the observation array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The steps to compute the weighted covariance are as follows::  >>> m = np.arange(10, dtype=np.float64) >>> f = np.arange(10) * 2 >>> a = np.arange(10) ** 2. >>> ddof = 1 >>> w = f * a >>> v1 = np.sum(w) >>> v2 = np.sum(w * a) >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1 >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)  Note that when ``a == 1``, the normalization factor ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)`` as it should.  Examples -------- Consider two variables, :math:`x_0` and :math:`x_1`, which correlate perfectly, but in opposite directions:  >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T >>> x array([[0, 1, 2], [2, 1, 0]])  Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance matrix shows this clearly:  >>> np.cov(x) array([[ 1., -1.], [-1.,  1.]])  Note that element :math:`C_{0,1}`, which shows the correlation between :math:`x_0` and :math:`x_1`, is negative.  Further, note how `x` and `y` are combined:  >>> x = [-2.1, -1,  4.3] >>> y = [3,  1.1,  0.12] >>> X = np.stack((x, y), axis=0) >>> np.cov(X) array([[11.71      , -4.286     ], # may vary [-4.286     ,  2.144133]]) >>> np.cov(x, y) array([[11.71      , -4.286     ], # may vary [-4.286     ,  2.144133]]) >>> np.cov(x) array(11.71)",
        "source": "@array_function_dispatch(_cov_dispatcher)\ndef cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,\n        aweights=None, *, dtype=None):\n    \n    # Check inputs\n    if ddof is not None and ddof != int(ddof):\n        raise ValueError(\n            \"ddof must be integer\")\n\n    # Handles complex arrays too\n    m = np.asarray(m)\n    if m.ndim > 2:\n        raise ValueError(\"m has more than 2 dimensions\")\n\n    if y is not None:\n        y = np.asarray(y)\n        if y.ndim > 2:\n            raise ValueError(\"y has more than 2 dimensions\")\n\n    if dtype is None:\n        if y is None:\n            dtype = np.result_type(m, np.float64)\n        else:\n            dtype = np.result_type(m, y, np.float64)\n\n    X = array(m, ndmin=2, dtype=dtype)\n    if not rowvar and X.shape[0] != 1:\n        X = X.T\n    if X.shape[0] == 0:\n        return np.array([]).reshape(0, 0)\n    if y is not None:\n        y = array(y, copy=False, ndmin=2, dtype=dtype)\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        X = np.concatenate((X, y), axis=0)\n\n    if ddof is None:\n        if bias == 0:\n            ddof = 1\n        else:\n            ddof = 0\n\n    # Get the product of frequencies and weights\n    w = None\n    if fweights is not None:\n        fweights = np.asarray(fweights, dtype=float)\n        if not np.all(fweights == np.around(fweights)):\n            raise TypeError(\n                \"fweights must be integer\")\n        if fweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional fweights\")\n        if fweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and fweights\")\n        if any(fweights < 0):\n            raise ValueError(\n                \"fweights cannot be negative\")\n        w = fweights\n    if aweights is not None:\n        aweights = np.asarray(aweights, dtype=float)\n        if aweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional aweights\")\n        if aweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and aweights\")\n        if any(aweights < 0):\n            raise ValueError(\n                \"aweights cannot be negative\")\n        if w is None:\n            w = aweights\n        else:\n            w *= aweights\n\n    avg, w_sum = average(X, axis=1, weights=w, returned=True)\n    w_sum = w_sum[0]\n\n    # Determine the normalization\n    if w is None:\n        fact = X.shape[1] - ddof\n    elif ddof == 0:\n        fact = w_sum\n    elif aweights is None:\n        fact = w_sum - ddof\n    else:\n        fact = w_sum - ddof*sum(w*aweights)/w_sum\n\n    if fact <= 0:\n        warnings.warn(\"Degrees of freedom <= 0 for slice\",\n                      RuntimeWarning, stacklevel=2)\n        fact = 0.0\n\n    X -= avg[:, None]\n    if w is None:\n        X_T = X.T\n    else:\n        X_T = (X*w).T\n    c = dot(X, X_T.conj())\n    c *= np.true_divide(1, fact)\n    return c.squeeze()"
    },
    {
        "name": "numpy.histogram",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram.html",
        "description": "Compute the histogram of a dataset.  Parameters ---------- a : array_like Input data. The histogram is computed over the flattened array. bins : int or sequence of scalars or str, optional If `bins` is an int, it defines the number of equal-width bins in the given range (10, by default). If `bins` is a sequence, it defines a monotonically increasing array of bin edges, including the rightmost edge, allowing for non-uniform bin widths.  .. versionadded:: 1.11.0  If `bins` is a string, it defines the method used to calculate the optimal bin width, as defined by `histogram_bin_edges`.  range : (float, float), optional The lower and upper range of the bins.  If not provided, range is simply ``(a.min(), a.max())``.  Values outside the range are ignored. The first element of the range must be less than or equal to the second. `range` affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within `range`, the bin count will fill the entire range including portions containing no data. weights : array_like, optional An array of weights, of the same shape as `a`.  Each value in `a` only contributes its associated weight towards the bin count (instead of 1). If `density` is True, the weights are normalized, so that the integral of the density over the range remains 1. density : bool, optional If ``False``, the result will contain the number of samples in each bin. If ``True``, the result is the value of the probability *density* function at the bin, normalized such that the *integral* over the range is 1. Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen; it is not a probability *mass* function.  Returns ------- hist : array The values of the histogram. See `density` and `weights` for a description of the possible semantics. bin_edges : array of dtype float Return the bin edges ``(length(hist)+1)``.   See Also -------- histogramdd, bincount, searchsorted, digitize, histogram_bin_edges  Notes ----- All but the last (righthand-most) bin is half-open.  In other words, if `bins` is::  [1, 2, 3, 4]  then the first bin is ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which *includes* 4.   Examples -------- >>> np.histogram([1, 2, 1], bins=[0, 1, 2, 3]) (array([0, 2, 1]), array([0, 1, 2, 3])) >>> np.histogram(np.arange(4), bins=np.arange(5), density=True) (array([0.25, 0.25, 0.25, 0.25]), array([0, 1, 2, 3, 4])) >>> np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3]) (array([1, 4, 1]), array([0, 1, 2, 3]))  >>> a = np.arange(5) >>> hist, bin_edges = np.histogram(a, density=True) >>> hist array([0.5, 0. , 0.5, 0. , 0. , 0.5, 0. , 0.5, 0. , 0.5]) >>> hist.sum() 2.4999999999999996 >>> np.sum(hist * np.diff(bin_edges)) 1.0  .. versionadded:: 1.11.0  Automated Bin Selection Methods example, using 2 peak random data with 2000 points:  >>> import matplotlib.pyplot as plt >>> rng = np.random.RandomState(10)  # deterministic random data >>> a = np.hstack((rng.normal(size=1000), ...                rng.normal(loc=5, scale=2, size=1000))) >>> _ = plt.hist(a, bins='auto')  # arguments are passed to np.histogram >>> plt.title(\"Histogram with 'auto' bins\") Text(0.5, 1.0, \"Histogram with 'auto' bins\") >>> plt.show()",
        "source": "@array_function_dispatch(_histogram_dispatcher)\ndef histogram(a, bins=10, range=None, density=None, weights=None):\n\n    a, weights = _ravel_and_check_weights(a, weights)\n\n    bin_edges, uniform_bins = _get_bin_edges(a, bins, range, weights)\n\n    # Histogram is an integer or a float array depending on the weights.\n    if weights is None:\n        ntype = np.dtype(np.intp)\n    else:\n        ntype = weights.dtype\n\n    # We set a block size, as this allows us to iterate over chunks when\n    # computing histograms, to minimize memory usage.\n    BLOCK = 65536\n\n    # The fast path uses bincount, but that only works for certain types\n    # of weight\n    simple_weights = (\n        weights is None or\n        np.can_cast(weights.dtype, np.double) or\n        np.can_cast(weights.dtype, complex)\n    )\n\n    if uniform_bins is not None and simple_weights:\n        # Fast algorithm for equal bins\n        # We now convert values of a to bin indices, under the assumption of\n        # equal bin widths (which is valid here).\n        first_edge, last_edge, n_equal_bins = uniform_bins\n\n        # Initialize empty histogram\n        n = np.zeros(n_equal_bins, ntype)\n\n        # Pre-compute histogram scaling factor\n        norm_numerator = n_equal_bins\n        norm_denom = _unsigned_subtract(last_edge, first_edge)\n\n        # We iterate over blocks here for two reasons: the first is that for\n        # large arrays, it is actually faster (for example for a 10^8 array it\n        # is 2x as fast) and it results in a memory footprint 3x lower in the\n        # limit of large arrays.\n        for i in _range(0, len(a), BLOCK):\n            tmp_a = a[i:i+BLOCK]\n            if weights is None:\n                tmp_w = None\n            else:\n                tmp_w = weights[i:i + BLOCK]\n\n            # Only include values in the right range\n            keep = (tmp_a >= first_edge)\n            keep &= (tmp_a <= last_edge)\n            if not np.logical_and.reduce(keep):\n                tmp_a = tmp_a[keep]\n                if tmp_w is not None:\n                    tmp_w = tmp_w[keep]\n\n            # This cast ensures no type promotions occur below, which gh-10322\n            # make unpredictable. Getting it wrong leads to precision errors\n            # like gh-8123.\n            tmp_a = tmp_a.astype(bin_edges.dtype, copy=False)\n\n            # Compute the bin indices, and for values that lie exactly on\n            # last_edge we need to subtract one\n            f_indices = ((_unsigned_subtract(tmp_a, first_edge) / norm_denom)\n                         * norm_numerator)\n            indices = f_indices.astype(np.intp)\n            indices[indices == n_equal_bins] -= 1\n\n            # The index computation is not guaranteed to give exactly\n            # consistent results within ~1 ULP of the bin edges.\n            decrement = tmp_a < bin_edges[indices]\n            indices[decrement] -= 1\n            # The last bin includes the right edge. The other bins do not.\n            increment = ((tmp_a >= bin_edges[indices + 1])\n                         & (indices != n_equal_bins - 1))\n            indices[increment] += 1\n\n            # We now compute the histogram using bincount\n            if ntype.kind == 'c':\n                n.real += np.bincount(indices, weights=tmp_w.real,\n                                      minlength=n_equal_bins)\n                n.imag += np.bincount(indices, weights=tmp_w.imag,\n                                      minlength=n_equal_bins)\n            else:\n                n += np.bincount(indices, weights=tmp_w,\n                                 minlength=n_equal_bins).astype(ntype)\n    else:\n        # Compute via cumulative histogram\n        cum_n = np.zeros(bin_edges.shape, ntype)\n        if weights is None:\n            for i in _range(0, len(a), BLOCK):\n                sa = np.sort(a[i:i+BLOCK])\n                cum_n += _search_sorted_inclusive(sa, bin_edges)\n        else:\n            zero = np.zeros(1, dtype=ntype)\n            for i in _range(0, len(a), BLOCK):\n                tmp_a = a[i:i+BLOCK]\n                tmp_w = weights[i:i+BLOCK]\n                sorting_index = np.argsort(tmp_a)\n                sa = tmp_a[sorting_index]\n                sw = tmp_w[sorting_index]\n                cw = np.concatenate((zero, sw.cumsum()))\n                bin_index = _search_sorted_inclusive(sa, bin_edges)\n                cum_n += cw[bin_index]\n\n        n = np.diff(cum_n)\n\n    if density:\n        db = np.array(np.diff(bin_edges), float)\n        return n/db/n.sum(), bin_edges\n\n    return n, bin_edges"
    },
    {
        "name": "numpy.histogram2d",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html",
        "description": "Compute the bi-dimensional histogram of two data samples.  Parameters ---------- x : array_like, shape (N,) An array containing the x coordinates of the points to be histogrammed. y : array_like, shape (N,) An array containing the y coordinates of the points to be histogrammed. bins : int or array_like or [int, int] or [array, array], optional The bin specification:  * If int, the number of bins for the two dimensions (nx=ny=bins). * If array_like, the bin edges for the two dimensions (x_edges=y_edges=bins). * If [int, int], the number of bins in each dimension (nx, ny = bins). * If [array, array], the bin edges in each dimension (x_edges, y_edges = bins). * A combination [int, array] or [array, int], where int is the number of bins and array is the bin edges.  range : array_like, shape(2,2), optional The leftmost and rightmost edges of the bins along each dimension (if not specified explicitly in the `bins` parameters): ``[[xmin, xmax], [ymin, ymax]]``. All values outside of this range will be considered outliers and not tallied in the histogram. density : bool, optional If False, the default, returns the number of samples in each bin. If True, returns the probability *density* function at the bin, ``bin_count / sample_count / bin_area``. weights : array_like, shape(N,), optional An array of values ``w_i`` weighing each sample ``(x_i, y_i)``. Weights are normalized to 1 if `density` is True. If `density` is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.  Returns ------- H : ndarray, shape(nx, ny) The bi-dimensional histogram of samples `x` and `y`. Values in `x` are histogrammed along the first dimension and values in `y` are histogrammed along the second dimension. xedges : ndarray, shape(nx+1,) The bin edges along the first dimension. yedges : ndarray, shape(ny+1,) The bin edges along the second dimension.  See Also -------- histogram : 1D histogram histogramdd : Multidimensional histogram  Notes ----- When `density` is True, then the returned histogram is the sample density, defined such that the sum over bins of the product ``bin_value * bin_area`` is 1.  Please note that the histogram does not follow the Cartesian convention where `x` values are on the abscissa and `y` values on the ordinate axis.  Rather, `x` is histogrammed along the first dimension of the array (vertical), and `y` along the second dimension of the array (horizontal).  This ensures compatibility with `histogramdd`.  Examples -------- >>> from matplotlib.image import NonUniformImage >>> import matplotlib.pyplot as plt  Construct a 2-D histogram with variable bin width. First define the bin edges:  >>> xedges = [0, 1, 3, 5] >>> yedges = [0, 2, 3, 4, 6]  Next we create a histogram H with random bin content:  >>> x = np.random.normal(2, 1, 100) >>> y = np.random.normal(1, 1, 100) >>> H, xedges, yedges = np.histogram2d(x, y, bins=(xedges, yedges)) >>> # Histogram does not follow Cartesian convention (see Notes), >>> # therefore transpose H for visualization purposes. >>> H = H.T  :func:`imshow <matplotlib.pyplot.imshow>` can only display square bins:  >>> fig = plt.figure(figsize=(7, 3)) >>> ax = fig.add_subplot(131, title='imshow: square bins') >>> plt.imshow(H, interpolation='nearest', origin='lower', ...         extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]]) <matplotlib.image.AxesImage object at 0x...>  :func:`pcolormesh <matplotlib.pyplot.pcolormesh>` can display actual edges:  >>> ax = fig.add_subplot(132, title='pcolormesh: actual edges', ...         aspect='equal') >>> X, Y = np.meshgrid(xedges, yedges) >>> ax.pcolormesh(X, Y, H) <matplotlib.collections.QuadMesh object at 0x...>  :class:`NonUniformImage <matplotlib.image.NonUniformImage>` can be used to display actual bin edges with interpolation:  >>> ax = fig.add_subplot(133, title='NonUniformImage: interpolated', ...         aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]]) >>> im = NonUniformImage(ax, interpolation='bilinear') >>> xcenters = (xedges[:-1] + xedges[1:]) / 2 >>> ycenters = (yedges[:-1] + yedges[1:]) / 2 >>> im.set_data(xcenters, ycenters, H) >>> ax.add_image(im) >>> plt.show()  It is also possible to construct a 2-D histogram without specifying bin edges:  >>> # Generate non-symmetric test data >>> n = 10000 >>> x = np.linspace(1, 100, n) >>> y = 2*np.log(x) + np.random.rand(n) - 0.5 >>> # Compute 2d histogram. Note the order of x/y and xedges/yedges >>> H, yedges, xedges = np.histogram2d(y, x, bins=20)  Now we can plot the histogram using :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`, and a :func:`hexbin <matplotlib.pyplot.hexbin>` for comparison.  >>> # Plot histogram using pcolormesh >>> fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True) >>> ax1.pcolormesh(xedges, yedges, H, cmap='rainbow') >>> ax1.plot(x, 2*np.log(x), 'k-') >>> ax1.set_xlim(x.min(), x.max()) >>> ax1.set_ylim(y.min(), y.max()) >>> ax1.set_xlabel('x') >>> ax1.set_ylabel('y') >>> ax1.set_title('histogram2d') >>> ax1.grid()  >>> # Create hexbin plot for comparison >>> ax2.hexbin(x, y, gridsize=20, cmap='rainbow') >>> ax2.plot(x, 2*np.log(x), 'k-') >>> ax2.set_title('hexbin') >>> ax2.set_xlim(x.min(), x.max()) >>> ax2.set_xlabel('x') >>> ax2.grid()  >>> plt.show()",
        "source": "@array_function_dispatch(_histogram2d_dispatcher)\ndef histogram2d(x, y, bins=10, range=None, density=None, weights=None):\n    \n    from numpy import histogramdd\n\n    if len(x) != len(y):\n        raise ValueError('x and y must have the same length.')\n\n    try:\n        N = len(bins)\n    except TypeError:\n        N = 1\n\n    if N != 1 and N != 2:\n        xedges = yedges = asarray(bins)\n        bins = [xedges, yedges]\n    hist, edges = histogramdd([x, y], bins, range, density, weights)\n    return hist, edges[0], edges[1]"
    },
    {
        "name": "numpy.histogramdd",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html",
        "description": "Compute the multidimensional histogram of some data.  Parameters ---------- sample : (N, D) array, or (N, D) array_like The data to be histogrammed.  Note the unusual interpretation of sample when an array_like:  * When an array, each row is a coordinate in a D-dimensional space - such as ``histogramdd(np.array([p1, p2, p3]))``. * When an array_like, each element is the list of values for single coordinate - such as ``histogramdd((X, Y, Z))``.  The first form should be preferred.  bins : sequence or int, optional The bin specification:  * A sequence of arrays describing the monotonically increasing bin edges along each dimension. * The number of bins for each dimension (nx, ny, ... =bins) * The number of bins for all dimensions (nx=ny=...=bins).  range : sequence, optional A sequence of length D, each an optional (lower, upper) tuple giving the outer bin edges to be used if the edges are not given explicitly in `bins`. An entry of None in the sequence results in the minimum and maximum values being used for the corresponding dimension. The default, None, is equivalent to passing a tuple of D None values. density : bool, optional If False, the default, returns the number of samples in each bin. If True, returns the probability *density* function at the bin, ``bin_count / sample_count / bin_volume``. weights : (N,) array_like, optional An array of values `w_i` weighing each sample `(x_i, y_i, z_i, ...)`. Weights are normalized to 1 if density is True. If density is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.  Returns ------- H : ndarray The multidimensional histogram of sample x. See density and weights for the different possible semantics. edges : list A list of D arrays describing the bin edges for each dimension.  See Also -------- histogram: 1-D histogram histogram2d: 2-D histogram  Examples -------- >>> r = np.random.randn(100,3) >>> H, edges = np.histogramdd(r, bins = (5, 8, 4)) >>> H.shape, edges[0].size, edges[1].size, edges[2].size ((5, 8, 4), 6, 9, 5)",
        "source": "@array_function_dispatch(_histogramdd_dispatcher)\ndef histogramdd(sample, bins=10, range=None, density=None, weights=None):\n    \n    try:\n        # Sample is an ND-array.\n        N, D = sample.shape\n    except (AttributeError, ValueError):\n        # Sample is a sequence of 1D arrays.\n        sample = np.atleast_2d(sample).T\n        N, D = sample.shape\n\n    nbin = np.empty(D, np.intp)\n    edges = D*[None]\n    dedges = D*[None]\n    if weights is not None:\n        weights = np.asarray(weights)\n\n    try:\n        M = len(bins)\n        if M != D:\n            raise ValueError(\n                'The dimension of bins must be equal to the dimension of the '\n                'sample x.')\n    except TypeError:\n        # bins is an integer\n        bins = D*[bins]\n\n    # normalize the range argument\n    if range is None:\n        range = (None,) * D\n    elif len(range) != D:\n        raise ValueError('range argument must have one entry per dimension')\n\n    # Create edge arrays\n    for i in _range(D):\n        if np.ndim(bins[i]) == 0:\n            if bins[i] < 1:\n                raise ValueError(\n                    '`bins[{}]` must be positive, when an integer'.format(i))\n            smin, smax = _get_outer_edges(sample[:,i], range[i])\n            try:\n                n = operator.index(bins[i])\n\n            except TypeError as e:\n                raise TypeError(\n                \t\"`bins[{}]` must be an integer, when a scalar\".format(i)\n                ) from e\n\n            edges[i] = np.linspace(smin, smax, n + 1)\n        elif np.ndim(bins[i]) == 1:\n            edges[i] = np.asarray(bins[i])\n            if np.any(edges[i][:-1] > edges[i][1:]):\n                raise ValueError(\n                    '`bins[{}]` must be monotonically increasing, when an array'\n                    .format(i))\n        else:\n            raise ValueError(\n                '`bins[{}]` must be a scalar or 1d array'.format(i))\n\n        nbin[i] = len(edges[i]) + 1  # includes an outlier on each end\n        dedges[i] = np.diff(edges[i])\n\n    # Compute the bin number each sample falls into.\n    Ncount = tuple(\n        # avoid np.digitize to work around gh-11022\n        np.searchsorted(edges[i], sample[:, i], side='right')\n        for i in _range(D)\n    )\n\n    # Using digitize, values that fall on an edge are put in the right bin.\n    # For the rightmost bin, we want values equal to the right edge to be\n    # counted in the last bin, and not as an outlier.\n    for i in _range(D):\n        # Find which points are on the rightmost edge.\n        on_edge = (sample[:, i] == edges[i][-1])\n        # Shift these points one bin to the left.\n        Ncount[i][on_edge] -= 1\n\n    # Compute the sample indices in the flattened histogram matrix.\n    # This raises an error if the array is too large.\n    xy = np.ravel_multi_index(Ncount, nbin)\n\n    # Compute the number of repetitions in xy and assign it to the\n    # flattened histmat.\n    hist = np.bincount(xy, weights, minlength=nbin.prod())\n\n    # Shape into a proper matrix\n    hist = hist.reshape(nbin)\n\n    # This preserves the (bad) behavior observed in gh-7845, for now.\n    hist = hist.astype(float, casting='safe')\n\n    # Remove outliers (indices 0 and -1 for each dimension).\n    core = D*(slice(1, -1),)\n    hist = hist[core]\n\n    if density:\n        # calculate the probability density function\n        s = hist.sum()\n        for i in _range(D):\n            shape = np.ones(D, int)\n            shape[i] = nbin[i] - 2\n            hist = hist / dedges[i].reshape(shape)\n        hist /= s\n\n    if (hist.shape != nbin - 2).any():\n        raise RuntimeError(\n            \"Internal Shape Error\")\n    return hist, edges"
    },
    {
        "name": "numpy.histogram_bin_edges",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html",
        "description": "Function to calculate only the edges of the bins used by the `histogram` function.  Parameters ---------- a : array_like Input data. The histogram is computed over the flattened array. bins : int or sequence of scalars or str, optional If `bins` is an int, it defines the number of equal-width bins in the given range (10, by default). If `bins` is a sequence, it defines the bin edges, including the rightmost edge, allowing for non-uniform bin widths.  If `bins` is a string from the list below, `histogram_bin_edges` will use the method chosen to calculate the optimal bin width and consequently the number of bins (see `Notes` for more detail on the estimators) from the data that falls within the requested range. While the bin width will be optimal for the actual data in the range, the number of bins will be computed to fill the entire range, including the empty portions. For visualisation, using the 'auto' option is suggested. Weighted data is not supported for automated bin size selection.  'auto' Maximum of the 'sturges' and 'fd' estimators. Provides good all around performance.  'fd' (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.  'doane' An improved version of Sturges' estimator that works better with non-normal datasets.  'scott' Less robust estimator that takes into account data variability and data size.  'stone' Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.  'rice' Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.  'sturges' R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.  'sqrt' Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.  range : (float, float), optional The lower and upper range of the bins.  If not provided, range is simply ``(a.min(), a.max())``.  Values outside the range are ignored. The first element of the range must be less than or equal to the second. `range` affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within `range`, the bin count will fill the entire range including portions containing no data.  weights : array_like, optional An array of weights, of the same shape as `a`.  Each value in `a` only contributes its associated weight towards the bin count (instead of 1). This is currently not used by any of the bin estimators, but may be in the future.  Returns ------- bin_edges : array of dtype float The edges to pass into `histogram`  See Also -------- histogram  Notes ----- The methods to estimate the optimal number of bins are well founded in literature, and are inspired by the choices R provides for histogram visualisation. Note that having the number of bins proportional to :math:`n^{1/3}` is asymptotically optimal, which is why it appears in most estimators. These are simply plug-in methods that give good starting points for number of bins. In the equations below, :math:`h` is the binwidth and :math:`n_h` is the number of bins. All estimators that compute bin counts are recast to bin width using the `ptp` of the data. The final bin count is obtained from ``np.round(np.ceil(range / h))``. The final bin width is often less than what is returned by the estimators below.  'auto' (maximum of the 'sturges' and 'fd' estimators) A compromise to get a good value. For small datasets the Sturges value will usually be chosen, while larger datasets will usually default to FD.  Avoids the overly conservative behaviour of FD and Sturges for small and large datasets respectively. Switchover point is usually :math:`a.size \u0007pprox 1000`.  'fd' (Freedman Diaconis Estimator) .. math:: h = 2 \frac{IQR}{n^{1/3}}  The binwidth is proportional to the interquartile range (IQR) and inversely proportional to cube root of a.size. Can be too conservative for small datasets, but is quite good for large datasets. The IQR is very robust to outliers.  'scott' .. math:: h = \\sigma \\sqrt[3]{\frac{24 \\sqrt{\\pi}}{n}}  The binwidth is proportional to the standard deviation of the data and inversely proportional to cube root of ``x.size``. Can be too conservative for small datasets, but is quite good for large datasets. The standard deviation is not very robust to outliers. Values are very similar to the Freedman-Diaconis estimator in the absence of outliers.  'rice' .. math:: n_h = 2n^{1/3}  The number of bins is only proportional to cube root of ``a.size``. It tends to overestimate the number of bins and it does not take into account data variability.  'sturges' .. math:: n_h = \\log _{2}(n) + 1  The number of bins is the base 2 log of ``a.size``.  This estimator assumes normality of data and is too conservative for larger, non-normal datasets. This is the default method in R's ``hist`` method.  'doane' .. math:: n_h = 1 + \\log_{2}(n) + \\log_{2}\\left(1 + \frac{|g_1|}{\\sigma_{g_1}}\right)  g_1 = mean\\left[\\left(\frac{x - \\mu}{\\sigma}\right)^3\right]  \\sigma_{g_1} = \\sqrt{\frac{6(n - 2)}{(n + 1)(n + 3)}}  An improved version of Sturges' formula that produces better estimates for non-normal datasets. This estimator attempts to account for the skew of the data.  'sqrt' .. math:: n_h = \\sqrt n  The simplest and fastest estimator. Only takes into account the data size.  Examples -------- >>> arr = np.array([0, 0, 0, 1, 2, 3, 3, 4, 5]) >>> np.histogram_bin_edges(arr, bins='auto', range=(0, 1)) array([0.  , 0.25, 0.5 , 0.75, 1.  ]) >>> np.histogram_bin_edges(arr, bins=2) array([0. , 2.5, 5. ])  For consistency with histogram, an array of pre-computed bins is passed through unmodified:  >>> np.histogram_bin_edges(arr, [1, 2]) array([1, 2])  This function allows one set of bins to be computed, and reused across multiple histograms:  >>> shared_bins = np.histogram_bin_edges(arr, bins='auto') >>> shared_bins array([0., 1., 2., 3., 4., 5.])  >>> group_id = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1]) >>> hist_0, _ = np.histogram(arr[group_id == 0], bins=shared_bins) >>> hist_1, _ = np.histogram(arr[group_id == 1], bins=shared_bins)  >>> hist_0; hist_1 array([1, 1, 0, 1, 0]) array([2, 0, 1, 1, 2])  Which gives more easily comparable results than using separate bins for each histogram:  >>> hist_0, bins_0 = np.histogram(arr[group_id == 0], bins='auto') >>> hist_1, bins_1 = np.histogram(arr[group_id == 1], bins='auto') >>> hist_0; hist_1 array([1, 1, 1]) array([2, 1, 1, 2]) >>> bins_0; bins_1 array([0., 1., 2., 3.]) array([0.  , 1.25, 2.5 , 3.75, 5.  ])",
        "source": "@array_function_dispatch(_histogram_bin_edges_dispatcher)\ndef histogram_bin_edges(a, bins=10, range=None, weights=None):\n    \n    a, weights = _ravel_and_check_weights(a, weights)\n    bin_edges, _ = _get_bin_edges(a, bins, range, weights)\n    return bin_edges"
    },
    {
        "name": "numpy.digitize",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.digitize.html",
        "description": "Return the indices of the bins to which each value in input array belongs.  =========  =============  ============================ `right`    order of bins  returned index `i` satisfies =========  =============  ============================ ``False``  increasing     ``bins[i-1] <= x < bins[i]`` ``True``   increasing     ``bins[i-1] < x <= bins[i]`` ``False``  decreasing     ``bins[i-1] > x >= bins[i]`` ``True``   decreasing     ``bins[i-1] >= x > bins[i]`` =========  =============  ============================  If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is returned as appropriate.  Parameters ---------- x : array_like Input array to be binned. Prior to NumPy 1.10.0, this array had to be 1-dimensional, but can now have any shape. bins : array_like Array of bins. It has to be 1-dimensional and monotonic. right : bool, optional Indicating whether the intervals include the right or the left bin edge. Default behavior is (right==False) indicating that the interval does not include the right edge. The left bin end is open in this case, i.e., bins[i-1] <= x < bins[i] is the default behavior for monotonically increasing bins.  Returns ------- indices : ndarray of ints Output array of indices, of same shape as `x`.  Raises ------ ValueError If `bins` is not monotonic. TypeError If the type of the input is complex.  See Also -------- bincount, histogram, unique, searchsorted  Notes ----- If values in `x` are such that they fall outside the bin range, attempting to index `bins` with the indices that `digitize` returns will result in an IndexError.  .. versionadded:: 1.10.0  `np.digitize` is  implemented in terms of `np.searchsorted`. This means that a binary search is used to bin the values, which scales much better for larger number of bins than the previous linear search. It also removes the requirement for the input array to be 1-dimensional.  For monotonically _increasing_ `bins`, the following are equivalent::  np.digitize(x, bins, right=True) np.searchsorted(bins, x, side='left')  Note that as the order of the arguments are reversed, the side must be too. The `searchsorted` call is marginally faster, as it does not do any monotonicity checks. Perhaps more importantly, it supports all dtypes.  Examples -------- >>> x = np.array([0.2, 6.4, 3.0, 1.6]) >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0]) >>> inds = np.digitize(x, bins) >>> inds array([1, 4, 3, 2]) >>> for n in range(x.size): ...   print(bins[inds[n]-1], \"<=\", x[n], \"<\", bins[inds[n]]) ... 0.0 <= 0.2 < 1.0 4.0 <= 6.4 < 10.0 2.5 <= 3.0 < 4.0 1.0 <= 1.6 < 2.5  >>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.]) >>> bins = np.array([0, 5, 10, 15, 20]) >>> np.digitize(x,bins,right=True) array([1, 2, 3, 4, 4]) >>> np.digitize(x,bins,right=False) array([1, 3, 3, 4, 5])",
        "source": "@array_function_dispatch(_digitize_dispatcher)\ndef digitize(x, bins, right=False):\n    \n    x = _nx.asarray(x)\n    bins = _nx.asarray(bins)\n\n    # here for compatibility, searchsorted below is happy to take this\n    if np.issubdtype(x.dtype, _nx.complexfloating):\n        raise TypeError(\"x may not be complex\")\n\n    mono = _monotonicity(bins)\n    if mono == 0:\n        raise ValueError(\"bins must be monotonically increasing or decreasing\")\n\n    # this is backwards because the arguments below are swapped\n    side = 'left' if right else 'right'\n    if mono == -1:\n        # reverse the bins, and invert the results\n        return len(bins) - _nx.searchsorted(bins[::-1], x, side=side)\n    else:\n        return _nx.searchsorted(bins, x, side=side)"
    }
]