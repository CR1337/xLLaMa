[
    {
        "name": "tokenizer.convert_ids_to_tokens",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens",
        "description": "Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.  Args: ids (`int` or `List[int]`): The token id (or token ids) to convert to tokens. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding.  Returns: `str` or `List[str]`: The decoded token(s).",
        "source": "def convert_ids_to_tokens(\n        self, ids: Union[int, List[int]], skip_special_tokens: bool = False\n    ) -> Union[str, List[str]]:\n        \n        if isinstance(ids, int):\n            if ids in self._added_tokens_decoder:\n                return self._added_tokens_decoder[ids].content\n            else:\n                return self._convert_id_to_token(ids)\n        tokens = []\n        for index in ids:\n            index = int(index)\n            if skip_special_tokens and index in self.all_special_ids:\n                continue\n            if index in self._added_tokens_decoder:\n                tokens.append(self._added_tokens_decoder[index].content)\n            else:\n                tokens.append(self._convert_id_to_token(index))\n        return tokens"
    },
    {
        "name": "tokenizer.add_tokens",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens",
        "description": "Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to it with indices starting from length of the current vocabulary and and will be isolated before the tokenization algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore not treated in the same way.  Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer.  In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.  Args: new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`): Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string token to let you personalize its behavior: whether this token should only match against a single word, whether this token should strip all potential whitespaces on the left side, whether this token should strip all potential whitespaces on the right side, etc. special_tokens (`bool`, *optional*, defaults to `False`): Can be used to specify if the token is a special token. This mostly change the normalization behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).  See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.  Returns: `int`: Number of tokens added to the vocabulary.  Examples:  ```python # Let's see how to increase the vocabulary of Bert model and tokenizer tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") model = BertModel.from_pretrained(\"bert-base-uncased\")  num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"]) print(\"We have added\", num_added_toks, \"tokens\") # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer. model.resize_token_embeddings(len(tokenizer)) ```",
        "source": "def add_tokens(\n        self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False\n    ) -> int:\n        \n        if not new_tokens:\n            return 0\n\n        if not isinstance(new_tokens, (list, tuple)):\n            new_tokens = [new_tokens]\n\n        return self._add_tokens(new_tokens, special_tokens=special_tokens)"
    },
    {
        "name": "tokenizer.add_special_tokens",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_special_tokens",
        "description": "Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).  When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer.  In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.  Using `add_special_tokens` will ensure your special tokens can be used in several ways:  - Special tokens can be skipped when decoding using `skip_special_tokens = True`. - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`. - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.  When possible, special tokens are already registered for provided pretrained models (for instance [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be `'</s>'`).  Args: special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`): Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].  Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the `unk_token` to them). replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`): If `True`, the existing list of additional special tokens will be replaced by the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is just extended. In the former case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous `additional_special_tokens` are still added tokens, and will not be split by the model.  Returns: `int`: Number of tokens added to the vocabulary.  Examples:  ```python # Let's see how to add a new classification token to GPT-2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") model = GPT2Model.from_pretrained(\"gpt2\")  special_tokens_dict = {\"cls_token\": \"<CLS>\"}  num_added_toks = tokenizer.add_special_tokens(special_tokens_dict) print(\"We have added\", num_added_toks, \"tokens\") # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer. model.resize_token_embeddings(len(tokenizer))  assert tokenizer.cls_token == \"<CLS>\" ```",
        "source": "def add_special_tokens(\n        self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True\n    ) -> int:\n        \n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = []\n        for key, value in special_tokens_dict.items():\n            assert key in self.SPECIAL_TOKENS_ATTRIBUTES, f\"Key {key} is not a special token\"\n\n            if self.verbose:\n                logger.info(f\"Assigning {value} to the {key} key of the tokenizer\")\n\n            if key == \"additional_special_tokens\":\n                assert isinstance(value, (list, tuple)) and all(\n                    isinstance(t, (str, AddedToken)) for t in value\n                ), f\"Tokens {value} for key {key} should all be str or AddedToken instances\"\n\n                to_add = set()\n                for token in value:\n                    if isinstance(token, str):\n                        # for legacy purpose we default to stripping. `test_add_tokens_tokenizer` depends on this\n                        token = AddedToken(token, rstrip=False, lstrip=False, normalized=False, special=True)\n                    if str(token) not in self.additional_special_tokens:\n                        to_add.add(token)\n                if replace_additional_special_tokens:\n                    setattr(self, key, list(to_add))\n                else:\n                    self._additional_special_tokens.extend(to_add)\n                added_tokens += to_add\n\n            else:\n                if not isinstance(value, (str, AddedToken)):\n                    raise ValueError(f\"Token {value} for key {key} should be a str or an AddedToken instance\")\n                if isinstance(value, (str)):\n                    # for legacy purpose we default to stripping. `False` depends on this\n                    value = AddedToken(value, rstrip=False, lstrip=False, normalized=False, special=True)\n                if isinstance(value, AddedToken):\n                    setattr(self, key, value)\n                if value not in added_tokens:\n                    added_tokens.append(value)\n\n        # if we are adding tokens that were not part of the vocab, we ought to add them\n        added_tokens = self.add_tokens(added_tokens, special_tokens=True)\n        return added_tokens"
    },
    {
        "name": "tokenizer.apply_chat_template",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.apply_chat_template",
        "description": "Converts a Conversation object or a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to determine the format and control tokens to use when converting. When chat_template is None, it will fall back to the default_chat_template specified at the class level.  Args: conversation (Union[List[Dict[str, str]], \"Conversation\"]): A Conversation object or list of dicts with \"role\" and \"content\" keys, representing the chat history so far. chat_template (str, *optional*): A Jinja template to use for this conversion. If this is not passed, the model's default chat template will be used instead. add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate the start of an assistant message. This is useful when you want to generate a response from the model. Note that this argument will be passed to the chat template, and so it must be supported in the template for this argument to have any effect. tokenize (`bool`, defaults to `True`): Whether to tokenize the output. If `False`, the output will be a string. padding (`bool`, defaults to `False`): Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`. truncation (`bool`, defaults to `False`): Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`. max_length (`int`, *optional*): Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizer's `max_length` attribute will be used as a default. return_tensors (`str` or [`~utils.TensorType`], *optional*): If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable values are: - `'tf'`: Return TensorFlow `tf.Tensor` objects. - `'pt'`: Return PyTorch `torch.Tensor` objects. - `'np'`: Return NumPy `np.ndarray` objects. - `'jax'`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs: Additional kwargs to pass to the tokenizer.  Returns: `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This output is ready to pass to the model, either directly or via methods like `generate()`.",
        "source": "def apply_chat_template(\n        self,\n        conversation: Union[List[Dict[str, str]], \"Conversation\"],\n        chat_template: Optional[str] = None,\n        add_generation_prompt: bool = False,\n        tokenize: bool = True,\n        padding: bool = False,\n        truncation: bool = False,\n        max_length: Optional[int] = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        **tokenizer_kwargs,\n    ) -> Union[str, List[int]]:\n        \n        if hasattr(conversation, \"messages\"):\n            # Indicates it's a Conversation object\n            conversation = conversation.messages\n\n        # priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template`\n        if chat_template is None:\n            if self.chat_template is not None:\n                chat_template = self.chat_template\n            else:\n                chat_template = self.default_chat_template\n\n        # Compilation function uses a cache to avoid recompiling the same template\n        compiled_template = self._compile_jinja_template(chat_template)\n\n        rendered = compiled_template.render(\n            messages=conversation, add_generation_prompt=add_generation_prompt, **self.special_tokens_map\n        )\n\n        if padding is True:\n            padding = \"max_length\"  # There's only one sequence here, so \"longest\" makes no sense\n        if tokenize:\n            return self.encode(\n                rendered,\n                add_special_tokens=False,\n                padding=padding,\n                truncation=truncation,\n                max_length=max_length,\n                return_tensors=return_tensors,\n                **tokenizer_kwargs,\n            )\n        else:\n            return rendered"
    },
    {
        "name": "tokenizer.batch_decode",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.batch_decode",
        "description": "Convert a list of lists of token ids into a list of strings by calling decode.  Args: sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`): List of tokenized input ids. Can be obtained using the `__call__` method. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding. clean_up_tokenization_spaces (`bool`, *optional*): Whether or not to clean up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`. kwargs (additional keyword arguments, *optional*): Will be passed to the underlying model specific decode method.  Returns: `List[str]`: The list of decoded sentences.",
        "source": "def batch_decode(\n        self,\n        sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n        skip_special_tokens: bool = False,\n        clean_up_tokenization_spaces: bool = None,\n        **kwargs,\n    ) -> List[str]:\n        \n        return [\n            self.decode(\n                seq,\n                skip_special_tokens=skip_special_tokens,\n                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n                **kwargs,\n            )\n            for seq in sequences\n        ]"
    },
    {
        "name": "tokenizer.decode",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode",
        "description": "Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special tokens and clean up tokenization spaces.  Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.  Args: token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`): List of tokenized input ids. Can be obtained using the `__call__` method. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding. clean_up_tokenization_spaces (`bool`, *optional*): Whether or not to clean up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`. kwargs (additional keyword arguments, *optional*): Will be passed to the underlying model specific decode method.  Returns: `str`: The decoded sentence.",
        "source": "def decode(\n        self,\n        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n        skip_special_tokens: bool = False,\n        clean_up_tokenization_spaces: bool = None,\n        **kwargs,\n    ) -> str:\n        \n        # Convert inputs to python lists\n        token_ids = to_py_obj(token_ids)\n\n        return self._decode(\n            token_ids=token_ids,\n            skip_special_tokens=skip_special_tokens,\n            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n            **kwargs,\n        )"
    },
    {
        "name": "tokenizer.encode",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode",
        "description": "Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.  Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.  Args: text (`str`, `List[str]` or `List[int]`): The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids` method). text_pair (`str`, `List[str]` or `List[int]`, *optional*): Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids` method).",
        "source": "def encode(\n        self,\n        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n        add_special_tokens: bool = True,\n        padding: Union[bool, str, PaddingStrategy] = False,\n        truncation: Union[bool, str, TruncationStrategy] = None,\n        max_length: Optional[int] = None,\n        stride: int = 0,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        **kwargs,\n    ) -> List[int]:\n        \n        encoded_inputs = self.encode_plus(\n            text,\n            text_pair=text_pair,\n            add_special_tokens=add_special_tokens,\n            padding=padding,\n            truncation=truncation,\n            max_length=max_length,\n            stride=stride,\n            return_tensors=return_tensors,\n            **kwargs,\n        )\n\n        return encoded_inputs[\"input_ids\"]"
    },
    {
        "name": "tokenizer.push_to_hub",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.push_to_hub",
        "description": "Upload the {object_files} to the \ud83e\udd17 Model Hub.  Parameters: repo_id (`str`): The name of the repository you want to push your {object} to. It should contain your organization name when pushing to a given organization. use_temp_dir (`bool`, *optional*): Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub. Will default to `True` if there is no directory named like `repo_id`, `False` otherwise. commit_message (`str`, *optional*): Message to commit while pushing. Will default to `\"Upload {object}\"`. private (`bool`, *optional*): Whether or not the repository created should be private. token (`bool` or `str`, *optional*): The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not specified. max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`): Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier Google Colab instances without any CPU OOM issues. create_pr (`bool`, *optional*, defaults to `False`): Whether or not to create a PR with the uploaded files or directly commit. safe_serialization (`bool`, *optional*, defaults to `True`): Whether or not to convert the model weights in safetensors format for safer serialization. revision (`str`, *optional*): Branch to push the uploaded files to. commit_description (`str`, *optional*): The description of the commit that will be created  Examples:  ```python from transformers import {object_class}  {object} = {object_class}.from_pretrained(\"bert-base-cased\")  # Push the {object} to your namespace with the name \"my-finetuned-bert\". {object}.push_to_hub(\"my-finetuned-bert\")  # Push the {object} to an organization with the name \"my-finetuned-bert\". {object}.push_to_hub(\"huggingface/my-finetuned-bert\") ```",
        "source": "def push_to_hub(\n        self,\n        repo_id: str,\n        use_temp_dir: Optional[bool] = None,\n        commit_message: Optional[str] = None,\n        private: Optional[bool] = None,\n        token: Optional[Union[bool, str]] = None,\n        max_shard_size: Optional[Union[int, str]] = \"5GB\",\n        create_pr: bool = False,\n        safe_serialization: bool = True,\n        revision: str = None,\n        commit_description: str = None,\n        **deprecated_kwargs,\n    ) -> str:\n        \n        use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n        if use_auth_token is not None:\n            warnings.warn(\n                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                FutureWarning,\n            )\n            if token is not None:\n                raise ValueError(\n                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                )\n            token = use_auth_token\n\n        repo_path_or_name = deprecated_kwargs.pop(\"repo_path_or_name\", None)\n        if repo_path_or_name is not None:\n            # Should use `repo_id` instead of `repo_path_or_name`. When using `repo_path_or_name`, we try to infer\n            # repo_id from the folder path, if it exists.\n            warnings.warn(\n                \"The `repo_path_or_name` argument is deprecated and will be removed in v5 of Transformers. Use \"\n                \"`repo_id` instead.\",\n                FutureWarning,\n            )\n            if repo_id is not None:\n                raise ValueError(\n                    \"`repo_id` and `repo_path_or_name` are both specified. Please set only the argument `repo_id`.\"\n                )\n            if os.path.isdir(repo_path_or_name):\n                # repo_path: infer repo_id from the path\n                repo_id = repo_id.split(os.path.sep)[-1]\n                working_dir = repo_id\n            else:\n                # repo_name: use it as repo_id\n                repo_id = repo_path_or_name\n                working_dir = repo_id.split(\"/\")[-1]\n        else:\n            # Repo_id is passed correctly: infer working_dir from it\n            working_dir = repo_id.split(\"/\")[-1]\n\n        # Deprecation warning will be sent after for repo_url and organization\n        repo_url = deprecated_kwargs.pop(\"repo_url\", None)\n        organization = deprecated_kwargs.pop(\"organization\", None)\n\n        repo_id = self._create_repo(\n            repo_id, private=private, token=token, repo_url=repo_url, organization=organization\n        )\n\n        if use_temp_dir is None:\n            use_temp_dir = not os.path.isdir(working_dir)\n\n        with working_or_temp_dir(working_dir=working_dir, use_temp_dir=use_temp_dir) as work_dir:\n            files_timestamps = self._get_files_timestamps(work_dir)\n\n            # Save all files.\n            self.save_pretrained(work_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n\n            return self._upload_modified_files(\n                work_dir,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=token,\n                create_pr=create_pr,\n                revision=revision,\n                commit_description=commit_description,\n            )"
    },
    {
        "name": "tokenizer.convert_tokens_to_ids",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids",
        "description": "Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.  Args: tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).  Returns: `int` or `List[int]`: The token id or list of token ids.",
        "source": "def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:\n        \n        if tokens is None:\n            return None\n\n        if isinstance(tokens, str):\n            return self._convert_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._convert_token_to_id_with_added_voc(token))\n        return ids"
    },
    {
        "name": "tokenizer.get_added_vocab",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.get_added_vocab",
        "description": "Returns the added tokens in the vocabulary as a dictionary of token to index. Results might be different from the fast call because for now we always add the tokens even if they are already in the vocabulary. This is something we should change.  Returns: `Dict[str, int]`: The added tokens.",
        "source": "def get_added_vocab(self) -> Dict[str, int]:\n        \n        return self._added_tokens_encoder"
    },
    {
        "name": "tokenizer.num_special_tokens_to_add",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.num_special_tokens_to_add",
        "description": "Returns the number of added tokens when encoding a sequence with special tokens.  <Tip>  This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put this inside your training loop.  </Tip>  Args: pair (`bool`, *optional*, defaults to `False`): Whether the number of added tokens should be computed in the case of a sequence pair or a single sequence.  Returns: `int`: Number of special tokens added to sequences.",
        "source": "def num_special_tokens_to_add(self, pair: bool = False) -> int:\n        \n        token_ids_0 = []\n        token_ids_1 = []\n        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))"
    },
    {
        "name": "tokenizer.prepare_for_tokenization",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.prepare_for_tokenization",
        "description": "Performs any necessary transformations before tokenization.  This method should pop the arguments from kwargs and return the remaining `kwargs` as well. We test the `kwargs` at the end of the encoding process to be sure all the arguments have been used.  Args: text (`str`): The text to prepare. is_split_into_words (`bool`, *optional*, defaults to `False`): Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize. This is useful for NER or token classification. kwargs (`Dict[str, Any]`, *optional*): Keyword arguments to use for the tokenization.  Returns: `Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.",
        "source": "def prepare_for_tokenization(\n        self, text: str, is_split_into_words: bool = False, **kwargs\n    ) -> Tuple[str, Dict[str, Any]]:\n        \n        return (text, kwargs)"
    },
    {
        "name": "tokenizer.tokenize",
        "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.tokenize",
        "description": "Converts a string in a sequence of tokens, using the tokenizer.  Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces). Takes care of added tokens.  Args: text (`str`): The sequence to be encoded. **kwargs (additional keyword arguments): Passed along to the model-specific `prepare_for_tokenization` preprocessing method.  Returns: `List[str]`: The list of tokens.",
        "source": "def tokenize(self, text: TextInput, **kwargs) -> List[str]:\n        \n        split_special_tokens = kwargs.pop(\"split_special_tokens\", self.split_special_tokens)\n\n        text, kwargs = self.prepare_for_tokenization(text, **kwargs)\n\n        if kwargs:\n            logger.warning(f\"Keyword arguments {kwargs} not recognized.\")\n\n        if hasattr(self, \"do_lower_case\") and self.do_lower_case:\n            # convert non-special tokens to lowercase. Might be super slow as well?\n            escaped_special_toks = [re.escape(s_tok) for s_tok in (self.all_special_tokens)]\n            escaped_special_toks += [\n                re.escape(s_tok.content)\n                for s_tok in (self._added_tokens_decoder.values())\n                if not s_tok.special and s_tok.normalized\n            ]\n            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n            text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n\n        if split_special_tokens:\n            no_split_token = []\n            tokens = [text]\n        else:\n            no_split_token = self._added_tokens_encoder.keys()  # don't split on any of the added tokens\n            # \"This is something<special_token_1>  else\"\n            tokens = self.tokens_trie.split(text)\n\n        # [\"This is something\", \"<special_token_1>\", \"  else\"]\n        for i, token in enumerate(tokens):\n            if token in no_split_token:\n                tok_extended = self._added_tokens_decoder.get(self._added_tokens_encoder[token], None)\n                left = tokens[i - 1] if i > 0 else None\n                right = tokens[i + 1] if i < len(tokens) - 1 else None\n                if isinstance(tok_extended, AddedToken):\n                    if tok_extended.rstrip and right:\n                        # A bit counter-intuitive but we strip the left of the string\n                        # since tok_extended.rstrip means the special token is eating all white spaces on its right\n                        tokens[i + 1] = right.lstrip()\n                    # Strip white spaces on the left\n                    if tok_extended.lstrip and left:\n                        tokens[i - 1] = left.rstrip()  # Opposite here\n                    if tok_extended.single_word and left and left[-1] != \" \":\n                        tokens[i - 1] += token\n                        tokens[i] = \"\"\n                    elif tok_extended.single_word and right and right[0] != \" \":\n                        tokens[i + 1] = token + tokens[i + 1]\n                        tokens[i] = \"\"\n                else:\n                    raise ValueError(\n                        f\"{tok_extended} cannot be tokenized because it was not properly added\"\n                        f\" to the tokenizer. This means that it is not an `AddedToken` but a {type(tok_extended)}\"\n                    )\n        # [\"This is something\", \"<special_token_1>\", \"else\"]\n        tokenized_text = []\n        for token in tokens:\n            # Need to skip eventual empty (fully stripped) tokens\n            if not token:\n                continue\n            if token in no_split_token:\n                tokenized_text.append(token)\n            else:\n                tokenized_text.extend(self._tokenize(token))\n        # [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\n        return tokenized_text"
    }
]