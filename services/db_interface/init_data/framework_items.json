[
    [
        {
            "name": "DataFrame.at",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.DataFrame.at.html",
            "description": "Access a single value for a row/column label pair.  Similar to ``loc``, in that both provide label-based lookups. Use ``at`` if you only need to get or set a single value in a DataFrame or Series.  Raises ------ KeyError * If getting a value and 'label' does not exist in a DataFrame or Series. ValueError * If row/column label pair is not a tuple or if any label from the pair is not a scalar for DataFrame. * If label is list-like (*excluding* NamedTuple) for Series.  See Also -------- DataFrame.at : Access a single value for a row/column pair by label. DataFrame.iat : Access a single value for a row/column pair by integer position. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s). Series.at : Access a single value by label. Series.iat : Access a single value by integer position. Series.loc : Access a group of rows by label(s). Series.iloc : Access a group of rows by integer position(s).  Notes ----- See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>` for more details.  Examples -------- >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ...                   index=[4, 5, 6], columns=['A', 'B', 'C']) >>> df A   B   C 4   0   2   3 5   0   4   1 6  10  20  30  Get value at specified row/column pair  >>> df.at[4, 'B'] 2  Set value at specified row/column pair  >>> df.at[4, 'B'] = 10 >>> df.at[4, 'B'] 10  Get value within a Series  >>> df.loc[5].at['B'] 4",
            "source": "@property\n    def at(self) -> _AtIndexer:\n        \n        return _AtIndexer(\"at\", self)"
        },
        {
            "name": "DataFrame.dtypes",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html",
            "description": "Return the dtypes in the DataFrame.  This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the ``object`` dtype. See :ref:`the User Guide <basics.dtypes>` for more.  Returns ------- pandas.Series The data type of each column.  Examples -------- >>> df = pd.DataFrame({'float': [1.0], ...                    'int': [1], ...                    'datetime': [pd.Timestamp('20180310')], ...                    'string': ['foo']}) >>> df.dtypes float              float64 int                  int64 datetime    datetime64[ns] string              object dtype: object",
            "source": "@property\n    def dtypes(self):\n        \n        data = self._mgr.get_dtypes()\n        return self._constructor_sliced(data, index=self._info_axis, dtype=np.object_)"
        },
        {
            "name": "DataFrame.values",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html",
            "description": "Return a Numpy representation of the DataFrame.  .. warning::  We recommend using :meth:`DataFrame.to_numpy` instead.  Only the values in the DataFrame will be returned, the axes labels will be removed.  Returns ------- numpy.ndarray The values of the DataFrame.  See Also -------- DataFrame.to_numpy : Recommended alternative to this method. DataFrame.index : Retrieve the index labels. DataFrame.columns : Retrieving the column names.  Notes ----- The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks.  e.g. If the dtypes are float16 and float32, dtype will be upcast to float32.  If dtypes are int32 and uint8, dtype will be upcast to int32. By :func:`numpy.find_common_type` convention, mixing int64 and uint64 will result in a float64 dtype.  Examples -------- A DataFrame where all columns are the same type (e.g., int64) results in an array of the same type.  >>> df = pd.DataFrame({'age':    [ 3,  29], ...                    'height': [94, 170], ...                    'weight': [31, 115]}) >>> df age  height  weight 0    3      94      31 1   29     170     115 >>> df.dtypes age       int64 height    int64 weight    int64 dtype: object >>> df.values array([[  3,  94,  31], [ 29, 170, 115]])  A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray of the broadest type that accommodates these mixed types (e.g., object).  >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'), ...                     ('lion',     80.5, 1), ...                     ('monkey', np.nan, None)], ...                   columns=('name', 'max_speed', 'rank')) >>> df2.dtypes name          object max_speed    float64 rank          object dtype: object >>> df2.values array([['parrot', 24.0, 'second'], ['lion', 80.5, 1], ['monkey', nan, None]], dtype=object)",
            "source": "@property\n    def values(self) -> np.ndarray:\n        \n        return self._mgr.as_array()"
        },
        {
            "name": "DataFrame.axes",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.axes.html",
            "description": "Return a list representing the axes of the DataFrame.  It has the row axis labels and column axis labels as the only members. They are returned in that order.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.axes [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='object')]",
            "source": "@property\n    def axes(self) -> list[Index]:\n        \n        return [self.index, self.columns]"
        },
        {
            "name": "DataFrame.ndim",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ndim.html",
            "description": "Return an int representing the number of axes / array dimensions.  Return 1 if Series. Otherwise return 2 if DataFrame.  See Also -------- ndarray.ndim : Number of array dimensions.  Examples -------- >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3}) >>> s.ndim 1  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.ndim 2",
            "source": "@final\n    @property\n    def ndim(self) -> int:\n        \n        return self._mgr.ndim"
        },
        {
            "name": "DataFrame.size",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html",
            "description": "Return an int representing the number of elements in this object.  Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.  See Also -------- ndarray.size : Number of elements in the array.  Examples -------- >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3}) >>> s.size 3  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.size 4",
            "source": "@final\n    @property\n    def size(self) -> int:\n        \n\n        return int(np.prod(self.shape))"
        },
        {
            "name": "DataFrame.shape",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html",
            "description": "Return a tuple representing the dimensionality of the DataFrame.  See Also -------- ndarray.shape : Tuple of array dimensions.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df.shape (2, 2)  >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4], ...                    'col3': [5, 6]}) >>> df.shape (2, 3)",
            "source": "@property\n    def shape(self) -> tuple[int, int]:\n        \n        return len(self.index), len(self.columns)"
        },
        {
            "name": "DataFrame.memory_usage",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.memory_usage.html",
            "description": "Return the memory usage of each column in bytes.  The memory usage can optionally include the contribution of the index and elements of `object` dtype.  This value is displayed in `DataFrame.info` by default. This can be suppressed by setting ``pandas.options.display.memory_usage`` to False.  Parameters ---------- index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If ``index=True``, the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating `object` dtypes for system-level memory consumption, and include it in the returned values.  Returns ------- Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes.  See Also -------- numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame.  Notes ----- See the :ref:`Frequently Asked Questions <df-memory-usage>` for more details.  Examples -------- >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t)) ...              for t in dtypes]) >>> df = pd.DataFrame(data) >>> df.head() int64  float64            complex128  object  bool 0      1      1.0              1.0+0.0j       1  True 1      1      1.0              1.0+0.0j       1  True 2      1      1.0              1.0+0.0j       1  True 3      1      1.0              1.0+0.0j       1  True 4      1      1.0              1.0+0.0j       1  True  >>> df.memory_usage() Index           128 int64         40000 float64       40000 complex128    80000 object        40000 bool           5000 dtype: int64  >>> df.memory_usage(index=False) int64         40000 float64       40000 complex128    80000 object        40000 bool           5000 dtype: int64  The memory footprint of `object` dtype columns is ignored by default:  >>> df.memory_usage(deep=True) Index            128 int64          40000 float64        40000 complex128     80000 object        180000 bool            5000 dtype: int64  Use a Categorical for efficient storage of an object-dtype column with many repeated values.  >>> df['object'].astype('category').memory_usage(deep=True) 5244",
            "source": "def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        \n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n            dtype=np.intp,\n        )\n        if index:\n            index_memory_usage = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            )\n            result = index_memory_usage._append(result)\n        return result"
        },
        {
            "name": "DataFrame.empty",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.empty.html",
            "description": "Indicator whether Series/DataFrame is empty.  True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0.  Returns ------- bool If Series/DataFrame is empty, return True, if not return False.  See Also -------- Series.dropna : Return series without null values. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing.  Notes ----- If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.  Examples -------- An example of an actual empty DataFrame. Notice the index is empty:  >>> df_empty = pd.DataFrame({'A' : []}) >>> df_empty Empty DataFrame Columns: [A] Index: [] >>> df_empty.empty True  If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty:  >>> df = pd.DataFrame({'A' : [np.nan]}) >>> df A 0 NaN >>> df.empty False >>> df.dropna().empty True  >>> ser_empty = pd.Series({'A' : []}) >>> ser_empty A    [] dtype: object >>> ser_empty.empty False >>> ser_empty = pd.Series() >>> ser_empty.empty True",
            "source": "@property\n    def empty(self) -> bool_t:\n        \n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)"
        },
        {
            "name": "DataFrame.set_flags",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_flags.html",
            "description": "Return a new object with updated flags.  Parameters ---------- copy : bool, default False Specify if a copy of the object should be made. allows_duplicate_labels : bool, optional Whether the returned object allows duplicate labels.  Returns ------- Series or DataFrame The same type as the caller.  See Also -------- DataFrame.attrs : Global metadata applying to this dataset. DataFrame.flags : Global flags applying to this object.  Notes ----- This method returns a new object that's a view on the same data as the input. Mutating the input or the output values will be reflected in the other.  This method is intended to be used in method chains.  \"Flags\" differ from \"metadata\". Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in :attr:`DataFrame.attrs`.  Examples -------- >>> df = pd.DataFrame({\"A\": [1, 2]}) >>> df.flags.allows_duplicate_labels True >>> df2 = df.set_flags(allows_duplicate_labels=False) >>> df2.flags.allows_duplicate_labels False",
            "source": "@final\n    def set_flags(\n        self,\n        *,\n        copy: bool_t = False,\n        allows_duplicate_labels: bool_t | None = None,\n    ) -> Self:\n        \n        df = self.copy(deep=copy and not using_copy_on_write())\n        if allows_duplicate_labels is not None:\n            df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels\n        return df"
        },
        {
            "name": "DataFrame.convert_dtypes",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html",
            "description": "Convert columns to the best possible dtypes using dtypes supporting ``pd.NA``.  Parameters ---------- infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to ``StringDtype()``. convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to ``BooleanDtypes()``. convert_floating : bool, defaults True Whether, if possible, conversion can be done to floating extension types. If `convert_integer` is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers.  .. versionadded:: 1.2.0 dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable' Back-end data type applied to the resultant :class:`DataFrame` (still experimental). Behaviour is as follows:  * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame` (default). * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype` DataFrame.  .. versionadded:: 2.0  Returns ------- Series or DataFrame Copy of input object with new dtype.  See Also -------- infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type.  Notes ----- By default, ``convert_dtypes`` will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options ``convert_string``, ``convert_integer``, ``convert_boolean`` and ``convert_floating``, it is possible to turn off individual conversions to ``StringDtype``, the integer extension types, ``BooleanDtype`` or floating extension types, respectively.  For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference rules as during normal Series/DataFrame construction.  Then, if possible, convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer or floating extension type, otherwise leave as ``object``.  If the dtype is integer, convert to an appropriate integer extension type.  If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type.  .. versionchanged:: 1.2 Starting with pandas 1.2, this method also converts float columns to the nullable floating extension type.  In the future, as new dtypes are added that support ``pd.NA``, the results of this method will change to support those new dtypes.  Examples -------- >>> df = pd.DataFrame( ...     { ...         \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")), ...         \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")), ...         \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")), ...         \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")), ...         \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")), ...         \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")), ...     } ... )  Start with a DataFrame with default dtypes.  >>> df a  b      c    d     e      f 0  1  x   True    h  10.0    NaN 1  2  y  False    i   NaN  100.5 2  3  z    NaN  NaN  20.0  200.0  >>> df.dtypes a      int32 b     object c     object d     object e    float64 f    float64 dtype: object  Convert the DataFrame to use best possible dtypes.  >>> dfn = df.convert_dtypes() >>> dfn a  b      c     d     e      f 0  1  x   True     h    10   <NA> 1  2  y  False     i  <NA>  100.5 2  3  z   <NA>  <NA>    20  200.0  >>> dfn.dtypes a             Int32 b    string[python] c           boolean d    string[python] e             Int64 f           Float64 dtype: object  Start with a Series of strings and missing data represented by ``np.nan``.  >>> s = pd.Series([\"a\", \"b\", np.nan]) >>> s 0      a 1      b 2    NaN dtype: object  Obtain a Series with dtype ``StringDtype``.  >>> s.convert_dtypes() 0       a 1       b 2    <NA> dtype: string",
            "source": "@final\n    def convert_dtypes(\n        self,\n        infer_objects: bool_t = True,\n        convert_string: bool_t = True,\n        convert_integer: bool_t = True,\n        convert_boolean: bool_t = True,\n        convert_floating: bool_t = True,\n        dtype_backend: DtypeBackend = \"numpy_nullable\",\n    ) -> Self:\n        \n        check_dtype_backend(dtype_backend)\n        if self.ndim == 1:\n            return self._convert_dtypes(\n                infer_objects,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n                dtype_backend=dtype_backend,\n            )\n        else:\n            results = [\n                col._convert_dtypes(\n                    infer_objects,\n                    convert_string,\n                    convert_integer,\n                    convert_boolean,\n                    convert_floating,\n                    dtype_backend=dtype_backend,\n                )\n                for col_name, col in self.items()\n            ]\n            if len(results) > 0:\n                result = concat(results, axis=1, copy=False, keys=self.columns)\n                cons = cast(type[\"DataFrame\"], self._constructor)\n                result = cons(result)\n                result = result.__finalize__(self, method=\"convert_dtypes\")\n                # https://github.com/python/mypy/issues/8354\n                return cast(Self, result)\n            else:\n                return self.copy(deep=None)"
        },
        {
            "name": "DataFrame.infer_objects",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.infer_objects.html",
            "description": "Attempt to infer better dtypes for object columns.  Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.  Parameters ---------- copy : bool, default True Whether to make a copy for non-object or non-inferable columns or Series.  Returns ------- same type as input object  See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype.  Examples -------- >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]}) >>> df = df.iloc[1:] >>> df A 1  1 2  2 3  3  >>> df.dtypes A    object dtype: object  >>> df.infer_objects().dtypes A    int64 dtype: object",
            "source": "@final\n    def infer_objects(self, copy: bool_t | None = None) -> Self:\n        \n        new_mgr = self._mgr.convert(copy=copy)\n        res = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n        return res.__finalize__(self, method=\"infer_objects\")"
        },
        {
            "name": "DataFrame.copy",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html",
            "description": "Make a copy of this object's indices and data.  When ``deep=True`` (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below).  When ``deep=False``, a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa).  Parameters ---------- deep : bool, default True Make a deep copy, including a copy of the data and the indices. With ``deep=False`` neither the indices nor the data are copied.  Returns ------- Series or DataFrame Object type matches caller.  Notes ----- When ``deep=True``, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to `copy.deepcopy` in the Standard Library, which recursively copies object data (see examples below).  While ``Index`` objects are copied when ``deep=True``, the underlying numpy array is not copied for performance reasons. Since ``Index`` is immutable, the underlying data can be safely shared and a copy is not needed.  Since pandas is not thread safe, see the :ref:`gotchas <gotchas.thread-safety>` when copying in a threading environment.  When ``copy_on_write`` in pandas config is set to ``True``, the ``copy_on_write`` config takes effect even when ``deep=False``. This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See :ref:`Copy_on_Write <copy_on_write>` for more information.  Examples -------- >>> s = pd.Series([1, 2], index=[\"a\", \"b\"]) >>> s a    1 b    2 dtype: int64  >>> s_copy = s.copy() >>> s_copy a    1 b    2 dtype: int64  **Shallow copy versus default (deep) copy:**  >>> s = pd.Series([1, 2], index=[\"a\", \"b\"]) >>> deep = s.copy() >>> shallow = s.copy(deep=False)  Shallow copy shares data and index with original.  >>> s is shallow False >>> s.values is shallow.values and s.index is shallow.index True  Deep copy has own copy of data and index.  >>> s is deep False >>> s.values is deep.values or s.index is deep.index False  Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged.  >>> s.iloc[0] = 3 >>> shallow.iloc[1] = 4 >>> s a    3 b    4 dtype: int64 >>> shallow a    3 b    4 dtype: int64 >>> deep a    1 b    2 dtype: int64  Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy.  >>> s = pd.Series([[1, 2], [3, 4]]) >>> deep = s.copy() >>> s[0][0] = 10 >>> s 0    [10, 2] 1     [3, 4] dtype: object >>> deep 0    [10, 2] 1     [3, 4] dtype: object  ** Copy-on-Write is set to true: **  >>> with pd.option_context(\"mode.copy_on_write\", True): ...     s = pd.Series([1, 2], index=[\"a\", \"b\"]) ...     copy = s.copy(deep=False) ...     s.iloc[0] = 100 ...     s a    100 b      2 dtype: int64 >>> copy a    1 b    2 dtype: int64",
            "source": "@final\n    def copy(self, deep: bool_t | None = True) -> Self:\n        \n        data = self._mgr.copy(deep=deep)\n        self._clear_item_cache()\n        return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n            self, method=\"copy\"\n        )"
        },
        {
            "name": "DataFrame.bool",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.bool.html",
            "description": "Return the bool of a single element Series or DataFrame.  .. deprecated:: 2.1.0  bool is deprecated and will be removed in future version of pandas  This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns ------- bool The value in the Series or DataFrame.  See Also -------- Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.  Examples -------- The method will only work for single element objects with a boolean value:  >>> pd.Series([True]).bool()  # doctest: +SKIP True >>> pd.Series([False]).bool()  # doctest: +SKIP False  >>> pd.DataFrame({'col': [True]}).bool()  # doctest: +SKIP True >>> pd.DataFrame({'col': [False]}).bool()  # doctest: +SKIP False",
            "source": "@final\n    def bool(self) -> bool_t:\n        warnings.warn(\n            f\"{type(self).__name__}.bool is now deprecated and will be removed \"\n            \"in future version of pandas\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n        v = self.squeeze()\n        if isinstance(v, (bool, np.bool_)):\n            return bool(v)\n        elif is_scalar(v):\n            raise ValueError(\n                \"bool cannot act on a non-boolean single element \"\n                f\"{type(self).__name__}\"\n            )\n\n        self.__nonzero__()\n        # for mypy (__nonzero__ raises)\n        return True"
        },
        {
            "name": "DataFrame.head",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html",
            "description": "Return the first `n` rows.  This function returns the first `n` rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.  For negative values of `n`, this function returns all rows except the last `|n|` rows, equivalent to ``df[:n]``.  If n is larger than the number of rows, this function returns all rows.  Parameters ---------- n : int, default 5 Number of rows to select.  Returns ------- same type as caller The first `n` rows of the caller object.  See Also -------- DataFrame.tail: Returns the last `n` rows.  Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey 5     parrot 6      shark 7      whale 8      zebra  Viewing the first 5 lines  >>> df.head() animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey  Viewing the first `n` lines (three in this case)  >>> df.head(3) animal 0  alligator 1        bee 2     falcon  For negative values of `n`  >>> df.head(-3) animal 0  alligator 1        bee 2     falcon 3       lion 4     monkey 5     parrot",
            "source": "@final\n    def head(self, n: int = 5) -> Self:\n        \n        if using_copy_on_write():\n            return self.iloc[:n].copy()\n        return self.iloc[:n]"
        },
        {
            "name": "DataFrame.iat",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iat.html",
            "description": "Access a single value for a row/column pair by integer position.  Similar to ``iloc``, in that both provide integer-based lookups. Use ``iat`` if you only need to get or set a single value in a DataFrame or Series.  Raises ------ IndexError When integer position is out of bounds.  See Also -------- DataFrame.at : Access a single value for a row/column label pair. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s).  Examples -------- >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ...                   columns=['A', 'B', 'C']) >>> df A   B   C 0   0   2   3 1   0   4   1 2  10  20  30  Get value at specified row/column pair  >>> df.iat[1, 2] 1  Set value at specified row/column pair  >>> df.iat[1, 2] = 10 >>> df.iat[1, 2] 10  Get value within a series  >>> df.loc[0].iat[1] 2",
            "source": "@property\n    def iat(self) -> _iAtIndexer:\n        \n        return _iAtIndexer(\"iat\", self)"
        },
        {
            "name": "DataFrame.loc",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html",
            "description": "Access a group of rows and columns by label(s) or a boolean array.  ``.loc[]`` is primarily label based, but may also be used with a boolean array.  Allowed inputs are:  - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is interpreted as a *label* of the index, and **never** as an integer position along the index). - A list or array of labels, e.g. ``['a', 'b', 'c']``. - A slice object with labels, e.g. ``'a':'f'``.  .. warning:: Note that contrary to usual python slices, **both** the start and the stop are included  - A boolean array of the same length as the axis being sliced, e.g. ``[True, False, True]``. - An alignable boolean Series. The index of the key will be aligned before masking. - An alignable Index. The Index of the returned selection will be the input. - A ``callable`` function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)  See more at :ref:`Selection by Label <indexing.label>`.  Raises ------ KeyError If any items are not found. IndexingError If an indexed key is passed and its index is unalignable to the frame index.  See Also -------- DataFrame.at : Access a single value for a row/column label pair. DataFrame.iloc : Access group of rows and columns by integer position(s). DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. Series.loc : Access group of values using labels.  Examples -------- **Getting values**  >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ...      index=['cobra', 'viper', 'sidewinder'], ...      columns=['max_speed', 'shield']) >>> df max_speed  shield cobra               1       2 viper               4       5 sidewinder          7       8  Single label. Note this returns the row as a Series.  >>> df.loc['viper'] max_speed    4 shield       5 Name: viper, dtype: int64  List of labels. Note using ``[[]]`` returns a DataFrame.  >>> df.loc[['viper', 'sidewinder']] max_speed  shield viper               4       5 sidewinder          7       8  Single label for row and column  >>> df.loc['cobra', 'shield'] 2  Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included.  >>> df.loc['cobra':'viper', 'max_speed'] cobra    1 viper    4 Name: max_speed, dtype: int64  Boolean list with the same length as the row axis  >>> df.loc[[False, False, True]] max_speed  shield sidewinder          7       8  Alignable boolean Series:  >>> df.loc[pd.Series([False, True, False], ...        index=['viper', 'sidewinder', 'cobra'])] max_speed  shield sidewinder          7       8  Index (same behavior as ``df.reindex``)  >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")] max_speed  shield foo cobra          1       2 viper          4       5  Conditional that returns a boolean Series  >>> df.loc[df['shield'] > 6] max_speed  shield sidewinder          7       8  Conditional that returns a boolean Series with column labels specified  >>> df.loc[df['shield'] > 6, ['max_speed']] max_speed sidewinder          7  Multiple conditional using ``&`` that returns a boolean Series  >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)] max_speed  shield viper          4       5  Multiple conditional using ``|`` that returns a boolean Series  >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)] max_speed  shield cobra               1       2 sidewinder          7       8  Please ensure that each condition is wrapped in parentheses ``()``. See the :ref:`user guide<indexing.boolean>` for more details and explanations of Boolean indexing.  .. note:: If you find yourself using 3 or more conditionals in ``.loc[]``, consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.  See below for using ``.loc[]`` on MultiIndex DataFrames.  Callable that returns a boolean Series  >>> df.loc[lambda df: df['shield'] == 8] max_speed  shield sidewinder          7       8  **Setting values**  Set value for all items matching the list of labels  >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50 >>> df max_speed  shield cobra               1       2 viper               4      50 sidewinder          7      50  Set value for an entire row  >>> df.loc['cobra'] = 10 >>> df max_speed  shield cobra              10      10 viper               4      50 sidewinder          7      50  Set value for an entire column  >>> df.loc[:, 'max_speed'] = 30 >>> df max_speed  shield cobra              30      10 viper              30      50 sidewinder         30      50  Set value for rows matching callable condition  >>> df.loc[df['shield'] > 35] = 0 >>> df max_speed  shield cobra              30      10 viper               0       0 sidewinder          0       0  Add value matching location  >>> df.loc[\"viper\", \"shield\"] += 5 >>> df max_speed  shield cobra              30      10 viper               0       5 sidewinder          0       0  Setting using a ``Series`` or a ``DataFrame`` sets the values matching the index labels, not the index positions.  >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]] >>> df.loc[:] += shuffled_df >>> df max_speed  shield cobra              60      20 viper               0      10 sidewinder          0       0  **Getting values on a DataFrame with an index that has integer labels**  Another example using integers for the index  >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ...      index=[7, 8, 9], columns=['max_speed', 'shield']) >>> df max_speed  shield 7          1       2 8          4       5 9          7       8  Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included.  >>> df.loc[7:9] max_speed  shield 7          1       2 8          4       5 9          7       8  **Getting values with a MultiIndex**  A number of examples using a DataFrame with a MultiIndex  >>> tuples = [ ...    ('cobra', 'mark i'), ('cobra', 'mark ii'), ...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'), ...    ('viper', 'mark ii'), ('viper', 'mark iii') ... ] >>> index = pd.MultiIndex.from_tuples(tuples) >>> values = [[12, 2], [0, 4], [10, 20], ...         [1, 4], [7, 1], [16, 36]] >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index) >>> df max_speed  shield cobra      mark i           12       2 mark ii           0       4 sidewinder mark i           10      20 mark ii           1       4 viper      mark ii           7       1 mark iii         16      36  Single label. Note this returns a DataFrame with a single index.  >>> df.loc['cobra'] max_speed  shield mark i          12       2 mark ii          0       4  Single index tuple. Note this returns a Series.  >>> df.loc[('cobra', 'mark ii')] max_speed    0 shield       4 Name: (cobra, mark ii), dtype: int64  Single label for row and column. Similar to passing in a tuple, this returns a Series.  >>> df.loc['cobra', 'mark i'] max_speed    12 shield        2 Name: (cobra, mark i), dtype: int64  Single tuple. Note using ``[[]]`` returns a DataFrame.  >>> df.loc[[('cobra', 'mark ii')]] max_speed  shield cobra mark ii          0       4  Single tuple for the index with a single label for the column  >>> df.loc[('cobra', 'mark i'), 'shield'] 2  Slice from index tuple to single label  >>> df.loc[('cobra', 'mark i'):'viper'] max_speed  shield cobra      mark i           12       2 mark ii           0       4 sidewinder mark i           10      20 mark ii           1       4 viper      mark ii           7       1 mark iii         16      36  Slice from index tuple to index tuple  >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')] max_speed  shield cobra      mark i          12       2 mark ii          0       4 sidewinder mark i          10      20 mark ii          1       4 viper      mark ii          7       1  Please see the :ref:`user guide<advanced.advanced_hierarchical>` for more details and explanations of advanced indexing.",
            "source": "@property\n    def loc(self) -> _LocIndexer:\n        \n        return _LocIndexer(\"loc\", self)"
        },
        {
            "name": "DataFrame.insert",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html",
            "description": "Insert column into DataFrame at specified location.  Raises a ValueError if `column` is already contained in the DataFrame, unless `allow_duplicates` is set to True.  Parameters ---------- loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : Scalar, Series, or array-like allow_duplicates : bool, optional, default lib.no_default  See Also -------- Index.insert : Insert new item by index.  Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df col1  col2 0     1     3 1     2     4 >>> df.insert(1, \"newcol\", [99, 99]) >>> df col1  newcol  col2 0     1      99     3 1     2      99     4 >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True) >>> df col1  col1  newcol  col2 0   100     1      99     3 1   100     2      99     4  Notice that pandas uses index alignment in case of `value` from type `Series`:  >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2])) >>> df col0  col1  col1  newcol  col2 0   NaN   100     1      99     3 1   5.0   100     2      99     4",
            "source": "def insert(\n        self,\n        loc: int,\n        column: Hashable,\n        value: Scalar | AnyArrayLike,\n        allow_duplicates: bool | lib.NoDefault = lib.no_default,\n    ) -> None:\n        \n        if allow_duplicates is lib.no_default:\n            allow_duplicates = False\n        if allow_duplicates and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'allow_duplicates=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n        if not allow_duplicates and column in self.columns:\n            # Should this be a different kind of error??\n            raise ValueError(f\"cannot insert {column}, already exists\")\n        if not is_integer(loc):\n            raise TypeError(\"loc must be int\")\n        # convert non stdlib ints to satisfy typing checks\n        loc = int(loc)\n        if isinstance(value, DataFrame) and len(value.columns) > 1:\n            raise ValueError(\n                f\"Expected a one-dimensional object, got a DataFrame with \"\n                f\"{len(value.columns)} columns instead.\"\n            )\n        elif isinstance(value, DataFrame):\n            value = value.iloc[:, 0]\n\n        value, refs = self._sanitize_column(value)\n        self._mgr.insert(loc, column, value, refs=refs)"
        },
        {
            "name": "DataFrame.keys",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.keys.html",
            "description": "Get the 'info axis' (see Indexing for more).  This is index for Series, columns for DataFrame.  Returns ------- Index Info axis.  Examples -------- >>> d = pd.DataFrame(data={'A': [1, 2, 3], 'B': [0, 4, 8]}, ...                  index=['a', 'b', 'c']) >>> d A  B a  1  0 b  2  4 c  3  8 >>> d.keys() Index(['A', 'B'], dtype='object')",
            "source": "def keys(self) -> Index:\n        \n        return self._info_axis"
        },
        {
            "name": "DataFrame.pop",
            "url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pop.html",
            "description": "Return item and drop from frame. Raise KeyError if not found.  Parameters ---------- item : label Label of column to be popped.  Returns ------- Series  Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ...                    ('parrot', 'bird', 24.0), ...                    ('lion', 'mammal', 80.5), ...                    ('monkey', 'mammal', np.nan)], ...                   columns=('name', 'class', 'max_speed')) >>> df name   class  max_speed 0  falcon    bird      389.0 1  parrot    bird       24.0 2    lion  mammal       80.5 3  monkey  mammal        NaN  >>> df.pop('class') 0      bird 1      bird 2    mammal 3    mammal Name: class, dtype: object  >>> df name  max_speed 0  falcon      389.0 1  parrot       24.0 2    lion       80.5 3  monkey        NaN",
            "source": "def pop(self, item: Hashable) -> Series:\n        \n        return super().pop(item=item)"
        }
    ],
    [
        {
            "name": "tokenizer.convert_ids_to_tokens",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens",
            "description": "Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.  Args: ids (`int` or `List[int]`): The token id (or token ids) to convert to tokens. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding.  Returns: `str` or `List[str]`: The decoded token(s).",
            "source": "def convert_ids_to_tokens(\n        self, ids: Union[int, List[int]], skip_special_tokens: bool = False\n    ) -> Union[str, List[str]]:\n        \n        if isinstance(ids, int):\n            if ids in self._added_tokens_decoder:\n                return self._added_tokens_decoder[ids].content\n            else:\n                return self._convert_id_to_token(ids)\n        tokens = []\n        for index in ids:\n            index = int(index)\n            if skip_special_tokens and index in self.all_special_ids:\n                continue\n            if index in self._added_tokens_decoder:\n                tokens.append(self._added_tokens_decoder[index].content)\n            else:\n                tokens.append(self._convert_id_to_token(index))\n        return tokens"
        },
        {
            "name": "tokenizer.add_tokens",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens",
            "description": "Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to it with indices starting from length of the current vocabulary and and will be isolated before the tokenization algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore not treated in the same way.  Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer.  In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.  Args: new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`): Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string token to let you personalize its behavior: whether this token should only match against a single word, whether this token should strip all potential whitespaces on the left side, whether this token should strip all potential whitespaces on the right side, etc. special_tokens (`bool`, *optional*, defaults to `False`): Can be used to specify if the token is a special token. This mostly change the normalization behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).  See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.  Returns: `int`: Number of tokens added to the vocabulary.  Examples:  ```python # Let's see how to increase the vocabulary of Bert model and tokenizer tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") model = BertModel.from_pretrained(\"bert-base-uncased\")  num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"]) print(\"We have added\", num_added_toks, \"tokens\") # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer. model.resize_token_embeddings(len(tokenizer)) ```",
            "source": "def add_tokens(\n        self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False\n    ) -> int:\n        \n        if not new_tokens:\n            return 0\n\n        if not isinstance(new_tokens, (list, tuple)):\n            new_tokens = [new_tokens]\n\n        return self._add_tokens(new_tokens, special_tokens=special_tokens)"
        },
        {
            "name": "tokenizer.add_special_tokens",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_special_tokens",
            "description": "Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).  When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer.  In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.  Using `add_special_tokens` will ensure your special tokens can be used in several ways:  - Special tokens can be skipped when decoding using `skip_special_tokens = True`. - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`. - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.  When possible, special tokens are already registered for provided pretrained models (for instance [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be `'</s>'`).  Args: special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`): Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].  Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the `unk_token` to them). replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`): If `True`, the existing list of additional special tokens will be replaced by the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is just extended. In the former case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous `additional_special_tokens` are still added tokens, and will not be split by the model.  Returns: `int`: Number of tokens added to the vocabulary.  Examples:  ```python # Let's see how to add a new classification token to GPT-2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") model = GPT2Model.from_pretrained(\"gpt2\")  special_tokens_dict = {\"cls_token\": \"<CLS>\"}  num_added_toks = tokenizer.add_special_tokens(special_tokens_dict) print(\"We have added\", num_added_toks, \"tokens\") # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer. model.resize_token_embeddings(len(tokenizer))  assert tokenizer.cls_token == \"<CLS>\" ```",
            "source": "def add_special_tokens(\n        self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True\n    ) -> int:\n        \n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = []\n        for key, value in special_tokens_dict.items():\n            assert key in self.SPECIAL_TOKENS_ATTRIBUTES, f\"Key {key} is not a special token\"\n\n            if self.verbose:\n                logger.info(f\"Assigning {value} to the {key} key of the tokenizer\")\n\n            if key == \"additional_special_tokens\":\n                assert isinstance(value, (list, tuple)) and all(\n                    isinstance(t, (str, AddedToken)) for t in value\n                ), f\"Tokens {value} for key {key} should all be str or AddedToken instances\"\n\n                to_add = set()\n                for token in value:\n                    if isinstance(token, str):\n                        # for legacy purpose we default to stripping. `test_add_tokens_tokenizer` depends on this\n                        token = AddedToken(token, rstrip=False, lstrip=False, normalized=False, special=True)\n                    if str(token) not in self.additional_special_tokens:\n                        to_add.add(token)\n                if replace_additional_special_tokens:\n                    setattr(self, key, list(to_add))\n                else:\n                    self._additional_special_tokens.extend(to_add)\n                added_tokens += to_add\n\n            else:\n                if not isinstance(value, (str, AddedToken)):\n                    raise ValueError(f\"Token {value} for key {key} should be a str or an AddedToken instance\")\n                if isinstance(value, (str)):\n                    # for legacy purpose we default to stripping. `False` depends on this\n                    value = AddedToken(value, rstrip=False, lstrip=False, normalized=False, special=True)\n                if isinstance(value, AddedToken):\n                    setattr(self, key, value)\n                if value not in added_tokens:\n                    added_tokens.append(value)\n\n        # if we are adding tokens that were not part of the vocab, we ought to add them\n        added_tokens = self.add_tokens(added_tokens, special_tokens=True)\n        return added_tokens"
        },
        {
            "name": "tokenizer.apply_chat_template",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.apply_chat_template",
            "description": "Converts a Conversation object or a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to determine the format and control tokens to use when converting. When chat_template is None, it will fall back to the default_chat_template specified at the class level.  Args: conversation (Union[List[Dict[str, str]], \"Conversation\"]): A Conversation object or list of dicts with \"role\" and \"content\" keys, representing the chat history so far. chat_template (str, *optional*): A Jinja template to use for this conversion. If this is not passed, the model's default chat template will be used instead. add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate the start of an assistant message. This is useful when you want to generate a response from the model. Note that this argument will be passed to the chat template, and so it must be supported in the template for this argument to have any effect. tokenize (`bool`, defaults to `True`): Whether to tokenize the output. If `False`, the output will be a string. padding (`bool`, defaults to `False`): Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`. truncation (`bool`, defaults to `False`): Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`. max_length (`int`, *optional*): Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizer's `max_length` attribute will be used as a default. return_tensors (`str` or [`~utils.TensorType`], *optional*): If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable values are: - `'tf'`: Return TensorFlow `tf.Tensor` objects. - `'pt'`: Return PyTorch `torch.Tensor` objects. - `'np'`: Return NumPy `np.ndarray` objects. - `'jax'`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs: Additional kwargs to pass to the tokenizer.  Returns: `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This output is ready to pass to the model, either directly or via methods like `generate()`.",
            "source": "def apply_chat_template(\n        self,\n        conversation: Union[List[Dict[str, str]], \"Conversation\"],\n        chat_template: Optional[str] = None,\n        add_generation_prompt: bool = False,\n        tokenize: bool = True,\n        padding: bool = False,\n        truncation: bool = False,\n        max_length: Optional[int] = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        **tokenizer_kwargs,\n    ) -> Union[str, List[int]]:\n        \n        if hasattr(conversation, \"messages\"):\n            # Indicates it's a Conversation object\n            conversation = conversation.messages\n\n        # priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template`\n        if chat_template is None:\n            if self.chat_template is not None:\n                chat_template = self.chat_template\n            else:\n                chat_template = self.default_chat_template\n\n        # Compilation function uses a cache to avoid recompiling the same template\n        compiled_template = self._compile_jinja_template(chat_template)\n\n        rendered = compiled_template.render(\n            messages=conversation, add_generation_prompt=add_generation_prompt, **self.special_tokens_map\n        )\n\n        if padding is True:\n            padding = \"max_length\"  # There's only one sequence here, so \"longest\" makes no sense\n        if tokenize:\n            return self.encode(\n                rendered,\n                add_special_tokens=False,\n                padding=padding,\n                truncation=truncation,\n                max_length=max_length,\n                return_tensors=return_tensors,\n                **tokenizer_kwargs,\n            )\n        else:\n            return rendered"
        },
        {
            "name": "tokenizer.batch_decode",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.batch_decode",
            "description": "Convert a list of lists of token ids into a list of strings by calling decode.  Args: sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`): List of tokenized input ids. Can be obtained using the `__call__` method. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding. clean_up_tokenization_spaces (`bool`, *optional*): Whether or not to clean up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`. kwargs (additional keyword arguments, *optional*): Will be passed to the underlying model specific decode method.  Returns: `List[str]`: The list of decoded sentences.",
            "source": "def batch_decode(\n        self,\n        sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n        skip_special_tokens: bool = False,\n        clean_up_tokenization_spaces: bool = None,\n        **kwargs,\n    ) -> List[str]:\n        \n        return [\n            self.decode(\n                seq,\n                skip_special_tokens=skip_special_tokens,\n                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n                **kwargs,\n            )\n            for seq in sequences\n        ]"
        },
        {
            "name": "tokenizer.decode",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode",
            "description": "Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special tokens and clean up tokenization spaces.  Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.  Args: token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`): List of tokenized input ids. Can be obtained using the `__call__` method. skip_special_tokens (`bool`, *optional*, defaults to `False`): Whether or not to remove special tokens in the decoding. clean_up_tokenization_spaces (`bool`, *optional*): Whether or not to clean up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`. kwargs (additional keyword arguments, *optional*): Will be passed to the underlying model specific decode method.  Returns: `str`: The decoded sentence.",
            "source": "def decode(\n        self,\n        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n        skip_special_tokens: bool = False,\n        clean_up_tokenization_spaces: bool = None,\n        **kwargs,\n    ) -> str:\n        \n        # Convert inputs to python lists\n        token_ids = to_py_obj(token_ids)\n\n        return self._decode(\n            token_ids=token_ids,\n            skip_special_tokens=skip_special_tokens,\n            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n            **kwargs,\n        )"
        },
        {
            "name": "tokenizer.encode",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode",
            "description": "Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.  Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.  Args: text (`str`, `List[str]` or `List[int]`): The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids` method). text_pair (`str`, `List[str]` or `List[int]`, *optional*): Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids` method).",
            "source": "def encode(\n        self,\n        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n        add_special_tokens: bool = True,\n        padding: Union[bool, str, PaddingStrategy] = False,\n        truncation: Union[bool, str, TruncationStrategy] = None,\n        max_length: Optional[int] = None,\n        stride: int = 0,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        **kwargs,\n    ) -> List[int]:\n        \n        encoded_inputs = self.encode_plus(\n            text,\n            text_pair=text_pair,\n            add_special_tokens=add_special_tokens,\n            padding=padding,\n            truncation=truncation,\n            max_length=max_length,\n            stride=stride,\n            return_tensors=return_tensors,\n            **kwargs,\n        )\n\n        return encoded_inputs[\"input_ids\"]"
        },
        {
            "name": "tokenizer.push_to_hub",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.push_to_hub",
            "description": "Upload the {object_files} to the \ud83e\udd17 Model Hub.  Parameters: repo_id (`str`): The name of the repository you want to push your {object} to. It should contain your organization name when pushing to a given organization. use_temp_dir (`bool`, *optional*): Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub. Will default to `True` if there is no directory named like `repo_id`, `False` otherwise. commit_message (`str`, *optional*): Message to commit while pushing. Will default to `\"Upload {object}\"`. private (`bool`, *optional*): Whether or not the repository created should be private. token (`bool` or `str`, *optional*): The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not specified. max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`): Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier Google Colab instances without any CPU OOM issues. create_pr (`bool`, *optional*, defaults to `False`): Whether or not to create a PR with the uploaded files or directly commit. safe_serialization (`bool`, *optional*, defaults to `True`): Whether or not to convert the model weights in safetensors format for safer serialization. revision (`str`, *optional*): Branch to push the uploaded files to. commit_description (`str`, *optional*): The description of the commit that will be created  Examples:  ```python from transformers import {object_class}  {object} = {object_class}.from_pretrained(\"bert-base-cased\")  # Push the {object} to your namespace with the name \"my-finetuned-bert\". {object}.push_to_hub(\"my-finetuned-bert\")  # Push the {object} to an organization with the name \"my-finetuned-bert\". {object}.push_to_hub(\"huggingface/my-finetuned-bert\") ```",
            "source": "def push_to_hub(\n        self,\n        repo_id: str,\n        use_temp_dir: Optional[bool] = None,\n        commit_message: Optional[str] = None,\n        private: Optional[bool] = None,\n        token: Optional[Union[bool, str]] = None,\n        max_shard_size: Optional[Union[int, str]] = \"5GB\",\n        create_pr: bool = False,\n        safe_serialization: bool = True,\n        revision: str = None,\n        commit_description: str = None,\n        **deprecated_kwargs,\n    ) -> str:\n        \n        use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n        if use_auth_token is not None:\n            warnings.warn(\n                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                FutureWarning,\n            )\n            if token is not None:\n                raise ValueError(\n                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                )\n            token = use_auth_token\n\n        repo_path_or_name = deprecated_kwargs.pop(\"repo_path_or_name\", None)\n        if repo_path_or_name is not None:\n            # Should use `repo_id` instead of `repo_path_or_name`. When using `repo_path_or_name`, we try to infer\n            # repo_id from the folder path, if it exists.\n            warnings.warn(\n                \"The `repo_path_or_name` argument is deprecated and will be removed in v5 of Transformers. Use \"\n                \"`repo_id` instead.\",\n                FutureWarning,\n            )\n            if repo_id is not None:\n                raise ValueError(\n                    \"`repo_id` and `repo_path_or_name` are both specified. Please set only the argument `repo_id`.\"\n                )\n            if os.path.isdir(repo_path_or_name):\n                # repo_path: infer repo_id from the path\n                repo_id = repo_id.split(os.path.sep)[-1]\n                working_dir = repo_id\n            else:\n                # repo_name: use it as repo_id\n                repo_id = repo_path_or_name\n                working_dir = repo_id.split(\"/\")[-1]\n        else:\n            # Repo_id is passed correctly: infer working_dir from it\n            working_dir = repo_id.split(\"/\")[-1]\n\n        # Deprecation warning will be sent after for repo_url and organization\n        repo_url = deprecated_kwargs.pop(\"repo_url\", None)\n        organization = deprecated_kwargs.pop(\"organization\", None)\n\n        repo_id = self._create_repo(\n            repo_id, private=private, token=token, repo_url=repo_url, organization=organization\n        )\n\n        if use_temp_dir is None:\n            use_temp_dir = not os.path.isdir(working_dir)\n\n        with working_or_temp_dir(working_dir=working_dir, use_temp_dir=use_temp_dir) as work_dir:\n            files_timestamps = self._get_files_timestamps(work_dir)\n\n            # Save all files.\n            self.save_pretrained(work_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n\n            return self._upload_modified_files(\n                work_dir,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=token,\n                create_pr=create_pr,\n                revision=revision,\n                commit_description=commit_description,\n            )"
        },
        {
            "name": "tokenizer.convert_tokens_to_ids",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids",
            "description": "Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.  Args: tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).  Returns: `int` or `List[int]`: The token id or list of token ids.",
            "source": "def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:\n        \n        if tokens is None:\n            return None\n\n        if isinstance(tokens, str):\n            return self._convert_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._convert_token_to_id_with_added_voc(token))\n        return ids"
        },
        {
            "name": "tokenizer.get_added_vocab",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.get_added_vocab",
            "description": "Returns the added tokens in the vocabulary as a dictionary of token to index. Results might be different from the fast call because for now we always add the tokens even if they are already in the vocabulary. This is something we should change.  Returns: `Dict[str, int]`: The added tokens.",
            "source": "def get_added_vocab(self) -> Dict[str, int]:\n        \n        return self._added_tokens_encoder"
        },
        {
            "name": "tokenizer.num_special_tokens_to_add",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.num_special_tokens_to_add",
            "description": "Returns the number of added tokens when encoding a sequence with special tokens.  <Tip>  This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put this inside your training loop.  </Tip>  Args: pair (`bool`, *optional*, defaults to `False`): Whether the number of added tokens should be computed in the case of a sequence pair or a single sequence.  Returns: `int`: Number of special tokens added to sequences.",
            "source": "def num_special_tokens_to_add(self, pair: bool = False) -> int:\n        \n        token_ids_0 = []\n        token_ids_1 = []\n        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))"
        },
        {
            "name": "tokenizer.prepare_for_tokenization",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.prepare_for_tokenization",
            "description": "Performs any necessary transformations before tokenization.  This method should pop the arguments from kwargs and return the remaining `kwargs` as well. We test the `kwargs` at the end of the encoding process to be sure all the arguments have been used.  Args: text (`str`): The text to prepare. is_split_into_words (`bool`, *optional*, defaults to `False`): Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize. This is useful for NER or token classification. kwargs (`Dict[str, Any]`, *optional*): Keyword arguments to use for the tokenization.  Returns: `Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.",
            "source": "def prepare_for_tokenization(\n        self, text: str, is_split_into_words: bool = False, **kwargs\n    ) -> Tuple[str, Dict[str, Any]]:\n        \n        return (text, kwargs)"
        },
        {
            "name": "tokenizer.tokenize",
            "url": "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.tokenize",
            "description": "Converts a string in a sequence of tokens, using the tokenizer.  Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces). Takes care of added tokens.  Args: text (`str`): The sequence to be encoded. **kwargs (additional keyword arguments): Passed along to the model-specific `prepare_for_tokenization` preprocessing method.  Returns: `List[str]`: The list of tokens.",
            "source": "def tokenize(self, text: TextInput, **kwargs) -> List[str]:\n        \n        split_special_tokens = kwargs.pop(\"split_special_tokens\", self.split_special_tokens)\n\n        text, kwargs = self.prepare_for_tokenization(text, **kwargs)\n\n        if kwargs:\n            logger.warning(f\"Keyword arguments {kwargs} not recognized.\")\n\n        if hasattr(self, \"do_lower_case\") and self.do_lower_case:\n            # convert non-special tokens to lowercase. Might be super slow as well?\n            escaped_special_toks = [re.escape(s_tok) for s_tok in (self.all_special_tokens)]\n            escaped_special_toks += [\n                re.escape(s_tok.content)\n                for s_tok in (self._added_tokens_decoder.values())\n                if not s_tok.special and s_tok.normalized\n            ]\n            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n            text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n\n        if split_special_tokens:\n            no_split_token = []\n            tokens = [text]\n        else:\n            no_split_token = self._added_tokens_encoder.keys()  # don't split on any of the added tokens\n            # \"This is something<special_token_1>  else\"\n            tokens = self.tokens_trie.split(text)\n\n        # [\"This is something\", \"<special_token_1>\", \"  else\"]\n        for i, token in enumerate(tokens):\n            if token in no_split_token:\n                tok_extended = self._added_tokens_decoder.get(self._added_tokens_encoder[token], None)\n                left = tokens[i - 1] if i > 0 else None\n                right = tokens[i + 1] if i < len(tokens) - 1 else None\n                if isinstance(tok_extended, AddedToken):\n                    if tok_extended.rstrip and right:\n                        # A bit counter-intuitive but we strip the left of the string\n                        # since tok_extended.rstrip means the special token is eating all white spaces on its right\n                        tokens[i + 1] = right.lstrip()\n                    # Strip white spaces on the left\n                    if tok_extended.lstrip and left:\n                        tokens[i - 1] = left.rstrip()  # Opposite here\n                    if tok_extended.single_word and left and left[-1] != \" \":\n                        tokens[i - 1] += token\n                        tokens[i] = \"\"\n                    elif tok_extended.single_word and right and right[0] != \" \":\n                        tokens[i + 1] = token + tokens[i + 1]\n                        tokens[i] = \"\"\n                else:\n                    raise ValueError(\n                        f\"{tok_extended} cannot be tokenized because it was not properly added\"\n                        f\" to the tokenizer. This means that it is not an `AddedToken` but a {type(tok_extended)}\"\n                    )\n        # [\"This is something\", \"<special_token_1>\", \"else\"]\n        tokenized_text = []\n        for token in tokens:\n            # Need to skip eventual empty (fully stripped) tokens\n            if not token:\n                continue\n            if token in no_split_token:\n                tokenized_text.append(token)\n            else:\n                tokenized_text.extend(self._tokenize(token))\n        # [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\n        return tokenized_text"
        }
    ],
    [
        {
            "name": "numpy.ptp",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.ptp.html",
            "description": "Range of values (maximum - minimum) along an axis.  The name of the function comes from the acronym for 'peak to peak'.  .. warning:: `ptp` preserves the data type of the array. This means the return value for an input of signed integers with n bits (e.g. `np.int8`, `np.int16`, etc) is also a signed integer with n bits.  In that case, peak-to-peak values greater than ``2**(n-1)-1`` will be returned as negative values. An example with a work-around is shown below.  Parameters ---------- a : array_like Input values. axis : None or int or tuple of ints, optional Axis along which to find the peaks.  By default, flatten the array.  `axis` may be negative, in which case it counts from the last to the first axis.  .. versionadded:: 1.15.0  If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before. out : array_like Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type of the output values will be cast if necessary.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `ptp` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  Returns ------- ptp : ndarray or scalar The range of a given array - `scalar` if array is one-dimensional or a new array holding the result along the given axis  Examples -------- >>> x = np.array([[4, 9, 2, 10], ...               [6, 9, 7, 12]])  >>> np.ptp(x, axis=1) array([8, 6])  >>> np.ptp(x, axis=0) array([2, 0, 5, 2])  >>> np.ptp(x) 10  This example shows that a negative value can be returned when the input is an array of signed integers.  >>> y = np.array([[1, 127], ...               [0, 127], ...               [-1, 127], ...               [-2, 127]], dtype=np.int8) >>> np.ptp(y, axis=1) array([ 126,  127, -128, -127], dtype=int8)  A work-around is to use the `view()` method to view the result as unsigned integers with the same bit width:  >>> np.ptp(y, axis=1).view(np.uint8) array([126, 127, 128, 129], dtype=uint8)",
            "source": "@array_function_dispatch(_ptp_dispatcher)\ndef ptp(a, axis=None, out=None, keepdims=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if type(a) is not mu.ndarray:\n        try:\n            ptp = a.ptp\n        except AttributeError:\n            pass\n        else:\n            return ptp(axis=axis, out=out, **kwargs)\n    return _methods._ptp(a, axis=axis, out=out, **kwargs)"
        },
        {
            "name": "numpy.percentile",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.percentile.html",
            "description": "Compute the q-th percentile of the data along the specified axis.  Returns the q-th percentile(s) of the array elements.  Parameters ---------- a : array_like of real numbers Input array or object that can be converted to an array. q : array_like of float Percentage or sequence of percentages for the percentiles to compute. Values must be between 0 and 100 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array.  .. versionchanged:: 1.9.0 A tuple of axes is supported out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the percentile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  .. versionadded:: 1.9.0  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- percentile : scalar or ndarray If `q` is a single percentile and `axis=None`, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean median : equivalent to ``percentile(..., 50)`` nanpercentile quantile : equivalent to percentile, except q in the range [0, 1].  Notes ----- Given a vector ``V`` of length ``n``, the q-th percentile of ``V`` is the value ``q/100`` of the way from the minimum to the maximum in a sorted copy of ``V``. The values and distances of the two nearest neighbors as well as the `method` parameter will determine the percentile if the normalized ranking does not match the location of ``q`` exactly. This function is the same as the median if ``q=50``, the same as the minimum if ``q=0`` and the same as the maximum if ``q=100``.  The optional `method` parameter specifies the method to use when the desired percentile lies between two indexes ``i`` and ``j = i + 1``. In that case, we first determine ``i + g``, a virtual index that lies between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the fractional part of the index. The final result is, then, an interpolation of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``, ``i`` and ``j`` are modified using correction constants ``alpha`` and ``beta`` whose choices depend on the ``method`` used. Finally, note that since Python uses 0-based indexing, the code subtracts another 1 from the index internally.  The following formula determines the virtual index ``i + g``, the location of the percentile in the sorted sample:  .. math:: i + g = (q / 100) * ( n - alpha - beta + 1 ) + alpha  The different methods then work as follows  inverted_cdf: method 1 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then take i  averaged_inverted_cdf: method 2 of H&F [1]_. This method give discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then average between bounds  closest_observation: method 3 of H&F [1]_. This method give discontinuous results:  * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i  interpolated_inverted_cdf: method 4 of H&F [1]_. This method give continuous results using:  * alpha = 0 * beta = 1  hazen: method 5 of H&F [1]_. This method give continuous results using:  * alpha = 1/2 * beta = 1/2  weibull: method 6 of H&F [1]_. This method give continuous results using:  * alpha = 0 * beta = 0  linear: method 7 of H&F [1]_. This method give continuous results using:  * alpha = 1 * beta = 1  median_unbiased: method 8 of H&F [1]_. This method is probably the best method if the sample distribution function is unknown (see reference). This method give continuous results using:  * alpha = 1/3 * beta = 1/3  normal_unbiased: method 9 of H&F [1]_. This method is probably the best method if the sample distribution function is known to be normal. This method give continuous results using:  * alpha = 3/8 * beta = 3/8  lower: NumPy method kept for backwards compatibility. Takes ``i`` as the interpolation point.  higher: NumPy method kept for backwards compatibility. Takes ``j`` as the interpolation point.  nearest: NumPy method kept for backwards compatibility. Takes ``i`` or ``j``, whichever is nearest.  midpoint: NumPy method kept for backwards compatibility. Uses ``(i + j) / 2``.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.percentile(a, 50) 3.5 >>> np.percentile(a, 50, axis=0) array([6.5, 4.5, 2.5]) >>> np.percentile(a, 50, axis=1) array([7.,  2.]) >>> np.percentile(a, 50, axis=1, keepdims=True) array([[7.], [2.]])  >>> m = np.percentile(a, 50, axis=0) >>> out = np.zeros_like(m) >>> np.percentile(a, 50, axis=0, out=out) array([6.5, 4.5, 2.5]) >>> m array([6.5, 4.5, 2.5])  >>> b = a.copy() >>> np.percentile(b, 50, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a == b)  The different methods can be visualized graphically:  .. plot::  import matplotlib.pyplot as plt  a = np.arange(4) p = np.linspace(0, 100, 6001) ax = plt.gca() lines = [ ('linear', '-', 'C0'), ('inverted_cdf', ':', 'C1'), # Almost the same as `inverted_cdf`: ('averaged_inverted_cdf', '-.', 'C1'), ('closest_observation', ':', 'C2'), ('interpolated_inverted_cdf', '--', 'C1'), ('hazen', '--', 'C3'), ('weibull', '-.', 'C4'), ('median_unbiased', '--', 'C5'), ('normal_unbiased', '-.', 'C6'), ] for method, style, color in lines: ax.plot( p, np.percentile(a, p, method=method), label=method, linestyle=style, color=color) ax.set( title='Percentiles for different methods and data: ' + str(a), xlabel='Percentile', ylabel='Estimated percentile value', yticks=a) ax.legend(bbox_to_anchor=(1.03, 1)) plt.tight_layout() plt.show()  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
            "source": "@array_function_dispatch(_percentile_dispatcher)\ndef percentile(a,\n               q,\n               axis=None,\n               out=None,\n               overwrite_input=False,\n               method=\"linear\",\n               keepdims=False,\n               *,\n               interpolation=None):\n    \n    if interpolation is not None:\n        method = _check_interpolation_as_method(\n            method, interpolation, \"percentile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.true_divide(q, 100)\n    q = asanyarray(q)  # undo any decay that the ufunc performed (see gh-13105)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Percentiles must be in the range [0, 100]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
        },
        {
            "name": "numpy.nanpercentile",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.nanpercentile.html",
            "description": "Compute the qth percentile of the data along the specified axis, while ignoring nan values.  Returns the qth percentile(s) of the array elements.  .. versionadded:: 1.9.0  Parameters ---------- a : array_like Input array or object that can be converted to an array, containing nan values to be ignored. q : array_like of float Percentile or sequence of percentiles to compute, which must be between 0 and 100 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the percentile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  If this is anything but the default value it will be passed through (in the special case of an empty array) to the `mean` function of the underlying array.  If the array is a sub-class and `mean` does not have the kwarg `keepdims` this will raise a RuntimeError.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- percentile : scalar or ndarray If `q` is a single percentile and `axis=None`, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- nanmean nanmedian : equivalent to ``nanpercentile(..., 50)`` percentile, median, mean nanquantile : equivalent to nanpercentile, except q in range [0, 1].  Notes ----- For more information please see `numpy.percentile`  Examples -------- >>> a = np.array([[10., 7., 4.], [3., 2., 1.]]) >>> a[0][1] = np.nan >>> a array([[10.,  nan,   4.], [ 3.,   2.,   1.]]) >>> np.percentile(a, 50) nan >>> np.nanpercentile(a, 50) 3.0 >>> np.nanpercentile(a, 50, axis=0) array([6.5, 2. , 2.5]) >>> np.nanpercentile(a, 50, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.nanpercentile(a, 50, axis=0) >>> out = np.zeros_like(m) >>> np.nanpercentile(a, 50, axis=0, out=out) array([6.5, 2. , 2.5]) >>> m array([6.5,  2. ,  2.5])  >>> b = a.copy() >>> np.nanpercentile(b, 50, axis=1, overwrite_input=True) array([7., 2.]) >>> assert not np.all(a==b)  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
            "source": "@array_function_dispatch(_nanpercentile_dispatcher)\ndef nanpercentile(\n        a,\n        q,\n        axis=None,\n        out=None,\n        overwrite_input=False,\n        method=\"linear\",\n        keepdims=np._NoValue,\n        *,\n        interpolation=None,\n):\n    \n    if interpolation is not None:\n        method = function_base._check_interpolation_as_method(\n            method, interpolation, \"nanpercentile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.true_divide(q, 100.0)\n    # undo any decay that the ufunc performed (see gh-13105)\n    q = np.asanyarray(q)\n    if not function_base._quantile_is_valid(q):\n        raise ValueError(\"Percentiles must be in the range [0, 100]\")\n    return _nanquantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
        },
        {
            "name": "numpy.quantile",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.quantile.html",
            "description": "Compute the q-th quantile of the data along the specified axis.  .. versionadded:: 1.15.0  Parameters ---------- a : array_like of real numbers Input array or object that can be converted to an array. q : array_like of float Probability or sequence of probabilities for the quantiles to compute. Values must be between 0 and 1 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the quantile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- quantile : scalar or ndarray If `q` is a single probability and `axis=None`, then the result is a scalar. If multiple probabilies levels are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean percentile : equivalent to quantile, but with q in the range [0, 100]. median : equivalent to ``quantile(..., 0.5)`` nanquantile  Notes ----- Given a vector ``V`` of length ``n``, the q-th quantile of ``V`` is the value ``q`` of the way from the minimum to the maximum in a sorted copy of ``V``. The values and distances of the two nearest neighbors as well as the `method` parameter will determine the quantile if the normalized ranking does not match the location of ``q`` exactly. This function is the same as the median if ``q=0.5``, the same as the minimum if ``q=0.0`` and the same as the maximum if ``q=1.0``.  The optional `method` parameter specifies the method to use when the desired quantile lies between two indexes ``i`` and ``j = i + 1``. In that case, we first determine ``i + g``, a virtual index that lies between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the fractional part of the index. The final result is, then, an interpolation of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``, ``i`` and ``j`` are modified using correction constants ``alpha`` and ``beta`` whose choices depend on the ``method`` used. Finally, note that since Python uses 0-based indexing, the code subtracts another 1 from the index internally.  The following formula determines the virtual index ``i + g``, the location of the quantile in the sorted sample:  .. math:: i + g = q * ( n - alpha - beta + 1 ) + alpha  The different methods then work as follows  inverted_cdf: method 1 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then take i  averaged_inverted_cdf: method 2 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 ; then average between bounds  closest_observation: method 3 of H&F [1]_. This method gives discontinuous results:  * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i  interpolated_inverted_cdf: method 4 of H&F [1]_. This method gives continuous results using:  * alpha = 0 * beta = 1  hazen: method 5 of H&F [1]_. This method gives continuous results using:  * alpha = 1/2 * beta = 1/2  weibull: method 6 of H&F [1]_. This method gives continuous results using:  * alpha = 0 * beta = 0  linear: method 7 of H&F [1]_. This method gives continuous results using:  * alpha = 1 * beta = 1  median_unbiased: method 8 of H&F [1]_. This method is probably the best method if the sample distribution function is unknown (see reference). This method gives continuous results using:  * alpha = 1/3 * beta = 1/3  normal_unbiased: method 9 of H&F [1]_. This method is probably the best method if the sample distribution function is known to be normal. This method gives continuous results using:  * alpha = 3/8 * beta = 3/8  lower: NumPy method kept for backwards compatibility. Takes ``i`` as the interpolation point.  higher: NumPy method kept for backwards compatibility. Takes ``j`` as the interpolation point.  nearest: NumPy method kept for backwards compatibility. Takes ``i`` or ``j``, whichever is nearest.  midpoint: NumPy method kept for backwards compatibility. Uses ``(i + j) / 2``.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.quantile(a, 0.5) 3.5 >>> np.quantile(a, 0.5, axis=0) array([6.5, 4.5, 2.5]) >>> np.quantile(a, 0.5, axis=1) array([7.,  2.]) >>> np.quantile(a, 0.5, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.quantile(a, 0.5, axis=0) >>> out = np.zeros_like(m) >>> np.quantile(a, 0.5, axis=0, out=out) array([6.5, 4.5, 2.5]) >>> m array([6.5, 4.5, 2.5]) >>> b = a.copy() >>> np.quantile(b, 0.5, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a == b)  See also `numpy.percentile` for a visualization of most methods.  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
            "source": "@array_function_dispatch(_quantile_dispatcher)\ndef quantile(a,\n             q,\n             axis=None,\n             out=None,\n             overwrite_input=False,\n             method=\"linear\",\n             keepdims=False,\n             *,\n             interpolation=None):\n    \n    if interpolation is not None:\n        method = _check_interpolation_as_method(\n            method, interpolation, \"quantile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.asanyarray(q)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
        },
        {
            "name": "numpy.nanquantile",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.nanquantile.html",
            "description": "Compute the qth quantile of the data along the specified axis, while ignoring nan values. Returns the qth quantile(s) of the array elements.  .. versionadded:: 1.15.0  Parameters ---------- a : array_like Input array or object that can be converted to an array, containing nan values to be ignored q : array_like of float Probability or sequence of probabilities for the quantiles to compute. Values must be between 0 and 1 inclusive. axis : {int, tuple of int, None}, optional Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow the input array `a` to be modified by intermediate calculations, to save memory. In this case, the contents of the input `a` after this function completes is undefined. method : str, optional This parameter specifies the method to use for estimating the quantile.  There are many different methods, some unique to NumPy. See the notes for explanation.  The options sorted by their R type as summarized in the H&F paper [1]_ are:  1. 'inverted_cdf' 2. 'averaged_inverted_cdf' 3. 'closest_observation' 4. 'interpolated_inverted_cdf' 5. 'hazen' 6. 'weibull' 7. 'linear'  (default) 8. 'median_unbiased' 9. 'normal_unbiased'  The first three methods are discontinuous.  NumPy further defines the following discontinuous variations of the default 'linear' (7.) option:  * 'lower' * 'higher', * 'midpoint' * 'nearest'  .. versionchanged:: 1.22.0 This argument was previously called \"interpolation\" and only offered the \"linear\" default and last four options.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array `a`.  If this is anything but the default value it will be passed through (in the special case of an empty array) to the `mean` function of the underlying array.  If the array is a sub-class and `mean` does not have the kwarg `keepdims` this will raise a RuntimeError.  interpolation : str, optional Deprecated name for the method keyword argument.  .. deprecated:: 1.22.0  Returns ------- quantile : scalar or ndarray If `q` is a single probability and `axis=None`, then the result is a scalar. If multiple probability levels are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of `a`. If the input contains integers or floats smaller than ``float64``, the output data-type is ``float64``. Otherwise, the output data-type is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- quantile nanmean, nanmedian nanmedian : equivalent to ``nanquantile(..., 0.5)`` nanpercentile : same as nanquantile, but with q in the range [0, 100].  Notes ----- For more information please see `numpy.quantile`  Examples -------- >>> a = np.array([[10., 7., 4.], [3., 2., 1.]]) >>> a[0][1] = np.nan >>> a array([[10.,  nan,   4.], [ 3.,   2.,   1.]]) >>> np.quantile(a, 0.5) nan >>> np.nanquantile(a, 0.5) 3.0 >>> np.nanquantile(a, 0.5, axis=0) array([6.5, 2. , 2.5]) >>> np.nanquantile(a, 0.5, axis=1, keepdims=True) array([[7.], [2.]]) >>> m = np.nanquantile(a, 0.5, axis=0) >>> out = np.zeros_like(m) >>> np.nanquantile(a, 0.5, axis=0, out=out) array([6.5, 2. , 2.5]) >>> m array([6.5,  2. ,  2.5]) >>> b = a.copy() >>> np.nanquantile(b, 0.5, axis=1, overwrite_input=True) array([7., 2.]) >>> assert not np.all(a==b)  References ---------- .. [1] R. J. Hyndman and Y. Fan, \"Sample quantiles in statistical packages,\" The American Statistician, 50(4), pp. 361-365, 1996",
            "source": "@array_function_dispatch(_nanquantile_dispatcher)\ndef nanquantile(\n        a,\n        q,\n        axis=None,\n        out=None,\n        overwrite_input=False,\n        method=\"linear\",\n        keepdims=np._NoValue,\n        *,\n        interpolation=None,\n):\n    \n\n    if interpolation is not None:\n        method = function_base._check_interpolation_as_method(\n            method, interpolation, \"nanquantile\")\n\n    a = np.asanyarray(a)\n    if a.dtype.kind == \"c\":\n        raise TypeError(\"a must be an array of real numbers\")\n\n    q = np.asanyarray(q)\n    if not function_base._quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n    return _nanquantile_unchecked(\n        a, q, axis, out, overwrite_input, method, keepdims)"
        },
        {
            "name": "numpy.median",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.median.html",
            "description": "Compute the median along the specified axis.  Returns the median of the array elements.  Parameters ---------- a : array_like Input array or object that can be converted to an array. axis : {int, sequence of int, None}, optional Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow use of memory of input array `a` for calculations. The input array will be modified by the call to `median`. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If `overwrite_input` is ``True`` and `a` is not already an `ndarray`, an error will be raised. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`.  .. versionadded:: 1.9.0  Returns ------- median : ndarray A new array holding the result. If the input contains integers or floats smaller than ``float64``, then the output data-type is ``np.float64``.  Otherwise, the data-type of the output is the same as that of the input. If `out` is specified, that array is returned instead.  See Also -------- mean, percentile  Notes ----- Given a vector ``V`` of length ``N``, the median of ``V`` is the middle value of a sorted copy of ``V``, ``V_sorted`` - i e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the two middle values of ``V_sorted`` when ``N`` is even.  Examples -------- >>> a = np.array([[10, 7, 4], [3, 2, 1]]) >>> a array([[10,  7,  4], [ 3,  2,  1]]) >>> np.median(a) 3.5 >>> np.median(a, axis=0) array([6.5, 4.5, 2.5]) >>> np.median(a, axis=1) array([7.,  2.]) >>> m = np.median(a, axis=0) >>> out = np.zeros_like(m) >>> np.median(a, axis=0, out=m) array([6.5,  4.5,  2.5]) >>> m array([6.5,  4.5,  2.5]) >>> b = a.copy() >>> np.median(b, axis=1, overwrite_input=True) array([7.,  2.]) >>> assert not np.all(a==b) >>> b = a.copy() >>> np.median(b, axis=None, overwrite_input=True) 3.5 >>> assert not np.all(a==b)",
            "source": "@array_function_dispatch(_median_dispatcher)\ndef median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n    \n    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n                    overwrite_input=overwrite_input)"
        },
        {
            "name": "numpy.average",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.average.html",
            "description": "Compute the weighted average along the specified axis.  Parameters ---------- a : array_like Array containing data to be averaged. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which to average `a`.  The default, axis=None, will average over all of the elements of the input array. If axis is negative it counts from the last to the first axis.  .. versionadded:: 1.7.0  If axis is a tuple of ints, averaging is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before. weights : array_like, optional An array of weights associated with the values in `a`. Each value in `a` contributes to the average according to its associated weight. The weights array can either be 1-D (in which case its length must be the size of `a` along the given axis) or of the same shape as `a`. If `weights=None`, then all data in `a` are assumed to have a weight equal to one.  The 1-D calculation is::  avg = sum(a * weights) / sum(weights)  The only constraint on `weights` is that `sum(weights)` must not be 0. returned : bool, optional Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`) is returned, otherwise only the average is returned. If `weights=None`, `sum_of_weights` is equivalent to the number of elements over which the average is taken. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `a`. *Note:* `keepdims` will not work with instances of `numpy.matrix` or other classes whose methods do not support `keepdims`.  .. versionadded:: 1.23.0  Returns ------- retval, [sum_of_weights] : array_type or double Return the average along the specified axis. When `returned` is `True`, return a tuple with the average as the first element and the sum of the weights as the second element. `sum_of_weights` is of the same type as `retval`. The result dtype follows a genereal pattern. If `weights` is None, the result dtype will be that of `a` , or ``float64`` if `a` is integral. Otherwise, if `weights` is not None and `a` is non- integral, the result type will be the type of lowest precision capable of representing values of both `a` and `weights`. If `a` happens to be integral, the previous rules still applies but the result dtype will at least be ``float64``.  Raises ------ ZeroDivisionError When all weights along axis are zero. See `numpy.ma.average` for a version robust to this type of error. TypeError When the length of 1D `weights` is not the same as the shape of `a` along axis.  See Also -------- mean  ma.average : average for masked arrays -- useful if your data contains \"missing\" values numpy.result_type : Returns the type that results from applying the numpy type promotion rules to the arguments.  Examples -------- >>> data = np.arange(1, 5) >>> data array([1, 2, 3, 4]) >>> np.average(data) 2.5 >>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1)) 4.0  >>> data = np.arange(6).reshape((3, 2)) >>> data array([[0, 1], [2, 3], [4, 5]]) >>> np.average(data, axis=1, weights=[1./4, 3./4]) array([0.75, 2.75, 4.75]) >>> np.average(data, weights=[1./4, 3./4]) Traceback (most recent call last): ... TypeError: Axis must be specified when shapes of a and weights differ.  >>> a = np.ones(5, dtype=np.float128) >>> w = np.ones(5, dtype=np.complex64) >>> avg = np.average(a, weights=w) >>> print(avg.dtype) complex256  With ``keepdims=True``, the following result has shape (3, 1).  >>> np.average(data, axis=1, keepdims=True) array([[0.5], [2.5], [4.5]])",
            "source": "@array_function_dispatch(_average_dispatcher)\ndef average(a, axis=None, weights=None, returned=False, *,\n            keepdims=np._NoValue):\n    \n    a = np.asanyarray(a)\n\n    if keepdims is np._NoValue:\n        # Don't pass on the keepdims argument if one wasn't given.\n        keepdims_kw = {}\n    else:\n        keepdims_kw = {'keepdims': keepdims}\n\n    if weights is None:\n        avg = a.mean(axis, **keepdims_kw)\n        avg_as_array = np.asanyarray(avg)\n        scl = avg_as_array.dtype.type(a.size/avg_as_array.size)\n    else:\n        wgt = np.asanyarray(weights)\n\n        if issubclass(a.dtype.type, (np.integer, np.bool_)):\n            result_dtype = np.result_type(a.dtype, wgt.dtype, 'f8')\n        else:\n            result_dtype = np.result_type(a.dtype, wgt.dtype)\n\n        # Sanity checks\n        if a.shape != wgt.shape:\n            if axis is None:\n                raise TypeError(\n                    \"Axis must be specified when shapes of a and weights \"\n                    \"differ.\")\n            if wgt.ndim != 1:\n                raise TypeError(\n                    \"1D weights expected when shapes of a and weights differ.\")\n            if wgt.shape[0] != a.shape[axis]:\n                raise ValueError(\n                    \"Length of weights not compatible with specified axis.\")\n\n            # setup wgt to broadcast along axis\n            wgt = np.broadcast_to(wgt, (a.ndim-1)*(1,) + wgt.shape)\n            wgt = wgt.swapaxes(-1, axis)\n\n        scl = wgt.sum(axis=axis, dtype=result_dtype, **keepdims_kw)\n        if np.any(scl == 0.0):\n            raise ZeroDivisionError(\n                \"Weights sum to zero, can't be normalized\")\n\n        avg = avg_as_array = np.multiply(a, wgt,\n                          dtype=result_dtype).sum(axis, **keepdims_kw) / scl\n\n    if returned:\n        if scl.shape != avg_as_array.shape:\n            scl = np.broadcast_to(scl, avg_as_array.shape).copy()\n        return avg, scl\n    else:\n        return avg"
        },
        {
            "name": "numpy.mean",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.mean.html",
            "description": "Compute the arithmetic mean along the specified axis.  Returns the average of the array elements.  The average is taken over the flattened array by default, otherwise over the specified axis. `float64` intermediate and return values are used for integer inputs.  Parameters ---------- a : array_like Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the mean.  For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype. out : ndarray, optional Alternate output array in which to place the result.  The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See :ref:`ufuncs-output-type` for more details.  keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `mean` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the mean. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- m : ndarray, see dtype parameter above If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned.  See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar  Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements.  Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue.  By default, `float16` results are computed using `float32` intermediates for extra precision.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.mean(a) 2.5 >>> np.mean(a, axis=0) array([2., 3.]) >>> np.mean(a, axis=1) array([1.5, 3.5])  In single precision, `mean` can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.mean(a) 0.54999924  Computing the mean in float64 is more accurate:  >>> np.mean(a, dtype=np.float64) 0.55000000074505806 # may vary  Specifying a where argument:  >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]]) >>> np.mean(a) 12.0 >>> np.mean(a, where=[[True], [False], [False]]) 9.0",
            "source": "@array_function_dispatch(_mean_dispatcher)\ndef mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n         where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            mean = a.mean\n        except AttributeError:\n            pass\n        else:\n            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\n    return _methods._mean(a, axis=axis, dtype=dtype,\n                          out=out, **kwargs)"
        },
        {
            "name": "numpy.std",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.std.html",
            "description": "Compute the standard deviation along the specified axis.  Returns the standard deviation, a measure of the spread of a distribution, of the array elements. The standard deviation is computed for the flattened array by default, otherwise over the specified axis.  Parameters ---------- a : array_like Calculate the standard deviation of these values. axis : None or int or tuple of ints, optional Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single axis or all the axes as before. dtype : dtype, optional Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary. ddof : int, optional Means Delta Degrees of Freedom.  The divisor used in calculations is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `std` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the standard deviation. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- standard_deviation : ndarray, see dtype parameter above. If `out` is None, return a new array containing the standard deviation, otherwise return a reference to the output array.  See Also -------- var, mean, nanmean, nanstd, nanvar :ref:`ufuncs-output-type`  Notes ----- The standard deviation is the square root of the average of the squared deviations from the mean, i.e., ``std = sqrt(mean(x))``, where ``x = abs(a - a.mean())**2``.  The average squared deviation is typically calculated as ``x.sum() / N``, where ``N = len(x)``. If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of the infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ``ddof=1``, it will not be an unbiased estimate of the standard deviation per se.  Note that, for complex numbers, `std` takes the absolute value before squaring, so that the result is always real and nonnegative.  For floating-point input, the *std* is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the `dtype` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.std(a) 1.1180339887498949 # may vary >>> np.std(a, axis=0) array([1.,  1.]) >>> np.std(a, axis=1) array([0.5,  0.5])  In single precision, std() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.std(a) 0.45000005  Computing the standard deviation in float64 is more accurate:  >>> np.std(a, dtype=np.float64) 0.44999999925494177 # may vary  Specifying a where argument:  >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]]) >>> np.std(a) 2.614064523559687 # may vary >>> np.std(a, where=[[True], [True], [False]]) 2.0",
            "source": "@array_function_dispatch(_std_dispatcher)\ndef std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            std = a.std\n        except AttributeError:\n            pass\n        else:\n            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)"
        },
        {
            "name": "numpy.var",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.var.html",
            "description": "Compute the variance along the specified axis.  Returns the variance of the array elements, a measure of the spread of a distribution.  The variance is computed for the flattened array by default, otherwise over the specified axis.  Parameters ---------- a : array_like Array containing numbers whose variance is desired.  If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the variance is computed.  The default is to compute the variance of the flattened array.  .. versionadded:: 1.7.0  If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the variance.  For arrays of integer type the default is `float64`; for arrays of float types it is the same as the array type. out : ndarray, optional Alternate output array in which to place the result.  It must have the same shape as the expected output, but the type is cast if necessary. ddof : int, optional \"Delta Degrees of Freedom\": the divisor used in the calculation is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then `keepdims` will not be passed through to the `var` method of sub-classes of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any exceptions will be raised.  where : array_like of bool, optional Elements to include in the variance. See `~numpy.ufunc.reduce` for details.  .. versionadded:: 1.20.0  Returns ------- variance : ndarray, see dtype parameter above If ``out=None``, returns a new array containing the variance; otherwise, a reference to the output array is returned.  See Also -------- std, mean, nanmean, nanstd, nanvar :ref:`ufuncs-output-type`  Notes ----- The variance is the average of the squared deviations from the mean, i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.  The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``. If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead.  In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of a hypothetical infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables.  Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.  For floating-point input, the variance is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-accuracy accumulator using the ``dtype`` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.var(a) 1.25 >>> np.var(a, axis=0) array([1.,  1.]) >>> np.var(a, axis=1) array([0.25,  0.25])  In single precision, var() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.var(a) 0.20250003  Computing the variance in float64 is more accurate:  >>> np.var(a, dtype=np.float64) 0.20249999932944759 # may vary >>> ((1-0.55)**2 + (0.1-0.55)**2)/2 0.2025  Specifying a where argument:  >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]]) >>> np.var(a) 6.833333333333333 # may vary >>> np.var(a, where=[[True], [True], [False]]) 4.0",
            "source": "@array_function_dispatch(_var_dispatcher)\ndef var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n\n    if type(a) is not mu.ndarray:\n        try:\n            var = a.var\n\n        except AttributeError:\n            pass\n        else:\n            return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)"
        },
        {
            "name": "numpy.corrcoef",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html",
            "description": "Return Pearson product-moment correlation coefficients.  Please refer to the documentation for `cov` for more detail.  The relationship between the correlation coefficient matrix, `R`, and the covariance matrix, `C`, is  .. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }  The values of `R` are between -1 and 1, inclusive.  Parameters ---------- x : array_like A 1-D or 2-D array containing multiple variables and observations. Each row of `x` represents a variable, and each column a single observation of all those variables. Also see `rowvar` below. y : array_like, optional An additional set of variables and observations. `y` has the same shape as `x`. rowvar : bool, optional If `rowvar` is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations. bias : _NoValue, optional Has no effect, do not use.  .. deprecated:: 1.10.0 ddof : _NoValue, optional Has no effect, do not use.  .. deprecated:: 1.10.0 dtype : data-type, optional Data-type of the result. By default, the return data-type will have at least `numpy.float64` precision.  .. versionadded:: 1.20  Returns ------- R : ndarray The correlation coefficient matrix of the variables.  See Also -------- cov : Covariance matrix  Notes ----- Due to floating point rounding the resulting array may not be Hermitian, the diagonal elements may not be 1, and the elements may not satisfy the inequality abs(a) <= 1. The real and imaginary parts are clipped to the interval [-1,  1] in an attempt to improve on that situation but is not much help in the complex case.  This function accepts but discards arguments `bias` and `ddof`.  This is for backwards compatibility with previous versions of this function.  These arguments had no effect on the return values of the function and can be safely ignored in this and previous versions of numpy.  Examples -------- In this example we generate two random arrays, ``xarr`` and ``yarr``, and compute the row-wise and column-wise Pearson correlation coefficients, ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise Pearson correlation coefficients between the variables of ``xarr``.  >>> import numpy as np >>> rng = np.random.default_rng(seed=42) >>> xarr = rng.random((3, 3)) >>> xarr array([[0.77395605, 0.43887844, 0.85859792], [0.69736803, 0.09417735, 0.97562235], [0.7611397 , 0.78606431, 0.12811363]]) >>> R1 = np.corrcoef(xarr) >>> R1 array([[ 1.        ,  0.99256089, -0.68080986], [ 0.99256089,  1.        , -0.76492172], [-0.68080986, -0.76492172,  1.        ]])  If we add another set of variables and observations ``yarr``, we can compute the row-wise Pearson correlation coefficients between the variables in ``xarr`` and ``yarr``.  >>> yarr = rng.random((3, 3)) >>> yarr array([[0.45038594, 0.37079802, 0.92676499], [0.64386512, 0.82276161, 0.4434142 ], [0.22723872, 0.55458479, 0.06381726]]) >>> R2 = np.corrcoef(xarr, yarr) >>> R2 array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  , -0.99004057], [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098, -0.99981569], [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355, 0.77714685], [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855, -0.83571711], [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        , 0.97517215], [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215, 1.        ]])  Finally if we use the option ``rowvar=False``, the columns are now being treated as the variables and we will find the column-wise Pearson correlation coefficients between variables in ``xarr`` and ``yarr``.  >>> R3 = np.corrcoef(xarr, yarr, rowvar=False) >>> R3 array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 , 0.22423734], [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587, -0.44069024], [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648, 0.75137473], [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469, 0.47536961], [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        , -0.46666491], [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491, 1.        ]])",
            "source": "@array_function_dispatch(_corrcoef_dispatcher)\ndef corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,\n             dtype=None):\n    \n    if bias is not np._NoValue or ddof is not np._NoValue:\n        # 2015-03-15, 1.10\n        warnings.warn('bias and ddof have no effect and are deprecated',\n                      DeprecationWarning, stacklevel=2)\n    c = cov(x, y, rowvar, dtype=dtype)\n    try:\n        d = diag(c)\n    except ValueError:\n        # scalar covariance\n        # nan if incorrect value (nan, inf, 0), 1 otherwise\n        return c / c\n    stddev = sqrt(d.real)\n    c /= stddev[:, None]\n    c /= stddev[None, :]\n\n    # Clip real and imaginary parts to [-1, 1].  This does not guarantee\n    # abs(a[i,j]) <= 1 for complex arrays, but is the best we can do without\n    # excessive work.\n    np.clip(c.real, -1, 1, out=c.real)\n    if np.iscomplexobj(c):\n        np.clip(c.imag, -1, 1, out=c.imag)\n\n    return c"
        },
        {
            "name": "numpy.correlate",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.correlate.html",
            "description": "Cross-correlation of two 1-dimensional sequences.  This function computes the correlation as generally defined in signal processing texts:  .. math:: c_k = \\sum_n a_{n+k} \\cdot \\overline{v}_n  with a and v sequences being zero-padded where necessary and :math:`\\overline x` denoting complex conjugation.  Parameters ---------- a, v : array_like Input sequences. mode : {'valid', 'same', 'full'}, optional Refer to the `convolve` docstring.  Note that the default is 'valid', unlike `convolve`, which uses 'full'. old_behavior : bool `old_behavior` was removed in NumPy 1.10. If you need the old behavior, use `multiarray.correlate`.  Returns ------- out : ndarray Discrete cross-correlation of `a` and `v`.  See Also -------- convolve : Discrete, linear convolution of two one-dimensional sequences. multiarray.correlate : Old, no conjugate, version of correlate. scipy.signal.correlate : uses FFT which has superior performance on large arrays.  Notes ----- The definition of correlation above is not unique and sometimes correlation may be defined differently. Another common definition is:  .. math:: c'_k = \\sum_n a_{n} \\cdot \\overline{v_{n+k}}  which is related to :math:`c_k` by :math:`c'_k = c_{-k}`.  `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might be preferable.   Examples -------- >>> np.correlate([1, 2, 3], [0, 1, 0.5]) array([3.5]) >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\") array([2. ,  3.5,  3. ]) >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\") array([0.5,  2. ,  3.5,  3. ,  0. ])  Using complex sequences:  >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full') array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])  Note that you get the time reversed, complex conjugated result (:math:`\\overline{c_{-k}}`) when the two input sequences a and v change places:  >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full') array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])",
            "source": "@array_function_dispatch(_correlate_dispatcher)\ndef correlate(a, v, mode='valid'):\n    \n    return multiarray.correlate2(a, v, mode)"
        },
        {
            "name": "numpy.cov",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.cov.html",
            "description": "Estimate a covariance matrix, given data and weights.  Covariance indicates the level to which two variables vary together. If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`, then the covariance matrix element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance of :math:`x_i`.  See the notes for an outline of the algorithm.  Parameters ---------- m : array_like A 1-D or 2-D array containing multiple variables and observations. Each row of `m` represents a variable, and each column a single observation of all those variables. Also see `rowvar` below. y : array_like, optional An additional set of variables and observations. `y` has the same form as that of `m`. rowvar : bool, optional If `rowvar` is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations. bias : bool, optional Default normalization (False) is by ``(N - 1)``, where ``N`` is the number of observations given (unbiased estimate). If `bias` is True, then normalization is by ``N``. These values can be overridden by using the keyword ``ddof`` in numpy versions >= 1.5. ddof : int, optional If not ``None`` the default value implied by `bias` is overridden. Note that ``ddof=1`` will return the unbiased estimate, even if both `fweights` and `aweights` are specified, and ``ddof=0`` will return the simple average. See the notes for the details. The default value is ``None``.  .. versionadded:: 1.5 fweights : array_like, int, optional 1-D array of integer frequency weights; the number of times each observation vector should be repeated.  .. versionadded:: 1.10 aweights : array_like, optional 1-D array of observation vector weights. These relative weights are typically large for observations considered \"important\" and smaller for observations considered less \"important\". If ``ddof=0`` the array of weights can be used to assign probabilities to observation vectors.  .. versionadded:: 1.10 dtype : data-type, optional Data-type of the result. By default, the return data-type will have at least `numpy.float64` precision.  .. versionadded:: 1.20  Returns ------- out : ndarray The covariance matrix of the variables.  See Also -------- corrcoef : Normalized covariance matrix  Notes ----- Assume that the observations are in the columns of the observation array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The steps to compute the weighted covariance are as follows::  >>> m = np.arange(10, dtype=np.float64) >>> f = np.arange(10) * 2 >>> a = np.arange(10) ** 2. >>> ddof = 1 >>> w = f * a >>> v1 = np.sum(w) >>> v2 = np.sum(w * a) >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1 >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)  Note that when ``a == 1``, the normalization factor ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)`` as it should.  Examples -------- Consider two variables, :math:`x_0` and :math:`x_1`, which correlate perfectly, but in opposite directions:  >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T >>> x array([[0, 1, 2], [2, 1, 0]])  Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance matrix shows this clearly:  >>> np.cov(x) array([[ 1., -1.], [-1.,  1.]])  Note that element :math:`C_{0,1}`, which shows the correlation between :math:`x_0` and :math:`x_1`, is negative.  Further, note how `x` and `y` are combined:  >>> x = [-2.1, -1,  4.3] >>> y = [3,  1.1,  0.12] >>> X = np.stack((x, y), axis=0) >>> np.cov(X) array([[11.71      , -4.286     ], # may vary [-4.286     ,  2.144133]]) >>> np.cov(x, y) array([[11.71      , -4.286     ], # may vary [-4.286     ,  2.144133]]) >>> np.cov(x) array(11.71)",
            "source": "@array_function_dispatch(_cov_dispatcher)\ndef cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,\n        aweights=None, *, dtype=None):\n    \n    # Check inputs\n    if ddof is not None and ddof != int(ddof):\n        raise ValueError(\n            \"ddof must be integer\")\n\n    # Handles complex arrays too\n    m = np.asarray(m)\n    if m.ndim > 2:\n        raise ValueError(\"m has more than 2 dimensions\")\n\n    if y is not None:\n        y = np.asarray(y)\n        if y.ndim > 2:\n            raise ValueError(\"y has more than 2 dimensions\")\n\n    if dtype is None:\n        if y is None:\n            dtype = np.result_type(m, np.float64)\n        else:\n            dtype = np.result_type(m, y, np.float64)\n\n    X = array(m, ndmin=2, dtype=dtype)\n    if not rowvar and X.shape[0] != 1:\n        X = X.T\n    if X.shape[0] == 0:\n        return np.array([]).reshape(0, 0)\n    if y is not None:\n        y = array(y, copy=False, ndmin=2, dtype=dtype)\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        X = np.concatenate((X, y), axis=0)\n\n    if ddof is None:\n        if bias == 0:\n            ddof = 1\n        else:\n            ddof = 0\n\n    # Get the product of frequencies and weights\n    w = None\n    if fweights is not None:\n        fweights = np.asarray(fweights, dtype=float)\n        if not np.all(fweights == np.around(fweights)):\n            raise TypeError(\n                \"fweights must be integer\")\n        if fweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional fweights\")\n        if fweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and fweights\")\n        if any(fweights < 0):\n            raise ValueError(\n                \"fweights cannot be negative\")\n        w = fweights\n    if aweights is not None:\n        aweights = np.asarray(aweights, dtype=float)\n        if aweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional aweights\")\n        if aweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and aweights\")\n        if any(aweights < 0):\n            raise ValueError(\n                \"aweights cannot be negative\")\n        if w is None:\n            w = aweights\n        else:\n            w *= aweights\n\n    avg, w_sum = average(X, axis=1, weights=w, returned=True)\n    w_sum = w_sum[0]\n\n    # Determine the normalization\n    if w is None:\n        fact = X.shape[1] - ddof\n    elif ddof == 0:\n        fact = w_sum\n    elif aweights is None:\n        fact = w_sum - ddof\n    else:\n        fact = w_sum - ddof*sum(w*aweights)/w_sum\n\n    if fact <= 0:\n        warnings.warn(\"Degrees of freedom <= 0 for slice\",\n                      RuntimeWarning, stacklevel=2)\n        fact = 0.0\n\n    X -= avg[:, None]\n    if w is None:\n        X_T = X.T\n    else:\n        X_T = (X*w).T\n    c = dot(X, X_T.conj())\n    c *= np.true_divide(1, fact)\n    return c.squeeze()"
        },
        {
            "name": "numpy.histogram",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram.html",
            "description": "Compute the histogram of a dataset.  Parameters ---------- a : array_like Input data. The histogram is computed over the flattened array. bins : int or sequence of scalars or str, optional If `bins` is an int, it defines the number of equal-width bins in the given range (10, by default). If `bins` is a sequence, it defines a monotonically increasing array of bin edges, including the rightmost edge, allowing for non-uniform bin widths.  .. versionadded:: 1.11.0  If `bins` is a string, it defines the method used to calculate the optimal bin width, as defined by `histogram_bin_edges`.  range : (float, float), optional The lower and upper range of the bins.  If not provided, range is simply ``(a.min(), a.max())``.  Values outside the range are ignored. The first element of the range must be less than or equal to the second. `range` affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within `range`, the bin count will fill the entire range including portions containing no data. weights : array_like, optional An array of weights, of the same shape as `a`.  Each value in `a` only contributes its associated weight towards the bin count (instead of 1). If `density` is True, the weights are normalized, so that the integral of the density over the range remains 1. density : bool, optional If ``False``, the result will contain the number of samples in each bin. If ``True``, the result is the value of the probability *density* function at the bin, normalized such that the *integral* over the range is 1. Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen; it is not a probability *mass* function.  Returns ------- hist : array The values of the histogram. See `density` and `weights` for a description of the possible semantics. bin_edges : array of dtype float Return the bin edges ``(length(hist)+1)``.   See Also -------- histogramdd, bincount, searchsorted, digitize, histogram_bin_edges  Notes ----- All but the last (righthand-most) bin is half-open.  In other words, if `bins` is::  [1, 2, 3, 4]  then the first bin is ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which *includes* 4.   Examples -------- >>> np.histogram([1, 2, 1], bins=[0, 1, 2, 3]) (array([0, 2, 1]), array([0, 1, 2, 3])) >>> np.histogram(np.arange(4), bins=np.arange(5), density=True) (array([0.25, 0.25, 0.25, 0.25]), array([0, 1, 2, 3, 4])) >>> np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3]) (array([1, 4, 1]), array([0, 1, 2, 3]))  >>> a = np.arange(5) >>> hist, bin_edges = np.histogram(a, density=True) >>> hist array([0.5, 0. , 0.5, 0. , 0. , 0.5, 0. , 0.5, 0. , 0.5]) >>> hist.sum() 2.4999999999999996 >>> np.sum(hist * np.diff(bin_edges)) 1.0  .. versionadded:: 1.11.0  Automated Bin Selection Methods example, using 2 peak random data with 2000 points:  >>> import matplotlib.pyplot as plt >>> rng = np.random.RandomState(10)  # deterministic random data >>> a = np.hstack((rng.normal(size=1000), ...                rng.normal(loc=5, scale=2, size=1000))) >>> _ = plt.hist(a, bins='auto')  # arguments are passed to np.histogram >>> plt.title(\"Histogram with 'auto' bins\") Text(0.5, 1.0, \"Histogram with 'auto' bins\") >>> plt.show()",
            "source": "@array_function_dispatch(_histogram_dispatcher)\ndef histogram(a, bins=10, range=None, density=None, weights=None):\n\n    a, weights = _ravel_and_check_weights(a, weights)\n\n    bin_edges, uniform_bins = _get_bin_edges(a, bins, range, weights)\n\n    # Histogram is an integer or a float array depending on the weights.\n    if weights is None:\n        ntype = np.dtype(np.intp)\n    else:\n        ntype = weights.dtype\n\n    # We set a block size, as this allows us to iterate over chunks when\n    # computing histograms, to minimize memory usage.\n    BLOCK = 65536\n\n    # The fast path uses bincount, but that only works for certain types\n    # of weight\n    simple_weights = (\n        weights is None or\n        np.can_cast(weights.dtype, np.double) or\n        np.can_cast(weights.dtype, complex)\n    )\n\n    if uniform_bins is not None and simple_weights:\n        # Fast algorithm for equal bins\n        # We now convert values of a to bin indices, under the assumption of\n        # equal bin widths (which is valid here).\n        first_edge, last_edge, n_equal_bins = uniform_bins\n\n        # Initialize empty histogram\n        n = np.zeros(n_equal_bins, ntype)\n\n        # Pre-compute histogram scaling factor\n        norm_numerator = n_equal_bins\n        norm_denom = _unsigned_subtract(last_edge, first_edge)\n\n        # We iterate over blocks here for two reasons: the first is that for\n        # large arrays, it is actually faster (for example for a 10^8 array it\n        # is 2x as fast) and it results in a memory footprint 3x lower in the\n        # limit of large arrays.\n        for i in _range(0, len(a), BLOCK):\n            tmp_a = a[i:i+BLOCK]\n            if weights is None:\n                tmp_w = None\n            else:\n                tmp_w = weights[i:i + BLOCK]\n\n            # Only include values in the right range\n            keep = (tmp_a >= first_edge)\n            keep &= (tmp_a <= last_edge)\n            if not np.logical_and.reduce(keep):\n                tmp_a = tmp_a[keep]\n                if tmp_w is not None:\n                    tmp_w = tmp_w[keep]\n\n            # This cast ensures no type promotions occur below, which gh-10322\n            # make unpredictable. Getting it wrong leads to precision errors\n            # like gh-8123.\n            tmp_a = tmp_a.astype(bin_edges.dtype, copy=False)\n\n            # Compute the bin indices, and for values that lie exactly on\n            # last_edge we need to subtract one\n            f_indices = ((_unsigned_subtract(tmp_a, first_edge) / norm_denom)\n                         * norm_numerator)\n            indices = f_indices.astype(np.intp)\n            indices[indices == n_equal_bins] -= 1\n\n            # The index computation is not guaranteed to give exactly\n            # consistent results within ~1 ULP of the bin edges.\n            decrement = tmp_a < bin_edges[indices]\n            indices[decrement] -= 1\n            # The last bin includes the right edge. The other bins do not.\n            increment = ((tmp_a >= bin_edges[indices + 1])\n                         & (indices != n_equal_bins - 1))\n            indices[increment] += 1\n\n            # We now compute the histogram using bincount\n            if ntype.kind == 'c':\n                n.real += np.bincount(indices, weights=tmp_w.real,\n                                      minlength=n_equal_bins)\n                n.imag += np.bincount(indices, weights=tmp_w.imag,\n                                      minlength=n_equal_bins)\n            else:\n                n += np.bincount(indices, weights=tmp_w,\n                                 minlength=n_equal_bins).astype(ntype)\n    else:\n        # Compute via cumulative histogram\n        cum_n = np.zeros(bin_edges.shape, ntype)\n        if weights is None:\n            for i in _range(0, len(a), BLOCK):\n                sa = np.sort(a[i:i+BLOCK])\n                cum_n += _search_sorted_inclusive(sa, bin_edges)\n        else:\n            zero = np.zeros(1, dtype=ntype)\n            for i in _range(0, len(a), BLOCK):\n                tmp_a = a[i:i+BLOCK]\n                tmp_w = weights[i:i+BLOCK]\n                sorting_index = np.argsort(tmp_a)\n                sa = tmp_a[sorting_index]\n                sw = tmp_w[sorting_index]\n                cw = np.concatenate((zero, sw.cumsum()))\n                bin_index = _search_sorted_inclusive(sa, bin_edges)\n                cum_n += cw[bin_index]\n\n        n = np.diff(cum_n)\n\n    if density:\n        db = np.array(np.diff(bin_edges), float)\n        return n/db/n.sum(), bin_edges\n\n    return n, bin_edges"
        },
        {
            "name": "numpy.histogram2d",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html",
            "description": "Compute the bi-dimensional histogram of two data samples.  Parameters ---------- x : array_like, shape (N,) An array containing the x coordinates of the points to be histogrammed. y : array_like, shape (N,) An array containing the y coordinates of the points to be histogrammed. bins : int or array_like or [int, int] or [array, array], optional The bin specification:  * If int, the number of bins for the two dimensions (nx=ny=bins). * If array_like, the bin edges for the two dimensions (x_edges=y_edges=bins). * If [int, int], the number of bins in each dimension (nx, ny = bins). * If [array, array], the bin edges in each dimension (x_edges, y_edges = bins). * A combination [int, array] or [array, int], where int is the number of bins and array is the bin edges.  range : array_like, shape(2,2), optional The leftmost and rightmost edges of the bins along each dimension (if not specified explicitly in the `bins` parameters): ``[[xmin, xmax], [ymin, ymax]]``. All values outside of this range will be considered outliers and not tallied in the histogram. density : bool, optional If False, the default, returns the number of samples in each bin. If True, returns the probability *density* function at the bin, ``bin_count / sample_count / bin_area``. weights : array_like, shape(N,), optional An array of values ``w_i`` weighing each sample ``(x_i, y_i)``. Weights are normalized to 1 if `density` is True. If `density` is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.  Returns ------- H : ndarray, shape(nx, ny) The bi-dimensional histogram of samples `x` and `y`. Values in `x` are histogrammed along the first dimension and values in `y` are histogrammed along the second dimension. xedges : ndarray, shape(nx+1,) The bin edges along the first dimension. yedges : ndarray, shape(ny+1,) The bin edges along the second dimension.  See Also -------- histogram : 1D histogram histogramdd : Multidimensional histogram  Notes ----- When `density` is True, then the returned histogram is the sample density, defined such that the sum over bins of the product ``bin_value * bin_area`` is 1.  Please note that the histogram does not follow the Cartesian convention where `x` values are on the abscissa and `y` values on the ordinate axis.  Rather, `x` is histogrammed along the first dimension of the array (vertical), and `y` along the second dimension of the array (horizontal).  This ensures compatibility with `histogramdd`.  Examples -------- >>> from matplotlib.image import NonUniformImage >>> import matplotlib.pyplot as plt  Construct a 2-D histogram with variable bin width. First define the bin edges:  >>> xedges = [0, 1, 3, 5] >>> yedges = [0, 2, 3, 4, 6]  Next we create a histogram H with random bin content:  >>> x = np.random.normal(2, 1, 100) >>> y = np.random.normal(1, 1, 100) >>> H, xedges, yedges = np.histogram2d(x, y, bins=(xedges, yedges)) >>> # Histogram does not follow Cartesian convention (see Notes), >>> # therefore transpose H for visualization purposes. >>> H = H.T  :func:`imshow <matplotlib.pyplot.imshow>` can only display square bins:  >>> fig = plt.figure(figsize=(7, 3)) >>> ax = fig.add_subplot(131, title='imshow: square bins') >>> plt.imshow(H, interpolation='nearest', origin='lower', ...         extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]]) <matplotlib.image.AxesImage object at 0x...>  :func:`pcolormesh <matplotlib.pyplot.pcolormesh>` can display actual edges:  >>> ax = fig.add_subplot(132, title='pcolormesh: actual edges', ...         aspect='equal') >>> X, Y = np.meshgrid(xedges, yedges) >>> ax.pcolormesh(X, Y, H) <matplotlib.collections.QuadMesh object at 0x...>  :class:`NonUniformImage <matplotlib.image.NonUniformImage>` can be used to display actual bin edges with interpolation:  >>> ax = fig.add_subplot(133, title='NonUniformImage: interpolated', ...         aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]]) >>> im = NonUniformImage(ax, interpolation='bilinear') >>> xcenters = (xedges[:-1] + xedges[1:]) / 2 >>> ycenters = (yedges[:-1] + yedges[1:]) / 2 >>> im.set_data(xcenters, ycenters, H) >>> ax.add_image(im) >>> plt.show()  It is also possible to construct a 2-D histogram without specifying bin edges:  >>> # Generate non-symmetric test data >>> n = 10000 >>> x = np.linspace(1, 100, n) >>> y = 2*np.log(x) + np.random.rand(n) - 0.5 >>> # Compute 2d histogram. Note the order of x/y and xedges/yedges >>> H, yedges, xedges = np.histogram2d(y, x, bins=20)  Now we can plot the histogram using :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`, and a :func:`hexbin <matplotlib.pyplot.hexbin>` for comparison.  >>> # Plot histogram using pcolormesh >>> fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True) >>> ax1.pcolormesh(xedges, yedges, H, cmap='rainbow') >>> ax1.plot(x, 2*np.log(x), 'k-') >>> ax1.set_xlim(x.min(), x.max()) >>> ax1.set_ylim(y.min(), y.max()) >>> ax1.set_xlabel('x') >>> ax1.set_ylabel('y') >>> ax1.set_title('histogram2d') >>> ax1.grid()  >>> # Create hexbin plot for comparison >>> ax2.hexbin(x, y, gridsize=20, cmap='rainbow') >>> ax2.plot(x, 2*np.log(x), 'k-') >>> ax2.set_title('hexbin') >>> ax2.set_xlim(x.min(), x.max()) >>> ax2.set_xlabel('x') >>> ax2.grid()  >>> plt.show()",
            "source": "@array_function_dispatch(_histogram2d_dispatcher)\ndef histogram2d(x, y, bins=10, range=None, density=None, weights=None):\n    \n    from numpy import histogramdd\n\n    if len(x) != len(y):\n        raise ValueError('x and y must have the same length.')\n\n    try:\n        N = len(bins)\n    except TypeError:\n        N = 1\n\n    if N != 1 and N != 2:\n        xedges = yedges = asarray(bins)\n        bins = [xedges, yedges]\n    hist, edges = histogramdd([x, y], bins, range, density, weights)\n    return hist, edges[0], edges[1]"
        },
        {
            "name": "numpy.histogramdd",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html",
            "description": "Compute the multidimensional histogram of some data.  Parameters ---------- sample : (N, D) array, or (N, D) array_like The data to be histogrammed.  Note the unusual interpretation of sample when an array_like:  * When an array, each row is a coordinate in a D-dimensional space - such as ``histogramdd(np.array([p1, p2, p3]))``. * When an array_like, each element is the list of values for single coordinate - such as ``histogramdd((X, Y, Z))``.  The first form should be preferred.  bins : sequence or int, optional The bin specification:  * A sequence of arrays describing the monotonically increasing bin edges along each dimension. * The number of bins for each dimension (nx, ny, ... =bins) * The number of bins for all dimensions (nx=ny=...=bins).  range : sequence, optional A sequence of length D, each an optional (lower, upper) tuple giving the outer bin edges to be used if the edges are not given explicitly in `bins`. An entry of None in the sequence results in the minimum and maximum values being used for the corresponding dimension. The default, None, is equivalent to passing a tuple of D None values. density : bool, optional If False, the default, returns the number of samples in each bin. If True, returns the probability *density* function at the bin, ``bin_count / sample_count / bin_volume``. weights : (N,) array_like, optional An array of values `w_i` weighing each sample `(x_i, y_i, z_i, ...)`. Weights are normalized to 1 if density is True. If density is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.  Returns ------- H : ndarray The multidimensional histogram of sample x. See density and weights for the different possible semantics. edges : list A list of D arrays describing the bin edges for each dimension.  See Also -------- histogram: 1-D histogram histogram2d: 2-D histogram  Examples -------- >>> r = np.random.randn(100,3) >>> H, edges = np.histogramdd(r, bins = (5, 8, 4)) >>> H.shape, edges[0].size, edges[1].size, edges[2].size ((5, 8, 4), 6, 9, 5)",
            "source": "@array_function_dispatch(_histogramdd_dispatcher)\ndef histogramdd(sample, bins=10, range=None, density=None, weights=None):\n    \n    try:\n        # Sample is an ND-array.\n        N, D = sample.shape\n    except (AttributeError, ValueError):\n        # Sample is a sequence of 1D arrays.\n        sample = np.atleast_2d(sample).T\n        N, D = sample.shape\n\n    nbin = np.empty(D, np.intp)\n    edges = D*[None]\n    dedges = D*[None]\n    if weights is not None:\n        weights = np.asarray(weights)\n\n    try:\n        M = len(bins)\n        if M != D:\n            raise ValueError(\n                'The dimension of bins must be equal to the dimension of the '\n                'sample x.')\n    except TypeError:\n        # bins is an integer\n        bins = D*[bins]\n\n    # normalize the range argument\n    if range is None:\n        range = (None,) * D\n    elif len(range) != D:\n        raise ValueError('range argument must have one entry per dimension')\n\n    # Create edge arrays\n    for i in _range(D):\n        if np.ndim(bins[i]) == 0:\n            if bins[i] < 1:\n                raise ValueError(\n                    '`bins[{}]` must be positive, when an integer'.format(i))\n            smin, smax = _get_outer_edges(sample[:,i], range[i])\n            try:\n                n = operator.index(bins[i])\n\n            except TypeError as e:\n                raise TypeError(\n                \t\"`bins[{}]` must be an integer, when a scalar\".format(i)\n                ) from e\n\n            edges[i] = np.linspace(smin, smax, n + 1)\n        elif np.ndim(bins[i]) == 1:\n            edges[i] = np.asarray(bins[i])\n            if np.any(edges[i][:-1] > edges[i][1:]):\n                raise ValueError(\n                    '`bins[{}]` must be monotonically increasing, when an array'\n                    .format(i))\n        else:\n            raise ValueError(\n                '`bins[{}]` must be a scalar or 1d array'.format(i))\n\n        nbin[i] = len(edges[i]) + 1  # includes an outlier on each end\n        dedges[i] = np.diff(edges[i])\n\n    # Compute the bin number each sample falls into.\n    Ncount = tuple(\n        # avoid np.digitize to work around gh-11022\n        np.searchsorted(edges[i], sample[:, i], side='right')\n        for i in _range(D)\n    )\n\n    # Using digitize, values that fall on an edge are put in the right bin.\n    # For the rightmost bin, we want values equal to the right edge to be\n    # counted in the last bin, and not as an outlier.\n    for i in _range(D):\n        # Find which points are on the rightmost edge.\n        on_edge = (sample[:, i] == edges[i][-1])\n        # Shift these points one bin to the left.\n        Ncount[i][on_edge] -= 1\n\n    # Compute the sample indices in the flattened histogram matrix.\n    # This raises an error if the array is too large.\n    xy = np.ravel_multi_index(Ncount, nbin)\n\n    # Compute the number of repetitions in xy and assign it to the\n    # flattened histmat.\n    hist = np.bincount(xy, weights, minlength=nbin.prod())\n\n    # Shape into a proper matrix\n    hist = hist.reshape(nbin)\n\n    # This preserves the (bad) behavior observed in gh-7845, for now.\n    hist = hist.astype(float, casting='safe')\n\n    # Remove outliers (indices 0 and -1 for each dimension).\n    core = D*(slice(1, -1),)\n    hist = hist[core]\n\n    if density:\n        # calculate the probability density function\n        s = hist.sum()\n        for i in _range(D):\n            shape = np.ones(D, int)\n            shape[i] = nbin[i] - 2\n            hist = hist / dedges[i].reshape(shape)\n        hist /= s\n\n    if (hist.shape != nbin - 2).any():\n        raise RuntimeError(\n            \"Internal Shape Error\")\n    return hist, edges"
        },
        {
            "name": "numpy.histogram_bin_edges",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html",
            "description": "Function to calculate only the edges of the bins used by the `histogram` function.  Parameters ---------- a : array_like Input data. The histogram is computed over the flattened array. bins : int or sequence of scalars or str, optional If `bins` is an int, it defines the number of equal-width bins in the given range (10, by default). If `bins` is a sequence, it defines the bin edges, including the rightmost edge, allowing for non-uniform bin widths.  If `bins` is a string from the list below, `histogram_bin_edges` will use the method chosen to calculate the optimal bin width and consequently the number of bins (see `Notes` for more detail on the estimators) from the data that falls within the requested range. While the bin width will be optimal for the actual data in the range, the number of bins will be computed to fill the entire range, including the empty portions. For visualisation, using the 'auto' option is suggested. Weighted data is not supported for automated bin size selection.  'auto' Maximum of the 'sturges' and 'fd' estimators. Provides good all around performance.  'fd' (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.  'doane' An improved version of Sturges' estimator that works better with non-normal datasets.  'scott' Less robust estimator that takes into account data variability and data size.  'stone' Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.  'rice' Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.  'sturges' R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.  'sqrt' Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.  range : (float, float), optional The lower and upper range of the bins.  If not provided, range is simply ``(a.min(), a.max())``.  Values outside the range are ignored. The first element of the range must be less than or equal to the second. `range` affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within `range`, the bin count will fill the entire range including portions containing no data.  weights : array_like, optional An array of weights, of the same shape as `a`.  Each value in `a` only contributes its associated weight towards the bin count (instead of 1). This is currently not used by any of the bin estimators, but may be in the future.  Returns ------- bin_edges : array of dtype float The edges to pass into `histogram`  See Also -------- histogram  Notes ----- The methods to estimate the optimal number of bins are well founded in literature, and are inspired by the choices R provides for histogram visualisation. Note that having the number of bins proportional to :math:`n^{1/3}` is asymptotically optimal, which is why it appears in most estimators. These are simply plug-in methods that give good starting points for number of bins. In the equations below, :math:`h` is the binwidth and :math:`n_h` is the number of bins. All estimators that compute bin counts are recast to bin width using the `ptp` of the data. The final bin count is obtained from ``np.round(np.ceil(range / h))``. The final bin width is often less than what is returned by the estimators below.  'auto' (maximum of the 'sturges' and 'fd' estimators) A compromise to get a good value. For small datasets the Sturges value will usually be chosen, while larger datasets will usually default to FD.  Avoids the overly conservative behaviour of FD and Sturges for small and large datasets respectively. Switchover point is usually :math:`a.size \u0007pprox 1000`.  'fd' (Freedman Diaconis Estimator) .. math:: h = 2 \frac{IQR}{n^{1/3}}  The binwidth is proportional to the interquartile range (IQR) and inversely proportional to cube root of a.size. Can be too conservative for small datasets, but is quite good for large datasets. The IQR is very robust to outliers.  'scott' .. math:: h = \\sigma \\sqrt[3]{\frac{24 \\sqrt{\\pi}}{n}}  The binwidth is proportional to the standard deviation of the data and inversely proportional to cube root of ``x.size``. Can be too conservative for small datasets, but is quite good for large datasets. The standard deviation is not very robust to outliers. Values are very similar to the Freedman-Diaconis estimator in the absence of outliers.  'rice' .. math:: n_h = 2n^{1/3}  The number of bins is only proportional to cube root of ``a.size``. It tends to overestimate the number of bins and it does not take into account data variability.  'sturges' .. math:: n_h = \\log _{2}(n) + 1  The number of bins is the base 2 log of ``a.size``.  This estimator assumes normality of data and is too conservative for larger, non-normal datasets. This is the default method in R's ``hist`` method.  'doane' .. math:: n_h = 1 + \\log_{2}(n) + \\log_{2}\\left(1 + \frac{|g_1|}{\\sigma_{g_1}}\right)  g_1 = mean\\left[\\left(\frac{x - \\mu}{\\sigma}\right)^3\right]  \\sigma_{g_1} = \\sqrt{\frac{6(n - 2)}{(n + 1)(n + 3)}}  An improved version of Sturges' formula that produces better estimates for non-normal datasets. This estimator attempts to account for the skew of the data.  'sqrt' .. math:: n_h = \\sqrt n  The simplest and fastest estimator. Only takes into account the data size.  Examples -------- >>> arr = np.array([0, 0, 0, 1, 2, 3, 3, 4, 5]) >>> np.histogram_bin_edges(arr, bins='auto', range=(0, 1)) array([0.  , 0.25, 0.5 , 0.75, 1.  ]) >>> np.histogram_bin_edges(arr, bins=2) array([0. , 2.5, 5. ])  For consistency with histogram, an array of pre-computed bins is passed through unmodified:  >>> np.histogram_bin_edges(arr, [1, 2]) array([1, 2])  This function allows one set of bins to be computed, and reused across multiple histograms:  >>> shared_bins = np.histogram_bin_edges(arr, bins='auto') >>> shared_bins array([0., 1., 2., 3., 4., 5.])  >>> group_id = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1]) >>> hist_0, _ = np.histogram(arr[group_id == 0], bins=shared_bins) >>> hist_1, _ = np.histogram(arr[group_id == 1], bins=shared_bins)  >>> hist_0; hist_1 array([1, 1, 0, 1, 0]) array([2, 0, 1, 1, 2])  Which gives more easily comparable results than using separate bins for each histogram:  >>> hist_0, bins_0 = np.histogram(arr[group_id == 0], bins='auto') >>> hist_1, bins_1 = np.histogram(arr[group_id == 1], bins='auto') >>> hist_0; hist_1 array([1, 1, 1]) array([2, 1, 1, 2]) >>> bins_0; bins_1 array([0., 1., 2., 3.]) array([0.  , 1.25, 2.5 , 3.75, 5.  ])",
            "source": "@array_function_dispatch(_histogram_bin_edges_dispatcher)\ndef histogram_bin_edges(a, bins=10, range=None, weights=None):\n    \n    a, weights = _ravel_and_check_weights(a, weights)\n    bin_edges, _ = _get_bin_edges(a, bins, range, weights)\n    return bin_edges"
        },
        {
            "name": "numpy.digitize",
            "url": "https://numpy.org/doc/stable/reference/generated/numpy.digitize.html",
            "description": "Return the indices of the bins to which each value in input array belongs.  =========  =============  ============================ `right`    order of bins  returned index `i` satisfies =========  =============  ============================ ``False``  increasing     ``bins[i-1] <= x < bins[i]`` ``True``   increasing     ``bins[i-1] < x <= bins[i]`` ``False``  decreasing     ``bins[i-1] > x >= bins[i]`` ``True``   decreasing     ``bins[i-1] >= x > bins[i]`` =========  =============  ============================  If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is returned as appropriate.  Parameters ---------- x : array_like Input array to be binned. Prior to NumPy 1.10.0, this array had to be 1-dimensional, but can now have any shape. bins : array_like Array of bins. It has to be 1-dimensional and monotonic. right : bool, optional Indicating whether the intervals include the right or the left bin edge. Default behavior is (right==False) indicating that the interval does not include the right edge. The left bin end is open in this case, i.e., bins[i-1] <= x < bins[i] is the default behavior for monotonically increasing bins.  Returns ------- indices : ndarray of ints Output array of indices, of same shape as `x`.  Raises ------ ValueError If `bins` is not monotonic. TypeError If the type of the input is complex.  See Also -------- bincount, histogram, unique, searchsorted  Notes ----- If values in `x` are such that they fall outside the bin range, attempting to index `bins` with the indices that `digitize` returns will result in an IndexError.  .. versionadded:: 1.10.0  `np.digitize` is  implemented in terms of `np.searchsorted`. This means that a binary search is used to bin the values, which scales much better for larger number of bins than the previous linear search. It also removes the requirement for the input array to be 1-dimensional.  For monotonically _increasing_ `bins`, the following are equivalent::  np.digitize(x, bins, right=True) np.searchsorted(bins, x, side='left')  Note that as the order of the arguments are reversed, the side must be too. The `searchsorted` call is marginally faster, as it does not do any monotonicity checks. Perhaps more importantly, it supports all dtypes.  Examples -------- >>> x = np.array([0.2, 6.4, 3.0, 1.6]) >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0]) >>> inds = np.digitize(x, bins) >>> inds array([1, 4, 3, 2]) >>> for n in range(x.size): ...   print(bins[inds[n]-1], \"<=\", x[n], \"<\", bins[inds[n]]) ... 0.0 <= 0.2 < 1.0 4.0 <= 6.4 < 10.0 2.5 <= 3.0 < 4.0 1.0 <= 1.6 < 2.5  >>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.]) >>> bins = np.array([0, 5, 10, 15, 20]) >>> np.digitize(x,bins,right=True) array([1, 2, 3, 4, 4]) >>> np.digitize(x,bins,right=False) array([1, 3, 3, 4, 5])",
            "source": "@array_function_dispatch(_digitize_dispatcher)\ndef digitize(x, bins, right=False):\n    \n    x = _nx.asarray(x)\n    bins = _nx.asarray(bins)\n\n    # here for compatibility, searchsorted below is happy to take this\n    if np.issubdtype(x.dtype, _nx.complexfloating):\n        raise TypeError(\"x may not be complex\")\n\n    mono = _monotonicity(bins)\n    if mono == 0:\n        raise ValueError(\"bins must be monotonically increasing or decreasing\")\n\n    # this is backwards because the arguments below are swapped\n    side = 'left' if right else 'right'\n    if mono == -1:\n        # reverse the bins, and invert the results\n        return len(bins) - _nx.searchsorted(bins[::-1], x, side=side)\n    else:\n        return _nx.searchsorted(bins, x, side=side)"
        }
    ]
]