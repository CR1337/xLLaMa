# LLM Facade

This service provides an abstraction for the Open AI and the Ollama interface. It can
- list installed models
- install new models (will be installed using Ollama)
- generate completions

## Endpoints

### `GET /`
This endpoint only exists for debugging purposes. It doesn't take any further parameters and always returns
```json
{"message": "Hello, world! This is 'llm_facade'."}
```

### `GET /models`
The `/models` endpoint returns a list of all models available. It returns a json response like this
```json
{
    "models": [
        "gpt-4",
        "gpt-3.5-turbo",
        "codellama:7b-instruct"
    ]
}
```

### `GET /install-models`
To install models one can call this endpoint. It takes a json body like this
```json
{
    "model": <MODEL_NAME>,
    "stream": true
}
```
where `stream` is optional and defaults to `true`. The specified model `<MODEL_NAME>` will be installed locally using Ollama.

If `stream` is `false` a json object will be returned after the installation is finished:
```json
{
    "status": "success"
}
```

If `stream` is `true` a event stream will be returned where each event looks like the following:
```
event: model_installation_progress
data: <JSON_DATA>
id: <ID>
retry: <RETRY_PERIOD>

```
Here `<ID>` is a continuously increasing integer starting at `0`. `<RETRY_PERIOD>` instructs the browser to retry connecting if no event was sent for this amount of milliseconds. `<JSON_OBJECT>` contains information about the progress which is described [here](https://github.com/jmorganca/ollama/blob/main/docs/api.md#response-10).


### `GET /generate`
This endpoint generates a completion using a prompt and some other parameters using a specified model. It expects a JSON body with the following fields:

|Field|Type|Optional|Default|Description|
|-----|----|--------|-------|-----------|
|`model`|`str`|no||The model to use for the completion.|
|`prompt`|`str`|no||The prompt given to the model.|
|`system_prompt`|`str`|yes||An additional prompt telling the model how to behave before the actual prompt.|
|`repeat_penalty`|`float`|yes|`1.1`|How strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.|
|`max_tokens`|`int`|yes|`256`|The maximum amount of tokens to generate.|
|`seed`|`int`|yes|`0`|A random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt.|
|`temperature`|`float`|yes|`0.8`|Increasing the temperature will make the model answer more creatively.|
|`top_p`|`float`|yes|`0.9`|A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.|
|`stop`|`List[str]`|yes||The stop sequences to use. When this pattern is encountered the LLM will stop generating text and return.|

It returns a JSON object as follows
```json
{
    "content": <GENERATED_TEXT>,
    "created_at": <DATETIME>,
    "token_count": <TOKEN_COUNT>
}
```
where `<GENERATED_TEXT>` is the text generated by the model, `<DATETIME>` is a datetime string that states when the generation was performed und `<TOKEN_COUNT>` is the amount of generated tokens.

### `GET /generate-stream`
This endpoint behaves like `GET /generate` except that it returns the generated tokens on thy fly using an event stream.

Each event has the following pattern:
```
event: token_generated
data: <JSON_DATA>
id: <ID>
retry: <RETRY_PERIOD>

```
where `<ID>` is a continuously increasing integer starting at `0`. `<RETRY_PERIOD>` instructs the browser to retry connecting if no event was sent for this amount of milliseconds. `<JSON_OBJECT>` contains the generated token and some other information.

If an event is not the last event `<JSON_DATA>` contains
```json
{
    "token": <TOKEN>,
    "created_at": <DATETIME>,
    "done": false
}
```

else `<JSON_DATA>` contains
```json
{
    "content": <GENERATED_TEXT>,
    "created_at": <DATETIME>,
    "done": true,
    "token_count": <TOKEN_COUNT>
}
```
where `<GENERATED_TEXT>` is the text generated by the model, `<DATETIME>` is a datetime string that states when the generation was performed und `<TOKEN_COUNT>` is the amount of generated tokens.
